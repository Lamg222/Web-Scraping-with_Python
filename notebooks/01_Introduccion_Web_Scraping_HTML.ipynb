{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🕷️ Lección 1: Introducción a Web Scraping y HTML Básico\n",
    "\n",
    "## 🎯 Objetivos de Aprendizaje\n",
    "\n",
    "Al finalizar esta lección, serás capaz de:\n",
    "- ✅ Comprender qué es el web scraping y sus aplicaciones\n",
    "- ✅ Entender la estructura básica de HTML y el DOM\n",
    "- ✅ Identificar elementos, atributos y jerarquías en HTML\n",
    "- ✅ Utilizar las herramientas de desarrollo del navegador\n",
    "- ✅ Escribir tu primer script de web scraping\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ¿Qué es Web Scraping? 🤖\n",
    "\n",
    "**Web Scraping** es la técnica de extracción automática de datos de sitios web. Es como tener un asistente digital que puede leer y recopilar información de páginas web de manera automática y estructurada.\n",
    "\n",
    "### 🌟 Aplicaciones del Web Scraping:\n",
    "\n",
    "| Área | Aplicación | Ejemplo |\n",
    "|------|------------|----------|\n",
    "| 💰 **E-commerce** | Monitoreo de precios | Comparar precios entre tiendas |\n",
    "| 📊 **Investigación** | Recopilación de datos | Análisis académico y estudios |\n",
    "| 📰 **Periodismo** | Extracción de información | Periodismo de datos |\n",
    "| 🤖 **Machine Learning** | Creación de datasets | Entrenar modelos de IA |\n",
    "| 👀 **Monitoreo** | Seguimiento de cambios | Alertas de contenido nuevo |\n",
    "| 🔍 **Agregación** | Comparadores | Portales de ofertas de empleo |\n",
    "\n",
    "### 🐍 ¿Por qué Python para Web Scraping?\n",
    "\n",
    "Python es el lenguaje ideal porque:\n",
    "- 📝 **Sintaxis simple**: Fácil de leer y escribir\n",
    "- 📚 **Bibliotecas especializadas**: Beautiful Soup, Scrapy, Selenium\n",
    "- 👥 **Gran comunidad**: Mucha documentación y soporte\n",
    "- 🔗 **Integración**: Se conecta fácilmente con pandas, numpy, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fundamentos de HTML 📄\n",
    "\n",
    "HTML (HyperText Markup Language) es el lenguaje de marcado que estructura las páginas web. Para hacer web scraping efectivo, necesitas entender cómo está organizado el HTML.\n",
    "\n",
    "### 2.1 Estructura Básica de un Documento HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de HTML que analizaremos durante la lección\n",
    "html_ejemplo = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"es\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>Mi Primera Página Web</title>\n",
    "    <meta name=\"description\" content=\"Ejemplo para aprender web scraping\">\n",
    "</head>\n",
    "<body>\n",
    "    <header>\n",
    "        <h1 id=\"titulo-principal\" class=\"titulo-grande\">🕷️ Bienvenido al Web Scraping</h1>\n",
    "        <nav>\n",
    "            <ul class=\"menu-navegacion\">\n",
    "                <li><a href=\"#seccion1\">Introducción</a></li>\n",
    "                <li><a href=\"#seccion2\">Conceptos</a></li>\n",
    "                <li><a href=\"#seccion3\">Ejemplos</a></li>\n",
    "            </ul>\n",
    "        </nav>\n",
    "    </header>\n",
    "    \n",
    "    <main>\n",
    "        <section id=\"seccion1\" class=\"contenido importante\">\n",
    "            <h2>🚀 Introducción al Web Scraping</h2>\n",
    "            <p class=\"texto-destacado\">El web scraping es una técnica <strong>poderosa</strong> para extraer datos.</p>\n",
    "            <p>Permite automatizar la recopilación de información de sitios web.</p>\n",
    "            <div class=\"estadisticas\" data-tipo=\"metricas\">\n",
    "                <span class=\"numero\">95%</span> de eficiencia\n",
    "            </div>\n",
    "        </section>\n",
    "        \n",
    "        <section id=\"seccion2\" class=\"contenido\">\n",
    "            <h2>🔑 Conceptos Clave</h2>\n",
    "            <div class=\"conceptos-grid\">\n",
    "                <article data-tema=\"html\" class=\"concepto\">\n",
    "                    <h3>HTML</h3>\n",
    "                    <p>Lenguaje de marcado para crear páginas web.</p>\n",
    "                    <span class=\"dificultad\" data-nivel=\"facil\">⭐ Fácil</span>\n",
    "                </article>\n",
    "                <article data-tema=\"css\" class=\"concepto\">\n",
    "                    <h3>CSS</h3>\n",
    "                    <p>Hojas de estilo para dar formato visual.</p>\n",
    "                    <span class=\"dificultad\" data-nivel=\"medio\">⭐⭐ Medio</span>\n",
    "                </article>\n",
    "                <article data-tema=\"javascript\" class=\"concepto\">\n",
    "                    <h3>JavaScript</h3>\n",
    "                    <p>Lenguaje para añadir interactividad.</p>\n",
    "                    <span class=\"dificultad\" data-nivel=\"dificil\">⭐⭐⭐ Difícil</span>\n",
    "                </article>\n",
    "            </div>\n",
    "        </section>\n",
    "        \n",
    "        <section id=\"seccion3\" class=\"contenido ejemplos\">\n",
    "            <h2>📊 Ejemplos Prácticos</h2>\n",
    "            <table class=\"tabla-datos\" border=\"1\">\n",
    "                <thead>\n",
    "                    <tr>\n",
    "                        <th>Técnica</th>\n",
    "                        <th>Dificultad</th>\n",
    "                        <th>Uso Común</th>\n",
    "                        <th>Velocidad</th>\n",
    "                    </tr>\n",
    "                </thead>\n",
    "                <tbody>\n",
    "                    <tr data-id=\"1\">\n",
    "                        <td>Beautiful Soup</td>\n",
    "                        <td class=\"facil\">🟢 Fácil</td>\n",
    "                        <td>Sitios estáticos</td>\n",
    "                        <td>⚡ Rápido</td>\n",
    "                    </tr>\n",
    "                    <tr data-id=\"2\">\n",
    "                        <td>Scrapy</td>\n",
    "                        <td class=\"medio\">🟡 Intermedio</td>\n",
    "                        <td>Proyectos grandes</td>\n",
    "                        <td>⚡⚡ Muy rápido</td>\n",
    "                    </tr>\n",
    "                    <tr data-id=\"3\">\n",
    "                        <td>Selenium</td>\n",
    "                        <td class=\"dificil\">🔴 Avanzado</td>\n",
    "                        <td>Sitios dinámicos</td>\n",
    "                        <td>🐌 Lento</td>\n",
    "                    </tr>\n",
    "                </tbody>\n",
    "            </table>\n",
    "            \n",
    "            <div class=\"lista-sitios\">\n",
    "                <h3>🌐 Sitios Populares para Practicar:</h3>\n",
    "                <ul>\n",
    "                    <li><a href=\"http://quotes.toscrape.com\" target=\"_blank\">Quotes to Scrape</a></li>\n",
    "                    <li><a href=\"http://books.toscrape.com\" target=\"_blank\">Books to Scrape</a></li>\n",
    "                    <li><a href=\"https://scrapethissite.com\" target=\"_blank\">Scrape This Site</a></li>\n",
    "                </ul>\n",
    "            </div>\n",
    "        </section>\n",
    "    </main>\n",
    "    \n",
    "    <footer>\n",
    "        <p>&copy; 2024 Curso de Web Scraping | <em>Aprende haciendo</em></p>\n",
    "        <div class=\"redes-sociales\">\n",
    "            <a href=\"#\" class=\"red-social\" data-plataforma=\"twitter\">🐦 Twitter</a>\n",
    "            <a href=\"#\" class=\"red-social\" data-plataforma=\"github\">🐙 GitHub</a>\n",
    "        </div>\n",
    "    </footer>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "print(\"✅ HTML de ejemplo creado exitosamente!\")\n",
    "print(f\"📏 Tamaño del HTML: {len(html_ejemplo)} caracteres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Anatomía de un Elemento HTML 🔍\n",
    "\n",
    "Un elemento HTML tiene la siguiente estructura:\n",
    "\n",
    "```html\n",
    "<etiqueta atributo=\"valor\" otro-atributo=\"otro-valor\">Contenido</etiqueta>\n",
    "```\n",
    "\n",
    "**Componentes:**\n",
    "- 🏷️ **Etiqueta de apertura**: `<p>`\n",
    "- 🎯 **Atributos**: `class=\"texto\"`, `id=\"parrafo1\"`, `data-info=\"importante\"`\n",
    "- 📝 **Contenido**: El texto o elementos dentro\n",
    "- 🔚 **Etiqueta de cierre**: `</p>`\n",
    "\n",
    "### 2.3 Elementos HTML Más Importantes para Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario de elementos HTML más comunes en web scraping\n",
    "elementos_html = {\n",
    "    '🏗️ Estructura': {\n",
    "        '<html>': 'Elemento raíz del documento HTML',\n",
    "        '<head>': 'Metadatos que no se muestran al usuario',\n",
    "        '<body>': 'Contenido visible de la página web',\n",
    "        '<header>': 'Encabezado de la página o sección',\n",
    "        '<main>': 'Contenido principal de la página',\n",
    "        '<footer>': 'Pie de página con información adicional'\n",
    "    },\n",
    "    '📝 Texto y Contenido': {\n",
    "        '<h1> - <h6>': 'Títulos y subtítulos (h1 es el más importante)',\n",
    "        '<p>': 'Párrafos de texto normal',\n",
    "        '<span>': 'Contenedor en línea para pequeñas porciones',\n",
    "        '<strong>': 'Texto importante (aparece en negrita)',\n",
    "        '<em>': 'Énfasis (aparece en cursiva)',\n",
    "        '<br>': 'Salto de línea (no tiene cierre)'\n",
    "    },\n",
    "    '🔗 Enlaces y Media': {\n",
    "        '<a>': 'Hipervínculos a otras páginas o secciones',\n",
    "        '<img>': 'Imágenes (no tiene etiqueta de cierre)',\n",
    "        '<video>': 'Contenido de video',\n",
    "        '<audio>': 'Contenido de audio'\n",
    "    },\n",
    "    '📋 Listas': {\n",
    "        '<ul>': 'Lista no ordenada (bullets)',\n",
    "        '<ol>': 'Lista ordenada (números)',\n",
    "        '<li>': 'Elemento individual de lista'\n",
    "    },\n",
    "    '📦 Contenedores': {\n",
    "        '<div>': 'División genérica (muy común en scraping)',\n",
    "        '<section>': 'Sección temática de contenido',\n",
    "        '<article>': 'Contenido independiente',\n",
    "        '<nav>': 'Navegación y menús'\n",
    "    },\n",
    "    '📊 Tablas': {\n",
    "        '<table>': 'Contenedor principal de la tabla',\n",
    "        '<thead>': 'Encabezado de la tabla',\n",
    "        '<tbody>': 'Cuerpo con datos de la tabla',\n",
    "        '<tr>': 'Fila de la tabla',\n",
    "        '<th>': 'Celda de encabezado',\n",
    "        '<td>': 'Celda con datos'\n",
    "    },\n",
    "    '📝 Formularios': {\n",
    "        '<form>': 'Formulario para enviar datos',\n",
    "        '<input>': 'Campo de entrada de datos',\n",
    "        '<button>': 'Botón clicable',\n",
    "        '<select>': 'Lista desplegable',\n",
    "        '<option>': 'Opción dentro de select'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Mostrar los elementos de forma organizada\n",
    "for categoria, elementos in elementos_html.items():\n",
    "    print(f\"\\n{categoria}:\")\n",
    "    print(\"─\" * 60)\n",
    "    for elemento, descripcion in elementos.items():\n",
    "        print(f\"  {elemento:<15} → {descripcion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. El DOM (Document Object Model) 🌳\n",
    "\n",
    "El DOM es una representación en forma de **árbol jerárquico** de la estructura HTML. Cada elemento HTML es un **nodo** en este árbol, y entender esta estructura es fundamental para el web scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización conceptual del DOM de nuestro ejemplo\n",
    "estructura_dom = \"\"\"\n",
    "📄 document (raíz)\n",
    "└── html (elemento raíz del HTML)\n",
    "    ├── head (metadatos)\n",
    "    │   ├── meta (charset)\n",
    "    │   ├── title (título de la página)\n",
    "    │   └── meta (descripción)\n",
    "    └── body (contenido visible)\n",
    "        ├── header (encabezado)\n",
    "        │   ├── h1#titulo-principal (título principal)\n",
    "        │   └── nav (navegación)\n",
    "        │       └── ul.menu-navegacion (lista)\n",
    "        │           ├── li → a (enlace 1)\n",
    "        │           ├── li → a (enlace 2)\n",
    "        │           └── li → a (enlace 3)\n",
    "        ├── main (contenido principal)\n",
    "        │   ├── section#seccion1 (primera sección)\n",
    "        │   │   ├── h2 (subtítulo)\n",
    "        │   │   ├── p.texto-destacado (párrafo destacado)\n",
    "        │   │   ├── p (párrafo normal)\n",
    "        │   │   └── div.estadisticas (métricas)\n",
    "        │   ├── section#seccion2 (segunda sección)\n",
    "        │   │   ├── h2 (subtítulo)\n",
    "        │   │   └── div.conceptos-grid\n",
    "        │   │       ├── article[data-tema=\"html\"]\n",
    "        │   │       ├── article[data-tema=\"css\"]\n",
    "        │   │       └── article[data-tema=\"javascript\"]\n",
    "        │   └── section#seccion3 (tercera sección)\n",
    "        │       ├── h2 (subtítulo)\n",
    "        │       ├── table.tabla-datos (tabla)\n",
    "        │       │   ├── thead → tr → th (encabezados)\n",
    "        │       │   └── tbody → tr → td (datos)\n",
    "        │       └── div.lista-sitios (lista de sitios)\n",
    "        └── footer (pie de página)\n",
    "            ├── p (copyright)\n",
    "            └── div.redes-sociales (enlaces sociales)\n",
    "\"\"\"\n",
    "\n",
    "print(\"🌳 Estructura del DOM como árbol jerárquico:\")\n",
    "print(estructura_dom)\n",
    "\n",
    "# Conceptos importantes del DOM\n",
    "conceptos_dom = {\n",
    "    \"👨‍👧‍👦 Nodo Padre (Parent)\": \"El elemento que contiene a otros elementos\",\n",
    "    \"👶 Nodo Hijo (Child)\": \"Elemento contenido directamente dentro de otro\",\n",
    "    \"👫 Hermanos (Siblings)\": \"Elementos que están al mismo nivel jerárquico\",\n",
    "    \"👴 Ancestros (Ancestors)\": \"Todos los elementos padre en la cadena hacia arriba\",\n",
    "    \"👪 Descendientes (Descendants)\": \"Todos los elementos contenidos (hijos, nietos, etc.)\",\n",
    "    \"🔍 Selector\": \"Patrón usado para encontrar elementos específicos\",\n",
    "    \"🏷️ Atributo\": \"Información adicional sobre un elemento (id, class, etc.)\"\n",
    "}\n",
    "\n",
    "print(\"\\n🔑 Conceptos Clave del DOM:\")\n",
    "print(\"═\" * 70)\n",
    "for concepto, descripcion in conceptos_dom.items():\n",
    "    print(f\"{concepto}: {descripcion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Atributos HTML Cruciales para Web Scraping 🎯\n",
    "\n",
    "Los atributos son características adicionales de los elementos HTML. Son tu **herramienta principal** para localizar elementos específicos al hacer scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los atributos más importantes para web scraping\n",
    "atributos_scraping = {\n",
    "    '🆔 id': {\n",
    "        'descripcion': 'Identificador único del elemento en toda la página',\n",
    "        'ejemplo': '<div id=\"contenido-principal\">',\n",
    "        'uso_scraping': '⭐⭐⭐ Perfecto para seleccionar UN elemento específico',\n",
    "        'ventaja': 'Garantiza unicidad, muy confiable',\n",
    "        'selector_css': '#mi-id',\n",
    "        'selector_xpath': '//*[@id=\"mi-id\"]'\n",
    "    },\n",
    "    '🏷️ class': {\n",
    "        'descripcion': 'Clase(s) CSS para agrupar elementos similares',\n",
    "        'ejemplo': '<p class=\"texto importante destacado\">',\n",
    "        'uso_scraping': '⭐⭐⭐ Ideal para seleccionar GRUPOS de elementos',\n",
    "        'ventaja': 'Muy común, puede tener múltiples valores',\n",
    "        'selector_css': '.mi-clase',\n",
    "        'selector_xpath': '//*[@class=\"mi-clase\"]'\n",
    "    },\n",
    "    '🔗 href': {\n",
    "        'descripcion': 'URL de destino en enlaces',\n",
    "        'ejemplo': '<a href=\"https://ejemplo.com\">Enlace</a>',\n",
    "        'uso_scraping': '⭐⭐ Para extraer URLs y navegar entre páginas',\n",
    "        'ventaja': 'Esencial para crawling y navegación',\n",
    "        'selector_css': 'a[href]',\n",
    "        'selector_xpath': '//a[@href]'\n",
    "    },\n",
    "    '🖼️ src': {\n",
    "        'descripcion': 'Fuente de recursos (imágenes, scripts, etc.)',\n",
    "        'ejemplo': '<img src=\"imagen.jpg\" alt=\"Descripción\">',\n",
    "        'uso_scraping': '⭐⭐ Para descargar imágenes o identificar recursos',\n",
    "        'ventaja': 'Útil para multimedia y assets',\n",
    "        'selector_css': 'img[src]',\n",
    "        'selector_xpath': '//img[@src]'\n",
    "    },\n",
    "    '📊 data-*': {\n",
    "        'descripcion': 'Atributos personalizados para almacenar datos',\n",
    "        'ejemplo': '<div data-producto-id=\"12345\" data-precio=\"99.99\">',\n",
    "        'uso_scraping': '⭐⭐⭐ Contienen información estructurada MUY valiosa',\n",
    "        'ventaja': 'Datos limpios y estructurados',\n",
    "        'selector_css': '[data-producto-id]',\n",
    "        'selector_xpath': '//*[@data-producto-id]'\n",
    "    },\n",
    "    '📝 name': {\n",
    "        'descripcion': 'Nombre de elementos de formulario',\n",
    "        'ejemplo': '<input name=\"email\" type=\"email\">',\n",
    "        'uso_scraping': '⭐ Para identificar campos de formulario',\n",
    "        'ventaja': 'Útil para automatizar formularios',\n",
    "        'selector_css': '[name=\"email\"]',\n",
    "        'selector_xpath': '//*[@name=\"email\"]'\n",
    "    },\n",
    "    '🏷️ title': {\n",
    "        'descripcion': 'Texto que aparece al hacer hover',\n",
    "        'ejemplo': '<img src=\"foto.jpg\" title=\"Mi foto favorita\">',\n",
    "        'uso_scraping': '⭐ Para obtener información adicional',\n",
    "        'ventaja': 'Información extra no siempre visible',\n",
    "        'selector_css': '[title]',\n",
    "        'selector_xpath': '//*[@title]'\n",
    "    },\n",
    "    '🎨 style': {\n",
    "        'descripcion': 'Estilos CSS inline',\n",
    "        'ejemplo': '<div style=\"color: red; font-size: 16px;\">',\n",
    "        'uso_scraping': '⭐ Ocasionalmente útil para filtrar elementos',\n",
    "        'ventaja': 'Puede contener información de estado',\n",
    "        'selector_css': '[style]',\n",
    "        'selector_xpath': '//*[@style]'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Mostrar información detallada sobre cada atributo\n",
    "for atributo, info in atributos_scraping.items():\n",
    "    print(f\"\\n{atributo}\")\n",
    "    print(\"═\" * 80)\n",
    "    for key, value in info.items():\n",
    "        emoji_key = {\n",
    "            'descripcion': '📋',\n",
    "            'ejemplo': '💻',\n",
    "            'uso_scraping': '🎯',\n",
    "            'ventaja': '✨',\n",
    "            'selector_css': '🎨',\n",
    "            'selector_xpath': '🛤️'\n",
    "        }.get(key, '•')\n",
    "        print(f\"  {emoji_key} {key.replace('_', ' ').title()}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tu Primer Script de Web Scraping 🚀\n",
    "\n",
    "¡Ahora viene la parte emocionante! Vamos a escribir nuestro primer script de web scraping usando las bibliotecas más populares de Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de bibliotecas necesarias\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def instalar_librerias():\n",
    "    \"\"\"Instala las bibliotecas necesarias para web scraping\"\"\"\n",
    "    librerias = ['requests', 'beautifulsoup4', 'lxml']\n",
    "    \n",
    "    for libreria in librerias:\n",
    "        try:\n",
    "            __import__(libreria.replace('-', '_').replace('4', ''))\n",
    "            print(f\"✅ {libreria} ya está instalada\")\n",
    "        except ImportError:\n",
    "            print(f\"📦 Instalando {libreria}...\")\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', libreria, '--quiet'])\n",
    "            print(f\"✅ {libreria} instalada correctamente\")\n",
    "\n",
    "# Ejecutar instalación\n",
    "instalar_librerias()\n",
    "print(\"\\n🎉 ¡Todas las bibliotecas están listas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Mi primer scraper: Extraer citas inspiradoras\n",
    "def mi_primer_scraping():\n",
    "    \"\"\"\n",
    "    Función que realiza web scraping de quotes.toscrape.com\n",
    "    Extrae citas, autores y tags de forma estructurada\n",
    "    \"\"\"\n",
    "    print(\"🕷️ INICIANDO MI PRIMER WEB SCRAPING\")\n",
    "    print(\"═\" * 50)\n",
    "    \n",
    "    # PASO 1: Configurar la solicitud\n",
    "    url = \"http://quotes.toscrape.com/\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    print(f\"🌐 Conectando a: {url}\")\n",
    "    print(f\"🤖 User-Agent: {headers['User-Agent'][:50]}...\")\n",
    "    \n",
    "    # PASO 2: Hacer la solicitud HTTP\n",
    "    try:\n",
    "        inicio = time.time()\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        tiempo_respuesta = time.time() - inicio\n",
    "        \n",
    "        # PASO 3: Verificar el estado de la respuesta\n",
    "        if response.status_code == 200:\n",
    "            print(f\"✅ Conexión exitosa!\")\n",
    "            print(f\"📊 Código de estado: {response.status_code}\")\n",
    "            print(f\"⏱️ Tiempo de respuesta: {tiempo_respuesta:.2f} segundos\")\n",
    "            print(f\"📄 Tamaño del HTML: {len(response.text):,} caracteres\")\n",
    "        else:\n",
    "            print(f\"❌ Error: Código de estado {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"❌ Error: Timeout en la conexión\")\n",
    "        return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"❌ Error en la solicitud: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # PASO 4: Parsear el HTML con Beautiful Soup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    print(f\"\\n🍲 HTML parseado con Beautiful Soup\")\n",
    "    print(f\"📋 Título de la página: {soup.find('title').text}\")\n",
    "    \n",
    "    # PASO 5: Extraer las citas\n",
    "    print(\"\\n📚 EXTRAYENDO CITAS INSPIRADORAS\")\n",
    "    print(\"─\" * 50)\n",
    "    \n",
    "    # Encontrar todos los contenedores de citas\n",
    "    quotes_containers = soup.find_all('div', class_='quote')\n",
    "    print(f\"🔍 Encontradas {len(quotes_containers)} citas en la página\")\n",
    "    \n",
    "    # Lista para almacenar las citas extraídas\n",
    "    citas_extraidas = []\n",
    "    \n",
    "    for i, quote_container in enumerate(quotes_containers[:5], 1):  # Primeras 5 citas\n",
    "        try:\n",
    "            # Extraer componentes de cada cita\n",
    "            texto_element = quote_container.find('span', class_='text')\n",
    "            autor_element = quote_container.find('small', class_='author')\n",
    "            tags_elements = quote_container.find_all('a', class_='tag')\n",
    "            \n",
    "            # Verificar que los elementos existen\n",
    "            if texto_element and autor_element:\n",
    "                texto = texto_element.text.strip()\n",
    "                autor = autor_element.text.strip()\n",
    "                tags = [tag.text.strip() for tag in tags_elements]\n",
    "                \n",
    "                # Crear diccionario con la información\n",
    "                cita_info = {\n",
    "                    'numero': i,\n",
    "                    'texto': texto,\n",
    "                    'autor': autor,\n",
    "                    'tags': tags,\n",
    "                    'num_palabras': len(texto.split()),\n",
    "                    'fecha_extraccion': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                }\n",
    "                \n",
    "                citas_extraidas.append(cita_info)\n",
    "                \n",
    "                # Mostrar información extraída\n",
    "                print(f\"\\n📖 Cita #{i}:\")\n",
    "                print(f\"   📝 Texto: {texto[:60]}{'...' if len(texto) > 60 else ''}\")\n",
    "                print(f\"   ✍️  Autor: {autor}\")\n",
    "                print(f\"   🏷️  Tags: {', '.join(tags) if tags else 'Sin tags'}\")\n",
    "                print(f\"   📊 Palabras: {cita_info['num_palabras']}\")\n",
    "            else:\n",
    "                print(f\"⚠️ Cita #{i}: Algunos elementos no encontrados\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error procesando cita #{i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # PASO 6: Estadísticas finales\n",
    "    print(\"\\n\" + \"═\" * 60)\n",
    "    print(\"📊 ESTADÍSTICAS FINALES\")\n",
    "    print(\"═\" * 60)\n",
    "    print(f\"✅ Citas extraídas exitosamente: {len(citas_extraidas)}\")\n",
    "    if citas_extraidas:\n",
    "        total_palabras = sum(cita['num_palabras'] for cita in citas_extraidas)\n",
    "        promedio_palabras = total_palabras / len(citas_extraidas)\n",
    "        todos_los_tags = []\n",
    "        for cita in citas_extraidas:\n",
    "            todos_los_tags.extend(cita['tags'])\n",
    "        tags_unicos = set(todos_los_tags)\n",
    "        \n",
    "        print(f\"📝 Total de palabras: {total_palabras}\")\n",
    "        print(f\"📈 Promedio de palabras por cita: {promedio_palabras:.1f}\")\n",
    "        print(f\"🏷️ Tags únicos encontrados: {len(tags_unicos)}\")\n",
    "        print(f\"🎯 Tags más comunes: {', '.join(list(tags_unicos)[:5])}\")\n",
    "    \n",
    "    return citas_extraidas\n",
    "\n",
    "# ¡Ejecutar nuestro primer web scraping!\n",
    "print(\"🚀 ¡VAMOS A HACER NUESTRO PRIMER WEB SCRAPING!\\n\")\n",
    "resultados = mi_primer_scraping()\n",
    "\n",
    "if resultados:\n",
    "    print(\"\\n🎉 ¡FELICIDADES! Has completado tu primer web scraping exitosamente.\")\n",
    "    print(\"🎓 Ahora tienes los fundamentos para scraping más avanzado.\")\n",
    "else:\n",
    "    print(\"\\n🤔 Algo salió mal, pero no te preocupes. ¡Es parte del aprendizaje!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Beautiful Soup en Profundidad 🍲\n",
    "\n",
    "Beautiful Soup es la biblioteca más popular para parsing HTML en Python. Es intuitiva, poderosa y perfecta para principiantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trabajando con nuestro HTML de ejemplo usando Beautiful Soup\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Crear objeto BeautifulSoup con nuestro HTML de ejemplo\n",
    "soup = BeautifulSoup(html_ejemplo, 'html.parser')\n",
    "\n",
    "print(\"🔍 MÉTODOS ESENCIALES DE BEAUTIFUL SOUP\\n\")\n",
    "print(\"═\" * 70)\n",
    "\n",
    "# 1. find() - El método más usado\n",
    "print(\"\\n1️⃣ Método find() - Encuentra el PRIMER elemento que coincida:\")\n",
    "print(\"─\" * 50)\n",
    "\n",
    "primer_h2 = soup.find('h2')\n",
    "print(f\"   🎯 Primer <h2>: '{primer_h2.text.strip()}'\")\n",
    "\n",
    "primer_parrafo_destacado = soup.find('p', class_='texto-destacado')\n",
    "print(f\"   🎨 Primer párrafo destacado: '{primer_parrafo_destacado.text.strip()}'\")\n",
    "\n",
    "elemento_con_id = soup.find(id='titulo-principal')\n",
    "print(f\"   🆔 Elemento con ID 'titulo-principal': '{elemento_con_id.text.strip()}'\")\n",
    "\n",
    "# 2. find_all() - Para múltiples elementos\n",
    "print(\"\\n2️⃣ Método find_all() - Encuentra TODOS los elementos que coincidan:\")\n",
    "print(\"─\" * 50)\n",
    "\n",
    "todos_los_h2 = soup.find_all('h2')\n",
    "print(f\"   📝 Total de <h2> encontrados: {len(todos_los_h2)}\")\n",
    "for i, h2 in enumerate(todos_los_h2, 1):\n",
    "    print(f\"      H2 #{i}: '{h2.text.strip()}'\")\n",
    "\n",
    "todos_los_parrafos = soup.find_all('p')\n",
    "print(f\"\\n   📄 Total de párrafos <p>: {len(todos_los_parrafos)}\")\n",
    "for i, p in enumerate(todos_los_parrafos, 1):\n",
    "    texto = p.text.strip()[:40] + '...' if len(p.text.strip()) > 40 else p.text.strip()\n",
    "    print(f\"      Párrafo #{i}: '{texto}'\")\n",
    "\n",
    "# 3. Búsqueda por múltiples clases\n",
    "print(\"\\n3️⃣ Búsqueda por clase específica:\")\n",
    "print(\"─\" * 50)\n",
    "\n",
    "elementos_contenido = soup.find_all(class_='contenido')\n",
    "print(f\"   🎨 Elementos con clase 'contenido': {len(elementos_contenido)}\")\n",
    "for i, elem in enumerate(elementos_contenido, 1):\n",
    "    id_elem = elem.get('id', 'sin ID')\n",
    "    print(f\"      Elemento #{i}: <{elem.name}> con ID '{id_elem}'\")\n",
    "\n",
    "# 4. Búsqueda por atributos personalizados\n",
    "print(\"\\n4️⃣ Búsqueda por atributos data-* (muy útil):\")\n",
    "print(\"─\" * 50)\n",
    "\n",
    "articulos_con_data = soup.find_all('article', attrs={'data-tema': True})\n",
    "print(f\"   📊 Articles con atributo data-tema: {len(articulos_con_data)}\")\n",
    "for articulo in articulos_con_data:\n",
    "    tema = articulo.get('data-tema')\n",
    "    titulo = articulo.find('h3').text.strip()\n",
    "    dificultad = articulo.find('span', class_='dificultad').text.strip()\n",
    "    print(f\"      📋 Tema: {tema} | Título: '{titulo}' | {dificultad}\")\n",
    "\n",
    "# 5. Navegación del árbol DOM\n",
    "print(\"\\n5️⃣ Navegación del árbol DOM (Relaciones familiares):\")\n",
    "print(\"─\" * 50)\n",
    "\n",
    "seccion1 = soup.find('section', id='seccion1')\n",
    "print(f\"   👨‍👧‍👦 Padre de seccion1: <{seccion1.parent.name}>\")\n",
    "print(f\"   👶 Hijos directos de seccion1:\")\n",
    "for child in seccion1.children:\n",
    "    if child.name:  # Solo elementos, no texto\n",
    "        texto_preview = child.text.strip()[:30] + '...' if len(child.text.strip()) > 30 else child.text.strip()\n",
    "        print(f\"      • <{child.name}>: '{texto_preview}'\")\n",
    "\n",
    "# 6. Hermanos (elementos al mismo nivel)\n",
    "primer_h2 = soup.find('h2')\n",
    "siguiente_hermano = primer_h2.find_next_sibling()\n",
    "if siguiente_hermano:\n",
    "    print(f\"   👫 Siguiente hermano de primer H2: <{siguiente_hermano.name}>\")\n",
    "\n",
    "# 7. Extracción de atributos específicos\n",
    "print(\"\\n6️⃣ Extracción de atributos específicos:\")\n",
    "print(\"─\" * 50)\n",
    "\n",
    "todos_los_enlaces = soup.find_all('a')\n",
    "print(f\"   🔗 Total de enlaces encontrados: {len(todos_los_enlaces)}\")\n",
    "for i, enlace in enumerate(todos_los_enlaces, 1):\n",
    "    href = enlace.get('href', 'Sin href')\n",
    "    texto = enlace.text.strip() or '[Sin texto]'\n",
    "    target = enlace.get('target', 'Misma ventana')\n",
    "    print(f\"      Enlace #{i}: '{texto}' → {href} ({target})\")\n",
    "\n",
    "# 8. Búsqueda con expresiones regulares\n",
    "print(\"\\n7️⃣ Búsqueda avanzada con expresiones regulares:\")\n",
    "print(\"─\" * 50)\n",
    "\n",
    "import re\n",
    "elementos_con_data = soup.find_all(attrs={'data-tema': re.compile(r'.*')})\n",
    "print(f\"   🔍 Elementos con cualquier atributo data-tema: {len(elementos_con_data)}\")\n",
    "\n",
    "clases_que_contienen_contenido = soup.find_all(class_=re.compile(r'contenido'))\n",
    "print(f\"   🎨 Elementos con clases que contienen 'contenido': {len(clases_que_contienen_contenido)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Selectores CSS: Tu Arma Secreta 🎨\n",
    "\n",
    "Los selectores CSS son una forma muy poderosa y elegante de encontrar elementos. Son especialmente útiles cuando ya conoces CSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🎯 SELECTORES CSS: PRECISIÓN Y ELEGANCIA\\n\")\n",
    "print(\"═\" * 70)\n",
    "\n",
    "# Ejemplos completos de selectores CSS\n",
    "selectores_css = [\n",
    "    # Selectores básicos\n",
    "    ('h1', '🏷️ Seleccionar todos los elementos h1'),\n",
    "    ('p', '📝 Todos los párrafos'),\n",
    "    \n",
    "    # Selectores por ID y clase\n",
    "    ('#titulo-principal', '🆔 Elemento con ID específico'),\n",
    "    ('.contenido', '🎨 Elementos con clase contenido'),\n",
    "    ('.texto-destacado', '⭐ Elementos con clase texto-destacado'),\n",
    "    \n",
    "    # Selectores combinados\n",
    "    ('section.contenido', '📦 Elementos section CON clase contenido'),\n",
    "    ('h2.titulo-grande', '🎯 H2 con clase específica'),\n",
    "    ('p.texto-destacado', '📝 Párrafos destacados'),\n",
    "    \n",
    "    # Selectores de descendientes\n",
    "    ('nav ul li', '🗂️ Li dentro de ul dentro de nav'),\n",
    "    ('section article h3', '📋 H3 dentro de article dentro de section'),\n",
    "    ('div.conceptos-grid article', '🎲 Articles dentro de div con clase específica'),\n",
    "    \n",
    "    # Selectores de hijos directos\n",
    "    ('section > h2', '👶 H2 hijos DIRECTOS de section'),\n",
    "    ('ul > li', '📋 Li hijos directos de ul'),\n",
    "    \n",
    "    # Selectores de atributos\n",
    "    ('a[href]', '🔗 Enlaces que tienen atributo href'),\n",
    "    ('article[data-tema]', '📊 Articles con atributo data-tema'),\n",
    "    ('article[data-tema=\"html\"]', '🎯 Article con data-tema=\"html\" exacto'),\n",
    "    ('span[data-nivel^=\"fa\"]', '🔍 Span con data-nivel que empiece con \"fa\"'),\n",
    "    ('a[href*=\"scrape\"]', '🌐 Enlaces cuyo href contenga \"scrape\"'),\n",
    "    \n",
    "    # Pseudo-selectores\n",
    "    ('li:first-child', '🥇 Primer li hijo'),\n",
    "    ('li:last-child', '🏁 Último li hijo'),\n",
    "    ('tr:nth-child(2)', '2️⃣ Segunda fila de tabla'),\n",
    "    \n",
    "    # Selectores de hermanos\n",
    "    ('h2 + p', '👫 Párrafo inmediatamente después de h2'),\n",
    "    ('h2 ~ p', '👥 Todos los párrafos hermanos después de h2'),\n",
    "    \n",
    "    # Selectores múltiples\n",
    "    ('h1, h2, h3', '📊 Todos los títulos principales'),\n",
    "    ('.contenido, .importante', '🎨 Elementos con cualquiera de estas clases'),\n",
    "]\n",
    "\n",
    "# Ejecutar cada selector y mostrar resultados\n",
    "for selector, descripcion in selectores_css:\n",
    "    try:\n",
    "        elementos = soup.select(selector)\n",
    "        print(f\"\\n{descripcion}\")\n",
    "        print(f\"   🎯 Selector: {selector}\")\n",
    "        print(f\"   📊 Elementos encontrados: {len(elementos)}\")\n",
    "        \n",
    "        if elementos:\n",
    "            # Mostrar información del primer resultado\n",
    "            primer_elemento = elementos[0]\n",
    "            texto = primer_elemento.text.strip()[:50]\n",
    "            if len(primer_elemento.text.strip()) > 50:\n",
    "                texto += '...'\n",
    "            \n",
    "            # Información adicional del elemento\n",
    "            tag_name = primer_elemento.name\n",
    "            elem_id = primer_elemento.get('id', '')\n",
    "            elem_class = primer_elemento.get('class', [])\n",
    "            \n",
    "            print(f\"   🔍 Primer resultado: <{tag_name}>\")\n",
    "            if elem_id:\n",
    "                print(f\"      🆔 ID: {elem_id}\")\n",
    "            if elem_class:\n",
    "                print(f\"      🎨 Clases: {', '.join(elem_class)}\")\n",
    "            print(f\"      📝 Texto: '{texto}'\")\n",
    "            \n",
    "            # Si hay múltiples resultados, mostrar resumen\n",
    "            if len(elementos) > 1:\n",
    "                tags_encontrados = [elem.name for elem in elementos]\n",
    "                from collections import Counter\n",
    "                conteo_tags = Counter(tags_encontrados)\n",
    "                print(f\"      📈 Distribución: {dict(conteo_tags)}\")\n",
    "        else:\n",
    "            print(f\"   ❌ No se encontraron elementos\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️ Error con selector '{selector}': {e}\")\n",
    "\n",
    "print(\"\\n\" + \"═\" * 70)\n",
    "print(\"💡 CONSEJO: Los selectores CSS son muy poderosos y expresivos.\")\n",
    "print(\"💡 Practica combinándolos para encontrar exactamente lo que necesitas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Extracción de Datos de Tablas 📊\n",
    "\n",
    "Las tablas HTML son una mina de oro para los web scrapers. Contienen datos estructurados que son perfectos para análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📊 EXTRACCIÓN PROFESIONAL DE DATOS DE TABLA\\n\")\n",
    "print(\"═\" * 70)\n",
    "\n",
    "# Encontrar la tabla en nuestro HTML\n",
    "tabla = soup.find('table', class_='tabla-datos')\n",
    "\n",
    "if tabla:\n",
    "    print(\"✅ Tabla encontrada exitosamente\\n\")\n",
    "    \n",
    "    # PASO 1: Extraer encabezados de la tabla\n",
    "    print(\"1️⃣ EXTRAYENDO ENCABEZADOS:\")\n",
    "    print(\"─\" * 30)\n",
    "    \n",
    "    thead = tabla.find('thead')\n",
    "    if thead:\n",
    "        encabezados_elementos = thead.find_all('th')\n",
    "        encabezados = [th.text.strip() for th in encabezados_elementos]\n",
    "        print(f\"   📋 Encabezados encontrados: {len(encabezados)}\")\n",
    "        print(f\"   🏷️ Columnas: {' | '.join(encabezados)}\")\n",
    "    else:\n",
    "        print(\"   ❌ No se encontró thead\")\n",
    "        encabezados = []\n",
    "    \n",
    "    # PASO 2: Extraer filas de datos\n",
    "    print(\"\\n2️⃣ EXTRAYENDO FILAS DE DATOS:\")\n",
    "    print(\"─\" * 30)\n",
    "    \n",
    "    tbody = tabla.find('tbody')\n",
    "    if tbody:\n",
    "        filas_elementos = tbody.find_all('tr')\n",
    "        print(f\"   📊 Filas de datos encontradas: {len(filas_elementos)}\")\n",
    "        \n",
    "        datos_tabla = []\n",
    "        for i, fila_elemento in enumerate(filas_elementos, 1):\n",
    "            # Extraer celdas de datos (td)\n",
    "            celdas_elementos = fila_elemento.find_all('td')\n",
    "            celdas_datos = [td.text.strip() for td in celdas_elementos]\n",
    "            \n",
    "            # Extraer atributos adicionales\n",
    "            fila_id = fila_elemento.get('data-id', f'fila-{i}')\n",
    "            \n",
    "            # Almacenar datos de la fila\n",
    "            fila_completa = {\n",
    "                'id': fila_id,\n",
    "                'datos': celdas_datos,\n",
    "                'num_celdas': len(celdas_datos)\n",
    "            }\n",
    "            datos_tabla.append(fila_completa)\n",
    "            \n",
    "            # Mostrar fila procesada\n",
    "            print(f\"   📝 Fila {i} (ID: {fila_id}): {' | '.join(celdas_datos)}\")\n",
    "    else:\n",
    "        print(\"   ❌ No se encontró tbody\")\n",
    "        datos_tabla = []\n",
    "    \n",
    "    # PASO 3: Convertir a estructura más útil (diccionarios)\n",
    "    if encabezados and datos_tabla:\n",
    "        print(\"\\n3️⃣ CONVIRTIENDO A ESTRUCTURA DE DATOS:\")\n",
    "        print(\"─\" * 30)\n",
    "        \n",
    "        datos_estructurados = []\n",
    "        for fila in datos_tabla:\n",
    "            if len(fila['datos']) == len(encabezados):\n",
    "                # Crear diccionario combinando encabezados con datos\n",
    "                fila_dict = dict(zip(encabezados, fila['datos']))\n",
    "                fila_dict['_id'] = fila['id']  # Preservar ID original\n",
    "                datos_estructurados.append(fila_dict)\n",
    "                \n",
    "                print(f\"   📋 Registro: {fila_dict}\")\n",
    "            else:\n",
    "                print(f\"   ⚠️ Fila {fila['id']}: Desajuste de columnas ({len(fila['datos'])} vs {len(encabezados)})\")\n",
    "    \n",
    "    # PASO 4: Análisis adicional de la tabla\n",
    "    print(\"\\n4️⃣ ANÁLISIS ADICIONAL DE LA TABLA:\")\n",
    "    print(\"─\" * 30)\n",
    "    \n",
    "    # Extraer información de clases CSS en celdas\n",
    "    celdas_con_clase = tabla.find_all('td', class_=True)\n",
    "    if celdas_con_clase:\n",
    "        print(f\"   🎨 Celdas con clases CSS: {len(celdas_con_clase)}\")\n",
    "        for celda in celdas_con_clase:\n",
    "            clases = celda.get('class', [])\n",
    "            texto = celda.text.strip()\n",
    "            print(f\"      • '{texto}' → Clases: {', '.join(clases)}\")\n",
    "    \n",
    "    # Estadísticas finales\n",
    "    print(\"\\n📈 ESTADÍSTICAS DE LA TABLA:\")\n",
    "    print(\"─\" * 30)\n",
    "    print(f\"   🔢 Total de columnas: {len(encabezados)}\")\n",
    "    print(f\"   📊 Total de filas de datos: {len(datos_tabla)}\")\n",
    "    print(f\"   📋 Registros estructurados: {len(datos_estructurados) if 'datos_estructurados' in locals() else 0}\")\n",
    "    print(f\"   🎨 Celdas con estilos: {len(celdas_con_clase)}\")\n",
    "    \n",
    "    # BONUS: Crear DataFrame si pandas está disponible\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        if 'datos_estructurados' in locals() and datos_estructurados:\n",
    "            df = pd.DataFrame(datos_estructurados)\n",
    "            print(\"\\n📊 DATAFRAME DE PANDAS CREADO:\")\n",
    "            print(\"─\" * 30)\n",
    "            print(df)\n",
    "            print(f\"\\n   💾 Forma del DataFrame: {df.shape}\")\n",
    "            print(f\"   📝 Columnas: {list(df.columns)}\")\n",
    "    except ImportError:\n",
    "        print(\"\\n💡 SUGERENCIA: Instala pandas con 'pip install pandas' para análisis avanzado\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No se encontró ninguna tabla en el HTML\")\n",
    "\n",
    "print(\"\\n\" + \"═\" * 70)\n",
    "print(\"🏆 ¡Extracción de tabla completada exitosamente!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Manejo de Errores y Scraping Robusto 🛡️\n",
    "\n",
    "Un buen web scraper debe manejar errores graciosamente. Las conexiones fallan, los sitios cambian, y los servidores se caen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "class ScraperRobusto:\n",
    "    \"\"\"\n",
    "    Clase para web scraping robusto con manejo de errores,\n",
    "    reintentos, rate limiting y mejores prácticas.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_reintentos=3, delay_base=1, timeout=10):\n",
    "        self.max_reintentos = max_reintentos\n",
    "        self.delay_base = delay_base\n",
    "        self.timeout = timeout\n",
    "        self.session = self._crear_session_con_reintentos()\n",
    "        \n",
    "        # Headers realistas\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "            'Accept-Language': 'es-ES,es;q=0.8,en-US;q=0.5,en;q=0.3',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Connection': 'keep-alive',\n",
    "        }\n",
    "        \n",
    "        # Estadísticas\n",
    "        self.stats = {\n",
    "            'solicitudes_exitosas': 0,\n",
    "            'errores_totales': 0,\n",
    "            'reintentos_usados': 0,\n",
    "            'tiempo_total': 0\n",
    "        }\n",
    "    \n",
    "    def _crear_session_con_reintentos(self):\n",
    "        \"\"\"Crear sesión con estrategia de reintentos automáticos\"\"\"\n",
    "        session = requests.Session()\n",
    "        \n",
    "        # Configurar estrategia de reintentos\n",
    "        retry_strategy = Retry(\n",
    "            total=self.max_reintentos,\n",
    "            backoff_factor=1,\n",
    "            status_forcelist=[429, 500, 502, 503, 504],\n",
    "            allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"]\n",
    "        )\n",
    "        \n",
    "        adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "        session.mount(\"http://\", adapter)\n",
    "        session.mount(\"https://\", adapter)\n",
    "        \n",
    "        return session\n",
    "    \n",
    "    def _delay_aleatorio(self, base_delay=None):\n",
    "        \"\"\"Implementar delay aleatorio para parecer más humano\"\"\"\n",
    "        if base_delay is None:\n",
    "            base_delay = self.delay_base\n",
    "        \n",
    "        # Delay aleatorio entre base_delay y base_delay * 2\n",
    "        delay = random.uniform(base_delay, base_delay * 2)\n",
    "        print(f\"   ⏳ Esperando {delay:.2f} segundos...\")\n",
    "        time.sleep(delay)\n",
    "    \n",
    "    def scrape_url(self, url, parser='html.parser'):\n",
    "        \"\"\"\n",
    "        Realizar scraping robusto de una URL\n",
    "        \n",
    "        Args:\n",
    "            url: URL a scrapear\n",
    "            parser: Parser de BeautifulSoup a usar\n",
    "            \n",
    "        Returns:\n",
    "            BeautifulSoup object o None si falla\n",
    "        \"\"\"\n",
    "        print(f\"\\n🚀 INICIANDO SCRAPING ROBUSTO\")\n",
    "        print(f\"🎯 URL: {url}\")\n",
    "        print(f\"⚙️ Parser: {parser}\")\n",
    "        print(\"─\" * 50)\n",
    "        \n",
    "        inicio_tiempo = time.time()\n",
    "        \n",
    "        for intento in range(self.max_reintentos + 1):\n",
    "            try:\n",
    "                print(f\"\\n🔄 Intento {intento + 1} de {self.max_reintentos + 1}\")\n",
    "                \n",
    "                # Hacer la solicitud\n",
    "                response = self.session.get(\n",
    "                    url, \n",
    "                    headers=self.headers, \n",
    "                    timeout=self.timeout\n",
    "                )\n",
    "                \n",
    "                # Verificar código de estado\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # Verificar que tenemos contenido HTML\n",
    "                content_type = response.headers.get('content-type', '').lower()\n",
    "                if 'text/html' not in content_type:\n",
    "                    raise ValueError(f\"Contenido no HTML: {content_type}\")\n",
    "                \n",
    "                # Parsear con Beautiful Soup\n",
    "                soup = BeautifulSoup(response.content, parser)\n",
    "                \n",
    "                # Validar que el HTML es válido\n",
    "                if not soup.find():\n",
    "                    raise ValueError(\"HTML vacío o inválido\")\n",
    "                \n",
    "                # Éxito!\n",
    "                tiempo_transcurrido = time.time() - inicio_tiempo\n",
    "                self.stats['solicitudes_exitosas'] += 1\n",
    "                self.stats['tiempo_total'] += tiempo_transcurrido\n",
    "                \n",
    "                print(f\"✅ ¡Scraping exitoso!\")\n",
    "                print(f\"📊 Código de estado: {response.status_code}\")\n",
    "                print(f\"📄 Tamaño del HTML: {len(response.content):,} bytes\")\n",
    "                print(f\"⏱️ Tiempo total: {tiempo_transcurrido:.2f} segundos\")\n",
    "                print(f\"🌐 Servidor: {urlparse(url).netloc}\")\n",
    "                \n",
    "                # Información adicional del HTML\n",
    "                title = soup.find('title')\n",
    "                if title:\n",
    "                    print(f\"📋 Título: {title.text.strip()}\")\n",
    "                \n",
    "                return soup\n",
    "                \n",
    "            except requests.exceptions.Timeout:\n",
    "                print(f\"⏱️ Timeout en intento {intento + 1}\")\n",
    "                self.stats['errores_totales'] += 1\n",
    "                \n",
    "            except requests.exceptions.ConnectionError:\n",
    "                print(f\"🔌 Error de conexión en intento {intento + 1}\")\n",
    "                self.stats['errores_totales'] += 1\n",
    "                \n",
    "            except requests.exceptions.HTTPError as e:\n",
    "                status_code = e.response.status_code\n",
    "                print(f\"❌ Error HTTP {status_code} en intento {intento + 1}\")\n",
    "                self.stats['errores_totales'] += 1\n",
    "                \n",
    "                # Algunos errores no valen la pena reintentar\n",
    "                if status_code in [404, 403, 401]:\n",
    "                    print(f\"   🚫 Error {status_code} - No reintentando\")\n",
    "                    break\n",
    "                    \n",
    "            except ValueError as e:\n",
    "                print(f\"❗ Error de validación: {e}\")\n",
    "                self.stats['errores_totales'] += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"💥 Error inesperado: {type(e).__name__}: {e}\")\n",
    "                self.stats['errores_totales'] += 1\n",
    "            \n",
    "            # Si no es el último intento, esperar antes de reintentar\n",
    "            if intento < self.max_reintentos:\n",
    "                self.stats['reintentos_usados'] += 1\n",
    "                delay = self.delay_base * (2 ** intento)  # Backoff exponencial\n",
    "                print(f\"⏳ Esperando {delay} segundos antes del siguiente intento...\")\n",
    "                time.sleep(delay)\n",
    "        \n",
    "        # Si llegamos aquí, todos los intentos fallaron\n",
    "        tiempo_total = time.time() - inicio_tiempo\n",
    "        self.stats['tiempo_total'] += tiempo_total\n",
    "        \n",
    "        print(f\"\\n💥 SCRAPING FALLIDO después de {self.max_reintentos + 1} intentos\")\n",
    "        print(f\"⏱️ Tiempo total gastado: {tiempo_total:.2f} segundos\")\n",
    "        return None\n",
    "    \n",
    "    def mostrar_estadisticas(self):\n",
    "        \"\"\"Mostrar estadísticas del scraper\"\"\"\n",
    "        print(\"\\n📊 ESTADÍSTICAS DEL SCRAPER ROBUSTO\")\n",
    "        print(\"═\" * 50)\n",
    "        print(f\"✅ Solicitudes exitosas: {self.stats['solicitudes_exitosas']}\")\n",
    "        print(f\"❌ Errores totales: {self.stats['errores_totales']}\")\n",
    "        print(f\"🔄 Reintentos usados: {self.stats['reintentos_usados']}\")\n",
    "        print(f\"⏱️ Tiempo total: {self.stats['tiempo_total']:.2f} segundos\")\n",
    "        \n",
    "        if self.stats['solicitudes_exitosas'] > 0:\n",
    "            promedio = self.stats['tiempo_total'] / self.stats['solicitudes_exitosas']\n",
    "            print(f\"📈 Tiempo promedio por solicitud: {promedio:.2f} segundos\")\n",
    "\n",
    "# Demostración del scraper robusto\n",
    "print(\"🛡️ DEMOSTRACIÓN DE SCRAPER ROBUSTO\\n\")\n",
    "print(\"═\" * 60)\n",
    "\n",
    "# Crear instancia del scraper\n",
    "scraper = ScraperRobusto(max_reintentos=2, delay_base=0.5)\n",
    "\n",
    "# URLs para probar\n",
    "urls_prueba = [\n",
    "    'http://quotes.toscrape.com/',  # URL válida\n",
    "    'http://quotes.toscrape.com/page/999/',  # URL que dará 404\n",
    "]\n",
    "\n",
    "resultados = []\n",
    "for url in urls_prueba:\n",
    "    resultado = scraper.scrape_url(url)\n",
    "    resultados.append({\n",
    "        'url': url,\n",
    "        'exitoso': resultado is not None,\n",
    "        'soup': resultado\n",
    "    })\n",
    "\n",
    "# Mostrar estadísticas finales\n",
    "scraper.mostrar_estadisticas()\n",
    "\n",
    "# Resumen de resultados\n",
    "print(\"\\n📋 RESUMEN DE RESULTADOS\")\n",
    "print(\"═\" * 50)\n",
    "for i, resultado in enumerate(resultados, 1):\n",
    "    status = \"✅ Éxito\" if resultado['exitoso'] else \"❌ Fallo\"\n",
    "    print(f\"{i}. {resultado['url']} → {status}\")\n",
    "\n",
    "print(\"\\n🎯 El manejo robusto de errores es ESENCIAL para scrapers en producción.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Ejercicio Práctico Avanzado: Tu Turno 💪\n",
    "\n",
    "¡Hora de poner en práctica todo lo aprendido! Completa este ejercicio desafiante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EJERCICIO AVANZADO: Scraper de Noticias Tecnológicas\n",
    "# Objetivo: Crear un scraper completo que extraiga múltiples tipos de datos\n",
    "\n",
    "# HTML de ejemplo: Portal de noticias tech\n",
    "html_noticias = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"es\">\n",
    "<head>\n",
    "    <title>TechNews - Las Últimas Noticias de Tecnología</title>\n",
    "    <meta name=\"description\" content=\"Portal de noticias tecnológicas\">\n",
    "</head>\n",
    "<body>\n",
    "    <header>\n",
    "        <h1>🚀 TechNews</h1>\n",
    "        <nav class=\"main-nav\">\n",
    "            <ul>\n",
    "                <li><a href=\"/ia\">Inteligencia Artificial</a></li>\n",
    "                <li><a href=\"/web\">Desarrollo Web</a></li>\n",
    "                <li><a href=\"/mobile\">Apps Móviles</a></li>\n",
    "            </ul>\n",
    "        </nav>\n",
    "    </header>\n",
    "    \n",
    "    <main>\n",
    "        <section id=\"noticias-destacadas\">\n",
    "            <h2>📰 Noticias Destacadas</h2>\n",
    "            \n",
    "            <article class=\"noticia destacada\" data-id=\"1\" data-categoria=\"ia\">\n",
    "                <header class=\"noticia-header\">\n",
    "                    <h3 class=\"titulo\">ChatGPT revoluciona la programación en 2024</h3>\n",
    "                    <div class=\"metadatos\">\n",
    "                        <span class=\"autor\" data-autor-id=\"123\">María García</span>\n",
    "                        <time class=\"fecha\" datetime=\"2024-01-15T10:30:00\">15 de Enero, 2024</time>\n",
    "                        <span class=\"categoria\">🤖 IA</span>\n",
    "                        <div class=\"estadisticas\">\n",
    "                            <span class=\"vistas\" data-count=\"1250\">1,250 vistas</span>\n",
    "                            <span class=\"comentarios\" data-count=\"45\">45 comentarios</span>\n",
    "                        </div>\n",
    "                    </div>\n",
    "                </header>\n",
    "                <div class=\"contenido\">\n",
    "                    <p class=\"resumen\">La nueva versión de ChatGPT introduce capacidades avanzadas para generar código de alta calidad, transformando la manera en que los desarrolladores trabajan.</p>\n",
    "                    <div class=\"tags\">\n",
    "                        <span class=\"tag\">IA</span>\n",
    "                        <span class=\"tag\">programación</span>\n",
    "                        <span class=\"tag\">productividad</span>\n",
    "                    </div>\n",
    "                </div>\n",
    "                <footer class=\"noticia-footer\">\n",
    "                    <a href=\"/noticia/1\" class=\"leer-mas\">Leer artículo completo</a>\n",
    "                    <div class=\"acciones\">\n",
    "                        <button class=\"like\" data-likes=\"89\">❤️ 89</button>\n",
    "                        <button class=\"share\">🔗 Compartir</button>\n",
    "                    </div>\n",
    "                </footer>\n",
    "            </article>\n",
    "            \n",
    "            <article class=\"noticia\" data-id=\"2\" data-categoria=\"web\">\n",
    "                <header class=\"noticia-header\">\n",
    "                    <h3 class=\"titulo\">React 19: Nuevas características que cambiarán todo</h3>\n",
    "                    <div class=\"metadatos\">\n",
    "                        <span class=\"autor\" data-autor-id=\"456\">Carlos Ruiz</span>\n",
    "                        <time class=\"fecha\" datetime=\"2024-01-14T16:45:00\">14 de Enero, 2024</time>\n",
    "                        <span class=\"categoria\">💻 Desarrollo Web</span>\n",
    "                        <div class=\"estadisticas\">\n",
    "                            <span class=\"vistas\" data-count=\"890\">890 vistas</span>\n",
    "                            <span class=\"comentarios\" data-count=\"23\">23 comentarios</span>\n",
    "                        </div>\n",
    "                    </div>\n",
    "                </header>\n",
    "                <div class=\"contenido\">\n",
    "                    <p class=\"resumen\">React 19 incluye mejoras significativas en rendimiento y nuevas APIs que simplifican el desarrollo de aplicaciones complejas.</p>\n",
    "                    <div class=\"tags\">\n",
    "                        <span class=\"tag\">React</span>\n",
    "                        <span class=\"tag\">JavaScript</span>\n",
    "                        <span class=\"tag\">frontend</span>\n",
    "                    </div>\n",
    "                </div>\n",
    "                <footer class=\"noticia-footer\">\n",
    "                    <a href=\"/noticia/2\" class=\"leer-mas\">Leer artículo completo</a>\n",
    "                    <div class=\"acciones\">\n",
    "                        <button class=\"like\" data-likes=\"156\">❤️ 156</button>\n",
    "                        <button class=\"share\">🔗 Compartir</button>\n",
    "                    </div>\n",
    "                </footer>\n",
    "            </article>\n",
    "            \n",
    "            <article class=\"noticia\" data-id=\"3\" data-categoria=\"mobile\">\n",
    "                <header class=\"noticia-header\">\n",
    "                    <h3 class=\"titulo\">Flutter vs React Native: Comparativa 2024</h3>\n",
    "                    <div class=\"metadatos\">\n",
    "                        <span class=\"autor\" data-autor-id=\"789\">Ana López</span>\n",
    "                        <time class=\"fecha\" datetime=\"2024-01-13T09:15:00\">13 de Enero, 2024</time>\n",
    "                        <span class=\"categoria\">📱 Apps Móviles</span>\n",
    "                        <div class=\"estadisticas\">\n",
    "                            <span class=\"vistas\" data-count=\"2100\">2,100 vistas</span>\n",
    "                            <span class=\"comentarios\" data-count=\"78\">78 comentarios</span>\n",
    "                        </div>\n",
    "                    </div>\n",
    "                </header>\n",
    "                <div class=\"contenido\">\n",
    "                    <p class=\"resumen\">Análisis completo de las dos tecnologías más populares para desarrollo móvil multiplataforma, con ejemplos prácticos y casos de uso.</p>\n",
    "                    <div class=\"tags\">\n",
    "                        <span class=\"tag\">Flutter</span>\n",
    "                        <span class=\"tag\">React Native</span>\n",
    "                        <span class=\"tag\">mobile</span>\n",
    "                    </div>\n",
    "                </div>\n",
    "                <footer class=\"noticia-footer\">\n",
    "                    <a href=\"/noticia/3\" class=\"leer-mas\">Leer artículo completo</a>\n",
    "                    <div class=\"acciones\">\n",
    "                        <button class=\"like\" data-likes=\"203\">❤️ 203</button>\n",
    "                        <button class=\"share\">🔗 Compartir</button>\n",
    "                    </div>\n",
    "                </footer>\n",
    "            </article>\n",
    "        </section>\n",
    "        \n",
    "        <aside id=\"trending\">\n",
    "            <h2>🔥 Trending Topics</h2>\n",
    "            <ul class=\"trending-list\">\n",
    "                <li data-trend=\"1\">#IA <span class=\"count\">(1,250 menciones)</span></li>\n",
    "                <li data-trend=\"2\">#React19 <span class=\"count\">(890 menciones)</span></li>\n",
    "                <li data-trend=\"3\">#Python <span class=\"count\">(650 menciones)</span></li>\n",
    "            </ul>\n",
    "        </aside>\n",
    "    </main>\n",
    "    \n",
    "    <footer>\n",
    "        <p>&copy; 2024 TechNews - Mantente actualizado</p>\n",
    "        <div class=\"stats-globales\">\n",
    "            <span>👥 15,430 lectores activos</span>\n",
    "            <span>📰 1,250 artículos publicados</span>\n",
    "        </div>\n",
    "    </footer>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "print(\"🏋️ EJERCICIO AVANZADO: SCRAPER DE NOTICIAS TECH\\n\")\n",
    "print(\"═\" * 60)\n",
    "print(\"Instrucciones:\")\n",
    "print(\"1. Extrae TODA la información de cada artículo\")\n",
    "print(\"2. Procesa los metadatos y estadísticas\")\n",
    "print(\"3. Extrae trending topics\")\n",
    "print(\"4. Calcula estadísticas avanzadas\")\n",
    "print(\"5. Estructura todo en formato JSON\")\n",
    "\n",
    "def scraper_noticias_avanzado(html):\n",
    "    \"\"\"\n",
    "    Scraper avanzado para portal de noticias.\n",
    "    Debe extraer TODA la información disponible.\n",
    "    \"\"\"\n",
    "    # PARSEAR HTML\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # ESTRUCTURA DE DATOS COMPLETA\n",
    "    portal_data = {\n",
    "        'info_portal': {},\n",
    "        'navegacion': [],\n",
    "        'noticias': [],\n",
    "        'trending': [],\n",
    "        'estadisticas_globales': {},\n",
    "        'resumen_scraping': {}\n",
    "    }\n",
    "    \n",
    "    # 1. INFORMACIÓN DEL PORTAL\n",
    "    title = soup.find('title')\n",
    "    description = soup.find('meta', attrs={'name': 'description'})\n",
    "    \n",
    "    portal_data['info_portal'] = {\n",
    "        'titulo': title.text if title else 'Sin título',\n",
    "        'descripcion': description.get('content', 'Sin descripción') if description else 'Sin descripción',\n",
    "        'nombre_sitio': soup.find('h1').text if soup.find('h1') else 'Desconocido'\n",
    "    }\n",
    "    \n",
    "    # 2. NAVEGACIÓN\n",
    "    nav = soup.find('nav', class_='main-nav')\n",
    "    if nav:\n",
    "        enlaces_nav = nav.find_all('a')\n",
    "        portal_data['navegacion'] = [\n",
    "            {\n",
    "                'texto': enlace.text.strip(),\n",
    "                'href': enlace.get('href', ''),\n",
    "                'seccion': enlace.get('href', '').replace('/', '')\n",
    "            } for enlace in enlaces_nav\n",
    "        ]\n",
    "    \n",
    "    # 3. EXTRACCIÓN COMPLETA DE NOTICIAS\n",
    "    noticias = soup.find_all('article', class_='noticia')\n",
    "    \n",
    "    for noticia in noticias:\n",
    "        # Datos básicos\n",
    "        noticia_data = {\n",
    "            'id': noticia.get('data-id'),\n",
    "            'categoria': noticia.get('data-categoria'),\n",
    "            'es_destacada': 'destacada' in noticia.get('class', [])\n",
    "        }\n",
    "        \n",
    "        # Título\n",
    "        titulo_elem = noticia.find('h3', class_='titulo')\n",
    "        noticia_data['titulo'] = titulo_elem.text.strip() if titulo_elem else 'Sin título'\n",
    "        \n",
    "        # Metadatos del autor\n",
    "        autor_elem = noticia.find('span', class_='autor')\n",
    "        if autor_elem:\n",
    "            noticia_data['autor'] = {\n",
    "                'nombre': autor_elem.text.strip(),\n",
    "                'id': autor_elem.get('data-autor-id')\n",
    "            }\n",
    "        \n",
    "        # Fecha\n",
    "        fecha_elem = noticia.find('time', class_='fecha')\n",
    "        if fecha_elem:\n",
    "            noticia_data['fecha'] = {\n",
    "                'texto': fecha_elem.text.strip(),\n",
    "                'datetime': fecha_elem.get('datetime'),\n",
    "                'timestamp': fecha_elem.get('datetime')\n",
    "            }\n",
    "        \n",
    "        # Categoría mostrada\n",
    "        categoria_elem = noticia.find('span', class_='categoria')\n",
    "        noticia_data['categoria_display'] = categoria_elem.text.strip() if categoria_elem else 'Sin categoría'\n",
    "        \n",
    "        # Estadísticas de engagement\n",
    "        vistas_elem = noticia.find('span', class_='vistas')\n",
    "        comentarios_elem = noticia.find('span', class_='comentarios')\n",
    "        likes_elem = noticia.find('button', class_='like')\n",
    "        \n",
    "        noticia_data['estadisticas'] = {\n",
    "            'vistas': {\n",
    "                'texto': vistas_elem.text.strip() if vistas_elem else '0',\n",
    "                'count': int(vistas_elem.get('data-count', 0)) if vistas_elem else 0\n",
    "            },\n",
    "            'comentarios': {\n",
    "                'texto': comentarios_elem.text.strip() if comentarios_elem else '0',\n",
    "                'count': int(comentarios_elem.get('data-count', 0)) if comentarios_elem else 0\n",
    "            },\n",
    "            'likes': {\n",
    "                'texto': likes_elem.text.strip() if likes_elem else '0',\n",
    "                'count': int(likes_elem.get('data-likes', 0)) if likes_elem else 0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Contenido\n",
    "        resumen_elem = noticia.find('p', class_='resumen')\n",
    "        noticia_data['resumen'] = resumen_elem.text.strip() if resumen_elem else 'Sin resumen'\n",
    "        \n",
    "        # Tags\n",
    "        tags_container = noticia.find('div', class_='tags')\n",
    "        if tags_container:\n",
    "            tags = [tag.text.strip() for tag in tags_container.find_all('span', class_='tag')]\n",
    "            noticia_data['tags'] = tags\n",
    "        else:\n",
    "            noticia_data['tags'] = []\n",
    "        \n",
    "        # Enlaces\n",
    "        leer_mas = noticia.find('a', class_='leer-mas')\n",
    "        noticia_data['enlace_completo'] = leer_mas.get('href') if leer_mas else None\n",
    "        \n",
    "        # Métricas calculadas\n",
    "        noticia_data['metricas_calculadas'] = {\n",
    "            'engagement_score': (\n",
    "                noticia_data['estadisticas']['likes']['count'] * 3 +\n",
    "                noticia_data['estadisticas']['comentarios']['count'] * 2 +\n",
    "                noticia_data['estadisticas']['vistas']['count'] * 0.01\n",
    "            ),\n",
    "            'palabras_titulo': len(noticia_data['titulo'].split()),\n",
    "            'palabras_resumen': len(noticia_data['resumen'].split()),\n",
    "            'num_tags': len(noticia_data['tags'])\n",
    "        }\n",
    "        \n",
    "        portal_data['noticias'].append(noticia_data)\n",
    "    \n",
    "    # 4. TRENDING TOPICS\n",
    "    trending_list = soup.find('ul', class_='trending-list')\n",
    "    if trending_list:\n",
    "        trending_items = trending_list.find_all('li')\n",
    "        for item in trending_items:\n",
    "            trend_data = {\n",
    "                'posicion': item.get('data-trend'),\n",
    "                'hashtag': item.text.split('(')[0].strip(),\n",
    "                'menciones_texto': item.find('span', class_='count').text.strip() if item.find('span', class_='count') else '0'\n",
    "            }\n",
    "            # Extraer número de menciones\n",
    "            import re\n",
    "            menciones_match = re.search(r'\\((\\d+[,\\d]*)', trend_data['menciones_texto'])\n",
    "            if menciones_match:\n",
    "                menciones_num = menciones_match.group(1).replace(',', '')\n",
    "                trend_data['menciones_count'] = int(menciones_num)\n",
    "            else:\n",
    "                trend_data['menciones_count'] = 0\n",
    "            \n",
    "            portal_data['trending'].append(trend_data)\n",
    "    \n",
    "    # 5. ESTADÍSTICAS GLOBALES\n",
    "    stats_footer = soup.find('div', class_='stats-globales')\n",
    "    if stats_footer:\n",
    "        stats_text = stats_footer.text\n",
    "        import re\n",
    "        \n",
    "        # Extraer números de las estadísticas\n",
    "        lectores_match = re.search(r'(\\d+[,\\d]*) lectores', stats_text)\n",
    "        articulos_match = re.search(r'(\\d+[,\\d]*) artículos', stats_text)\n",
    "        \n",
    "        portal_data['estadisticas_globales'] = {\n",
    "            'lectores_activos': int(lectores_match.group(1).replace(',', '')) if lectores_match else 0,\n",
    "            'articulos_publicados': int(articulos_match.group(1).replace(',', '')) if articulos_match else 0\n",
    "        }\n",
    "    \n",
    "    # 6. RESUMEN DEL SCRAPING\n",
    "    total_vistas = sum(n['estadisticas']['vistas']['count'] for n in portal_data['noticias'])\n",
    "    total_likes = sum(n['estadisticas']['likes']['count'] for n in portal_data['noticias'])\n",
    "    total_comentarios = sum(n['estadisticas']['comentarios']['count'] for n in portal_data['noticias'])\n",
    "    \n",
    "    todos_tags = []\n",
    "    for noticia in portal_data['noticias']:\n",
    "        todos_tags.extend(noticia['tags'])\n",
    "    \n",
    "    from collections import Counter\n",
    "    tags_mas_comunes = Counter(todos_tags).most_common(5)\n",
    "    \n",
    "    portal_data['resumen_scraping'] = {\n",
    "        'total_noticias': len(portal_data['noticias']),\n",
    "        'noticias_destacadas': len([n for n in portal_data['noticias'] if n['es_destacada']]),\n",
    "        'categorias_unicas': len(set(n['categoria'] for n in portal_data['noticias'])),\n",
    "        'autores_unicos': len(set(n['autor']['nombre'] for n in portal_data['noticias'] if 'autor' in n)),\n",
    "        'engagement_total': {\n",
    "            'vistas': total_vistas,\n",
    "            'likes': total_likes,\n",
    "            'comentarios': total_comentarios\n",
    "        },\n",
    "        'tags_mas_populares': tags_mas_comunes,\n",
    "        'fecha_scraping': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    \n",
    "    return portal_data\n",
    "\n",
    "# ¡EJECUTAR EL SCRAPER AVANZADO!\n",
    "print(\"\\n🚀 EJECUTANDO SCRAPER AVANZADO...\\n\")\n",
    "\n",
    "datos_extraidos = scraper_noticias_avanzado(html_noticias)\n",
    "\n",
    "# MOSTRAR RESULTADOS DE FORMA ORGANIZADA\n",
    "import json\n",
    "\n",
    "print(\"📊 RESULTADOS DEL SCRAPING AVANZADO\")\n",
    "print(\"═\" * 60)\n",
    "\n",
    "# Info del portal\n",
    "print(f\"\\n🌐 PORTAL: {datos_extraidos['info_portal']['nombre_sitio']}\")\n",
    "print(f\"📋 Título: {datos_extraidos['info_portal']['titulo']}\")\n",
    "print(f\"📝 Descripción: {datos_extraidos['info_portal']['descripcion']}\")\n",
    "\n",
    "# Navegación\n",
    "print(f\"\\n🧭 NAVEGACIÓN ({len(datos_extraidos['navegacion'])} secciones):\")\n",
    "for nav in datos_extraidos['navegacion']:\n",
    "    print(f\"   • {nav['texto']} → {nav['href']}\")\n",
    "\n",
    "# Noticias\n",
    "print(f\"\\n📰 NOTICIAS EXTRAÍDAS ({len(datos_extraidos['noticias'])} artículos):\")\n",
    "for i, noticia in enumerate(datos_extraidos['noticias'], 1):\n",
    "    destacada = \"⭐\" if noticia['es_destacada'] else \"  \"\n",
    "    print(f\"\\n{destacada} {i}. {noticia['titulo']}\")\n",
    "    print(f\"      👤 Autor: {noticia.get('autor', {}).get('nombre', 'Desconocido')}\")\n",
    "    print(f\"      📅 Fecha: {noticia.get('fecha', {}).get('texto', 'Sin fecha')}\")\n",
    "    print(f\"      🏷️ Categoría: {noticia['categoria_display']}\")\n",
    "    print(f\"      📊 Engagement: {noticia['estadisticas']['vistas']['count']} vistas, {noticia['estadisticas']['likes']['count']} likes\")\n",
    "    print(f\"      🏷️ Tags: {', '.join(noticia['tags']) if noticia['tags'] else 'Sin tags'}\")\n",
    "    print(f\"      💯 Score: {noticia['metricas_calculadas']['engagement_score']:.1f}\")\n",
    "\n",
    "# Trending\n",
    "print(f\"\\n🔥 TRENDING TOPICS ({len(datos_extraidos['trending'])} temas):\")\n",
    "for trend in datos_extraidos['trending']:\n",
    "    print(f\"   {trend['posicion']}. {trend['hashtag']} - {trend['menciones_count']:,} menciones\")\n",
    "\n",
    "# Estadísticas globales\n",
    "print(f\"\\n📈 ESTADÍSTICAS GLOBALES:\")\n",
    "print(f\"   👥 Lectores activos: {datos_extraidos['estadisticas_globales']['lectores_activos']:,}\")\n",
    "print(f\"   📰 Artículos publicados: {datos_extraidos['estadisticas_globales']['articulos_publicados']:,}\")\n",
    "\n",
    "# Resumen del scraping\n",
    "resumen = datos_extraidos['resumen_scraping']\n",
    "print(f\"\\n📋 RESUMEN DEL SCRAPING:\")\n",
    "print(f\"   📰 Total de noticias: {resumen['total_noticias']}\")\n",
    "print(f\"   ⭐ Noticias destacadas: {resumen['noticias_destacadas']}\")\n",
    "print(f\"   🗂️ Categorías únicas: {resumen['categorias_unicas']}\")\n",
    "print(f\"   ✍️ Autores únicos: {resumen['autores_unicos']}\")\n",
    "print(f\"   👀 Total de vistas: {resumen['engagement_total']['vistas']:,}\")\n",
    "print(f\"   ❤️ Total de likes: {resumen['engagement_total']['likes']:,}\")\n",
    "print(f\"   💬 Total de comentarios: {resumen['engagement_total']['comentarios']:,}\")\n",
    "print(f\"   🏷️ Tags más populares: {', '.join([f'{tag} ({count})' for tag, count in resumen['tags_mas_populares']])}\")\n",
    "\n",
    "print(\"\\n\" + \"═\" * 60)\n",
    "print(\"🎉 ¡EJERCICIO AVANZADO COMPLETADO EXITOSAMENTE!\")\n",
    "print(\"🏆 Has demostrado dominio avanzado de web scraping.\")\n",
    "print(f\"📅 Scraping realizado: {resumen['fecha_scraping']}\")\n",
    "\n",
    "# Guardar en JSON para análisis posterior\n",
    "print(\"\\n💾 Datos también disponibles en formato JSON para análisis posterior.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Herramientas del Navegador para Web Scraping 🛠️\n",
    "\n",
    "Las herramientas de desarrollo del navegador son tu **mejor amigo** para el web scraping. Te permiten inspeccionar, probar y entender la estructura de cualquier sitio web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚀 Accesos Rápidos a DevTools\n",
    "\n",
    "| Acción | Windows/Linux | Mac | Descripción |\n",
    "|--------|---------------|-----|-------------|\n",
    "| **Abrir DevTools** | `F12` o `Ctrl+Shift+I` | `Cmd+Option+I` | Abre las herramientas de desarrollador |\n",
    "| **Inspeccionar elemento** | `Ctrl+Shift+C` | `Cmd+Shift+C` | Activa el selector de elementos |\n",
    "| **Consola JavaScript** | `Ctrl+Shift+J` | `Cmd+Option+J` | Abre directamente la consola |\n",
    "| **Ver código fuente** | `Ctrl+U` | `Cmd+U` | Muestra el HTML completo |\n",
    "| **Buscar en página** | `Ctrl+F` | `Cmd+F` | Buscar texto en la página actual |\n",
    "\n",
    "### 🕵️ Técnicas de Inspección Avanzadas\n",
    "\n",
    "#### 1. **Inspector de Elementos**\n",
    "```\n",
    "• Click derecho en cualquier elemento → \"Inspeccionar\"\n",
    "• Navega por la jerarquía HTML\n",
    "• Ve estilos CSS aplicados en tiempo real\n",
    "• Observa cambios dinámicos en el DOM\n",
    "```\n",
    "\n",
    "#### 2. **Consola JavaScript para Testing**\n",
    "```javascript\n",
    "// Probar selectores CSS\n",
    "document.querySelectorAll('.clase')\n",
    "document.querySelector('#mi-id')\n",
    "\n",
    "// Probar XPath\n",
    "$x('//div[@class=\"ejemplo\"]')\n",
    "$x('//a[contains(@href, \"scrape\")]')\n",
    "\n",
    "// Obtener texto de elementos\n",
    "document.querySelector('h1').textContent\n",
    "\n",
    "// Ver atributos\n",
    "document.querySelector('img').getAttribute('src')\n",
    "```\n",
    "\n",
    "#### 3. **Pestaña Network (Red)**\n",
    "```\n",
    "✅ Ve todas las solicitudes HTTP\n",
    "✅ Examina headers y cookies\n",
    "✅ Identifica APIs y endpoints\n",
    "✅ Analiza tiempos de respuesta\n",
    "✅ Detecta llamadas AJAX/fetch\n",
    "```\n",
    "\n",
    "#### 4. **Copiar Selectores Automáticamente**\n",
    "```\n",
    "• Click derecho en elemento en Inspector\n",
    "• Copy → Copy selector (CSS)\n",
    "• Copy → Copy XPath\n",
    "• Copy → Copy full XPath\n",
    "```\n",
    "\n",
    "### 🎯 Trucos Profesionales\n",
    "\n",
    "#### **Buscar en todo el HTML**\n",
    "- `Ctrl+F` en la pestaña Elements\n",
    "- Busca por: texto, selectores CSS, XPath\n",
    "- Ejemplo: buscar `class=\"noticia\"` encuentra todos los elementos con esa clase\n",
    "\n",
    "#### **Variables especiales en consola**\n",
    "- `$0`: Último elemento seleccionado en Inspector\n",
    "- `$1`, `$2`, etc.: Elementos seleccionados anteriormente\n",
    "- `$$('selector')`: Equivale a `querySelectorAll`\n",
    "- `$('selector')`: Equivale a `querySelector`\n",
    "\n",
    "#### **Monitorear cambios dinámicos**\n",
    "- Click derecho en elemento → \"Break on...\" → \"Subtree modifications\"\n",
    "- Pausa cuando JavaScript modifica el contenido\n",
    "- Útil para sitios con contenido que cambia dinámicamente\n",
    "\n",
    "### 🔍 Estrategias de Investigación\n",
    "\n",
    "#### **Para encontrar el selector perfecto:**\n",
    "1. **Inspecciona** el elemento que te interesa\n",
    "2. **Mira los atributos** disponibles (`id`, `class`, `data-*`)\n",
    "3. **Prueba selectores** en la consola\n",
    "4. **Verifica unicidad** - ¿selecciona solo lo que quieres?\n",
    "5. **Considera la estabilidad** - ¿seguirá funcionando si el diseño cambia?\n",
    "\n",
    "#### **Para sitios dinámicos:**\n",
    "1. **Network tab** para ver llamadas AJAX\n",
    "2. **Buscar APIs JSON** en lugar de scraping HTML\n",
    "3. **Observar patrones** en URLs de datos\n",
    "4. **Identificar headers necesarios** para las peticiones\n",
    "\n",
    "### ⚡ Workflow Eficiente\n",
    "\n",
    "```\n",
    "1. 🌐 Abrir página objetivo\n",
    "2. 🔍 Inspeccionar elementos de interés\n",
    "3. 📝 Anotar selectores y estructura\n",
    "4. 🧪 Probar selectores en consola\n",
    "5. 📊 Verificar en Network si hay APIs\n",
    "6. 💻 Implementar en Python\n",
    "7. 🐛 Debuggear y refinar\n",
    "```\n",
    "\n",
    "### 💡 Consejos Pro\n",
    "\n",
    "- **Usa el modo Responsive** para ver cómo cambia el HTML en móvil\n",
    "- **Simula conexiones lentas** en Network para probar timeouts\n",
    "- **Deshabilita JavaScript** para ver el HTML básico\n",
    "- **Usa Screenshots** para documentar elementos importantes\n",
    "- **Guarda HAR files** para analizar tráfico de red offline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Resumen y Próximos Pasos 🎯\n",
    "\n",
    "### 🎓 Lo que has dominado en esta lección:\n",
    "\n",
    "#### ✅ **Conceptos Fundamentales**\n",
    "- ✨ Qué es el web scraping y sus aplicaciones en el mundo real\n",
    "- 🏗️ Estructura completa de HTML y el DOM\n",
    "- 🎯 Elementos y atributos HTML más importantes para scraping\n",
    "- 🌳 Navegación por la jerarquía del DOM\n",
    "\n",
    "#### ✅ **Habilidades Técnicas Avanzadas**\n",
    "- 🌐 Realizar solicitudes HTTP robustas con `requests`\n",
    "- 🍲 Dominio completo de Beautiful Soup\n",
    "- 🎨 Selectores CSS avanzados y precisos\n",
    "- 📊 Extracción profesional de datos de tablas\n",
    "- 🛡️ Manejo de errores y scraping robusto\n",
    "- 🧪 Testing y debugging con DevTools\n",
    "\n",
    "#### ✅ **Herramientas Profesionales**\n",
    "- 📦 Beautiful Soup para parsing HTML/XML\n",
    "- 🌐 Requests para comunicación HTTP\n",
    "- 🛠️ DevTools del navegador para investigación\n",
    "- 🔍 Técnicas de inspección y análisis\n",
    "\n",
    "#### ✅ **Mejores Prácticas**\n",
    "- 🤖 Headers realistas y User-Agents\n",
    "- ⏱️ Rate limiting y delays apropiados\n",
    "- 🔄 Estrategias de reintentos\n",
    "- 📊 Estructuración de datos extraídos\n",
    "\n",
    "### 🚀 Próxima Lección: HTTP Requests y Beautiful Soup Avanzado\n",
    "\n",
    "En la **Lección 2** profundizaremos en:\n",
    "\n",
    "#### 🌐 **HTTP Avanzado**\n",
    "- 📋 Headers HTTP personalizados y cookies\n",
    "- 🔐 Sesiones persistentes y autenticación\n",
    "- 📤 POST requests y manejo de formularios\n",
    "- 🔄 Redirects y códigos de estado\n",
    "\n",
    "#### 🍲 **Beautiful Soup Profesional**\n",
    "- 🎯 Técnicas de parsing avanzadas\n",
    "- 🔍 Búsquedas complejas con expresiones regulares\n",
    "- 🌳 Navegación avanzada del árbol DOM\n",
    "- ⚡ Optimización de rendimiento\n",
    "\n",
    "#### 📄 **Manejo de Contenido Complejo**\n",
    "- 📋 Paginación y navegación entre páginas\n",
    "- 🔗 Crawling y seguimiento de enlaces\n",
    "- 📊 Procesamiento de diferentes formatos\n",
    "- 🛡️ Detección y manejo de anti-scraping\n",
    "\n",
    "### 💪 Desafíos para Practicar\n",
    "\n",
    "#### 🥉 **Nivel Básico**\n",
    "1. Scraping de un blog personal simple\n",
    "2. Extracción de productos de una tienda online básica\n",
    "3. Recopilación de noticias de un portal sencillo\n",
    "\n",
    "#### 🥈 **Nivel Intermedio**\n",
    "1. Comparador de precios entre múltiples tiendas\n",
    "2. Agregador de ofertas de empleo\n",
    "3. Monitor de cambios en páginas web\n",
    "\n",
    "#### 🥇 **Nivel Avanzado**\n",
    "1. Scraper de redes sociales (datos públicos)\n",
    "2. Análisis de sentimientos de reseñas de productos\n",
    "3. Sistema de alertas en tiempo real\n",
    "\n",
    "### 📚 Recursos para Seguir Aprendiendo\n",
    "\n",
    "#### 📖 **Documentación Oficial**\n",
    "- [Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Requests Documentation](https://docs.python-requests.org/)\n",
    "- [Python HTML Parsing Libraries](https://docs.python.org/3/library/html.parser.html)\n",
    "\n",
    "#### 🌐 **Sitios de Práctica**\n",
    "- [Quotes to Scrape](http://quotes.toscrape.com/) - Perfecto para principiantes\n",
    "- [Books to Scrape](http://books.toscrape.com/) - Catálogo con paginación\n",
    "- [Scrape This Site](https://scrapethissite.com/) - Desafíos progresivos\n",
    "\n",
    "#### 🛠️ **Herramientas Útiles**\n",
    "- [XPath Tester](https://www.freeformatter.com/xpath-tester.html)\n",
    "- [CSS Selector Tester](https://www.w3schools.com/cssref/trysel.asp)\n",
    "- [Regex101](https://regex101.com/) - Testing de expresiones regulares\n",
    "- [Postman](https://www.postman.com/) - Testing de APIs\n",
    "\n",
    "---\n",
    "\n",
    "### 🏆 ¡Felicidades por Completar la Lección 1!\n",
    "\n",
    "Has dado el primer paso importante en tu viaje hacia el dominio del web scraping. Los fundamentos que has aprendido aquí son la base sólida sobre la cual construirás habilidades más avanzadas.\n",
    "\n",
    "#### 🎯 **Recuerda siempre:**\n",
    "- 🤝 **Sé respetuoso** con los sitios web y sus recursos\n",
    "- 📜 **Lee los términos de servicio** antes de hacer scraping\n",
    "- 🤖 **Revisa robots.txt** para conocer las reglas\n",
    "- ⏱️ **Implementa delays** apropiados entre requests\n",
    "- 🛡️ **Maneja errores** graciosamente\n",
    "\n",
    "#### 🚀 **Tu próxima misión:**\n",
    "1. 💻 **Practica** con los ejercicios de esta lección\n",
    "2. 🔍 **Explora** sitios web simples por tu cuenta\n",
    "3. 🧪 **Experimenta** con diferentes selectores\n",
    "4. 📝 **Documenta** tus hallazgos y aprendizajes\n",
    "5. ➡️ **Continúa** con la Lección 2 cuando te sientas cómodo\n",
    "\n",
    "**¡El mundo de los datos web te está esperando! 🌟**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}