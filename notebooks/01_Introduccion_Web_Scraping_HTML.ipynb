{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ•·ï¸ LecciÃ³n 1: IntroducciÃ³n a Web Scraping y HTML BÃ¡sico\n",
    "\n",
    "## ğŸ¯ Objetivos de Aprendizaje\n",
    "\n",
    "Al finalizar esta lecciÃ³n, serÃ¡s capaz de:\n",
    "- âœ… Comprender quÃ© es el web scraping y sus aplicaciones\n",
    "- âœ… Entender la estructura bÃ¡sica de HTML y el DOM\n",
    "- âœ… Identificar elementos, atributos y jerarquÃ­as en HTML\n",
    "- âœ… Utilizar las herramientas de desarrollo del navegador\n",
    "- âœ… Escribir tu primer script de web scraping\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Â¿QuÃ© es Web Scraping? ğŸ¤–\n",
    "\n",
    "**Web Scraping** es la tÃ©cnica de extracciÃ³n automÃ¡tica de datos de sitios web. Es como tener un asistente digital que puede leer y recopilar informaciÃ³n de pÃ¡ginas web de manera automÃ¡tica y estructurada.\n",
    "\n",
    "### ğŸŒŸ Aplicaciones del Web Scraping:\n",
    "\n",
    "| Ãrea | AplicaciÃ³n | Ejemplo |\n",
    "|------|------------|----------|\n",
    "| ğŸ’° **E-commerce** | Monitoreo de precios | Comparar precios entre tiendas |\n",
    "| ğŸ“Š **InvestigaciÃ³n** | RecopilaciÃ³n de datos | AnÃ¡lisis acadÃ©mico y estudios |\n",
    "| ğŸ“° **Periodismo** | ExtracciÃ³n de informaciÃ³n | Periodismo de datos |\n",
    "| ğŸ¤– **Machine Learning** | CreaciÃ³n de datasets | Entrenar modelos de IA |\n",
    "| ğŸ‘€ **Monitoreo** | Seguimiento de cambios | Alertas de contenido nuevo |\n",
    "| ğŸ” **AgregaciÃ³n** | Comparadores | Portales de ofertas de empleo |\n",
    "\n",
    "### ğŸ Â¿Por quÃ© Python para Web Scraping?\n",
    "\n",
    "Python es el lenguaje ideal porque:\n",
    "- ğŸ“ **Sintaxis simple**: FÃ¡cil de leer y escribir\n",
    "- ğŸ“š **Bibliotecas especializadas**: Beautiful Soup, Scrapy, Selenium\n",
    "- ğŸ‘¥ **Gran comunidad**: Mucha documentaciÃ³n y soporte\n",
    "- ğŸ”— **IntegraciÃ³n**: Se conecta fÃ¡cilmente con pandas, numpy, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fundamentos de HTML ğŸ“„\n",
    "\n",
    "HTML (HyperText Markup Language) es el lenguaje de marcado que estructura las pÃ¡ginas web. Para hacer web scraping efectivo, necesitas entender cÃ³mo estÃ¡ organizado el HTML.\n",
    "\n",
    "### 2.1 Estructura BÃ¡sica de un Documento HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de HTML que analizaremos durante la lecciÃ³n\n",
    "html_ejemplo = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"es\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>Mi Primera PÃ¡gina Web</title>\n",
    "    <meta name=\"description\" content=\"Ejemplo para aprender web scraping\">\n",
    "</head>\n",
    "<body>\n",
    "    <header>\n",
    "        <h1 id=\"titulo-principal\" class=\"titulo-grande\">ğŸ•·ï¸ Bienvenido al Web Scraping</h1>\n",
    "        <nav>\n",
    "            <ul class=\"menu-navegacion\">\n",
    "                <li><a href=\"#seccion1\">IntroducciÃ³n</a></li>\n",
    "                <li><a href=\"#seccion2\">Conceptos</a></li>\n",
    "                <li><a href=\"#seccion3\">Ejemplos</a></li>\n",
    "            </ul>\n",
    "        </nav>\n",
    "    </header>\n",
    "    \n",
    "    <main>\n",
    "        <section id=\"seccion1\" class=\"contenido importante\">\n",
    "            <h2>ğŸš€ IntroducciÃ³n al Web Scraping</h2>\n",
    "            <p class=\"texto-destacado\">El web scraping es una tÃ©cnica <strong>poderosa</strong> para extraer datos.</p>\n",
    "            <p>Permite automatizar la recopilaciÃ³n de informaciÃ³n de sitios web.</p>\n",
    "            <div class=\"estadisticas\" data-tipo=\"metricas\">\n",
    "                <span class=\"numero\">95%</span> de eficiencia\n",
    "            </div>\n",
    "        </section>\n",
    "        \n",
    "        <section id=\"seccion2\" class=\"contenido\">\n",
    "            <h2>ğŸ”‘ Conceptos Clave</h2>\n",
    "            <div class=\"conceptos-grid\">\n",
    "                <article data-tema=\"html\" class=\"concepto\">\n",
    "                    <h3>HTML</h3>\n",
    "                    <p>Lenguaje de marcado para crear pÃ¡ginas web.</p>\n",
    "                    <span class=\"dificultad\" data-nivel=\"facil\">â­ FÃ¡cil</span>\n",
    "                </article>\n",
    "                <article data-tema=\"css\" class=\"concepto\">\n",
    "                    <h3>CSS</h3>\n",
    "                    <p>Hojas de estilo para dar formato visual.</p>\n",
    "                    <span class=\"dificultad\" data-nivel=\"medio\">â­â­ Medio</span>\n",
    "                </article>\n",
    "                <article data-tema=\"javascript\" class=\"concepto\">\n",
    "                    <h3>JavaScript</h3>\n",
    "                    <p>Lenguaje para aÃ±adir interactividad.</p>\n",
    "                    <span class=\"dificultad\" data-nivel=\"dificil\">â­â­â­ DifÃ­cil</span>\n",
    "                </article>\n",
    "            </div>\n",
    "        </section>\n",
    "        \n",
    "        <section id=\"seccion3\" class=\"contenido ejemplos\">\n",
    "            <h2>ğŸ“Š Ejemplos PrÃ¡cticos</h2>\n",
    "            <table class=\"tabla-datos\" border=\"1\">\n",
    "                <thead>\n",
    "                    <tr>\n",
    "                        <th>TÃ©cnica</th>\n",
    "                        <th>Dificultad</th>\n",
    "                        <th>Uso ComÃºn</th>\n",
    "                        <th>Velocidad</th>\n",
    "                    </tr>\n",
    "                </thead>\n",
    "                <tbody>\n",
    "                    <tr data-id=\"1\">\n",
    "                        <td>Beautiful Soup</td>\n",
    "                        <td class=\"facil\">ğŸŸ¢ FÃ¡cil</td>\n",
    "                        <td>Sitios estÃ¡ticos</td>\n",
    "                        <td>âš¡ RÃ¡pido</td>\n",
    "                    </tr>\n",
    "                    <tr data-id=\"2\">\n",
    "                        <td>Scrapy</td>\n",
    "                        <td class=\"medio\">ğŸŸ¡ Intermedio</td>\n",
    "                        <td>Proyectos grandes</td>\n",
    "                        <td>âš¡âš¡ Muy rÃ¡pido</td>\n",
    "                    </tr>\n",
    "                    <tr data-id=\"3\">\n",
    "                        <td>Selenium</td>\n",
    "                        <td class=\"dificil\">ğŸ”´ Avanzado</td>\n",
    "                        <td>Sitios dinÃ¡micos</td>\n",
    "                        <td>ğŸŒ Lento</td>\n",
    "                    </tr>\n",
    "                </tbody>\n",
    "            </table>\n",
    "            \n",
    "            <div class=\"lista-sitios\">\n",
    "                <h3>ğŸŒ Sitios Populares para Practicar:</h3>\n",
    "                <ul>\n",
    "                    <li><a href=\"http://quotes.toscrape.com\" target=\"_blank\">Quotes to Scrape</a></li>\n",
    "                    <li><a href=\"http://books.toscrape.com\" target=\"_blank\">Books to Scrape</a></li>\n",
    "                    <li><a href=\"https://scrapethissite.com\" target=\"_blank\">Scrape This Site</a></li>\n",
    "                </ul>\n",
    "            </div>\n",
    "        </section>\n",
    "    </main>\n",
    "    \n",
    "    <footer>\n",
    "        <p>&copy; 2024 Curso de Web Scraping | <em>Aprende haciendo</em></p>\n",
    "        <div class=\"redes-sociales\">\n",
    "            <a href=\"#\" class=\"red-social\" data-plataforma=\"twitter\">ğŸ¦ Twitter</a>\n",
    "            <a href=\"#\" class=\"red-social\" data-plataforma=\"github\">ğŸ™ GitHub</a>\n",
    "        </div>\n",
    "    </footer>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "print(\"âœ… HTML de ejemplo creado exitosamente!\")\n",
    "print(f\"ğŸ“ TamaÃ±o del HTML: {len(html_ejemplo)} caracteres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 AnatomÃ­a de un Elemento HTML ğŸ”\n",
    "\n",
    "Un elemento HTML tiene la siguiente estructura:\n",
    "\n",
    "```html\n",
    "<etiqueta atributo=\"valor\" otro-atributo=\"otro-valor\">Contenido</etiqueta>\n",
    "```\n",
    "\n",
    "**Componentes:**\n",
    "- ğŸ·ï¸ **Etiqueta de apertura**: `<p>`\n",
    "- ğŸ¯ **Atributos**: `class=\"texto\"`, `id=\"parrafo1\"`, `data-info=\"importante\"`\n",
    "- ğŸ“ **Contenido**: El texto o elementos dentro\n",
    "- ğŸ”š **Etiqueta de cierre**: `</p>`\n",
    "\n",
    "### 2.3 Elementos HTML MÃ¡s Importantes para Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario de elementos HTML mÃ¡s comunes en web scraping\n",
    "elementos_html = {\n",
    "    'ğŸ—ï¸ Estructura': {\n",
    "        '<html>': 'Elemento raÃ­z del documento HTML',\n",
    "        '<head>': 'Metadatos que no se muestran al usuario',\n",
    "        '<body>': 'Contenido visible de la pÃ¡gina web',\n",
    "        '<header>': 'Encabezado de la pÃ¡gina o secciÃ³n',\n",
    "        '<main>': 'Contenido principal de la pÃ¡gina',\n",
    "        '<footer>': 'Pie de pÃ¡gina con informaciÃ³n adicional'\n",
    "    },\n",
    "    'ğŸ“ Texto y Contenido': {\n",
    "        '<h1> - <h6>': 'TÃ­tulos y subtÃ­tulos (h1 es el mÃ¡s importante)',\n",
    "        '<p>': 'PÃ¡rrafos de texto normal',\n",
    "        '<span>': 'Contenedor en lÃ­nea para pequeÃ±as porciones',\n",
    "        '<strong>': 'Texto importante (aparece en negrita)',\n",
    "        '<em>': 'Ã‰nfasis (aparece en cursiva)',\n",
    "        '<br>': 'Salto de lÃ­nea (no tiene cierre)'\n",
    "    },\n",
    "    'ğŸ”— Enlaces y Media': {\n",
    "        '<a>': 'HipervÃ­nculos a otras pÃ¡ginas o secciones',\n",
    "        '<img>': 'ImÃ¡genes (no tiene etiqueta de cierre)',\n",
    "        '<video>': 'Contenido de video',\n",
    "        '<audio>': 'Contenido de audio'\n",
    "    },\n",
    "    'ğŸ“‹ Listas': {\n",
    "        '<ul>': 'Lista no ordenada (bullets)',\n",
    "        '<ol>': 'Lista ordenada (nÃºmeros)',\n",
    "        '<li>': 'Elemento individual de lista'\n",
    "    },\n",
    "    'ğŸ“¦ Contenedores': {\n",
    "        '<div>': 'DivisiÃ³n genÃ©rica (muy comÃºn en scraping)',\n",
    "        '<section>': 'SecciÃ³n temÃ¡tica de contenido',\n",
    "        '<article>': 'Contenido independiente',\n",
    "        '<nav>': 'NavegaciÃ³n y menÃºs'\n",
    "    },\n",
    "    'ğŸ“Š Tablas': {\n",
    "        '<table>': 'Contenedor principal de la tabla',\n",
    "        '<thead>': 'Encabezado de la tabla',\n",
    "        '<tbody>': 'Cuerpo con datos de la tabla',\n",
    "        '<tr>': 'Fila de la tabla',\n",
    "        '<th>': 'Celda de encabezado',\n",
    "        '<td>': 'Celda con datos'\n",
    "    },\n",
    "    'ğŸ“ Formularios': {\n",
    "        '<form>': 'Formulario para enviar datos',\n",
    "        '<input>': 'Campo de entrada de datos',\n",
    "        '<button>': 'BotÃ³n clicable',\n",
    "        '<select>': 'Lista desplegable',\n",
    "        '<option>': 'OpciÃ³n dentro de select'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Mostrar los elementos de forma organizada\n",
    "for categoria, elementos in elementos_html.items():\n",
    "    print(f\"\\n{categoria}:\")\n",
    "    print(\"â”€\" * 60)\n",
    "    for elemento, descripcion in elementos.items():\n",
    "        print(f\"  {elemento:<15} â†’ {descripcion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. El DOM (Document Object Model) ğŸŒ³\n",
    "\n",
    "El DOM es una representaciÃ³n en forma de **Ã¡rbol jerÃ¡rquico** de la estructura HTML. Cada elemento HTML es un **nodo** en este Ã¡rbol, y entender esta estructura es fundamental para el web scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VisualizaciÃ³n conceptual del DOM de nuestro ejemplo\n",
    "estructura_dom = \"\"\"\n",
    "ğŸ“„ document (raÃ­z)\n",
    "â””â”€â”€ html (elemento raÃ­z del HTML)\n",
    "    â”œâ”€â”€ head (metadatos)\n",
    "    â”‚   â”œâ”€â”€ meta (charset)\n",
    "    â”‚   â”œâ”€â”€ title (tÃ­tulo de la pÃ¡gina)\n",
    "    â”‚   â””â”€â”€ meta (descripciÃ³n)\n",
    "    â””â”€â”€ body (contenido visible)\n",
    "        â”œâ”€â”€ header (encabezado)\n",
    "        â”‚   â”œâ”€â”€ h1#titulo-principal (tÃ­tulo principal)\n",
    "        â”‚   â””â”€â”€ nav (navegaciÃ³n)\n",
    "        â”‚       â””â”€â”€ ul.menu-navegacion (lista)\n",
    "        â”‚           â”œâ”€â”€ li â†’ a (enlace 1)\n",
    "        â”‚           â”œâ”€â”€ li â†’ a (enlace 2)\n",
    "        â”‚           â””â”€â”€ li â†’ a (enlace 3)\n",
    "        â”œâ”€â”€ main (contenido principal)\n",
    "        â”‚   â”œâ”€â”€ section#seccion1 (primera secciÃ³n)\n",
    "        â”‚   â”‚   â”œâ”€â”€ h2 (subtÃ­tulo)\n",
    "        â”‚   â”‚   â”œâ”€â”€ p.texto-destacado (pÃ¡rrafo destacado)\n",
    "        â”‚   â”‚   â”œâ”€â”€ p (pÃ¡rrafo normal)\n",
    "        â”‚   â”‚   â””â”€â”€ div.estadisticas (mÃ©tricas)\n",
    "        â”‚   â”œâ”€â”€ section#seccion2 (segunda secciÃ³n)\n",
    "        â”‚   â”‚   â”œâ”€â”€ h2 (subtÃ­tulo)\n",
    "        â”‚   â”‚   â””â”€â”€ div.conceptos-grid\n",
    "        â”‚   â”‚       â”œâ”€â”€ article[data-tema=\"html\"]\n",
    "        â”‚   â”‚       â”œâ”€â”€ article[data-tema=\"css\"]\n",
    "        â”‚   â”‚       â””â”€â”€ article[data-tema=\"javascript\"]\n",
    "        â”‚   â””â”€â”€ section#seccion3 (tercera secciÃ³n)\n",
    "        â”‚       â”œâ”€â”€ h2 (subtÃ­tulo)\n",
    "        â”‚       â”œâ”€â”€ table.tabla-datos (tabla)\n",
    "        â”‚       â”‚   â”œâ”€â”€ thead â†’ tr â†’ th (encabezados)\n",
    "        â”‚       â”‚   â””â”€â”€ tbody â†’ tr â†’ td (datos)\n",
    "        â”‚       â””â”€â”€ div.lista-sitios (lista de sitios)\n",
    "        â””â”€â”€ footer (pie de pÃ¡gina)\n",
    "            â”œâ”€â”€ p (copyright)\n",
    "            â””â”€â”€ div.redes-sociales (enlaces sociales)\n",
    "\"\"\"\n",
    "\n",
    "print(\"ğŸŒ³ Estructura del DOM como Ã¡rbol jerÃ¡rquico:\")\n",
    "print(estructura_dom)\n",
    "\n",
    "# Conceptos importantes del DOM\n",
    "conceptos_dom = {\n",
    "    \"ğŸ‘¨â€ğŸ‘§â€ğŸ‘¦ Nodo Padre (Parent)\": \"El elemento que contiene a otros elementos\",\n",
    "    \"ğŸ‘¶ Nodo Hijo (Child)\": \"Elemento contenido directamente dentro de otro\",\n",
    "    \"ğŸ‘« Hermanos (Siblings)\": \"Elementos que estÃ¡n al mismo nivel jerÃ¡rquico\",\n",
    "    \"ğŸ‘´ Ancestros (Ancestors)\": \"Todos los elementos padre en la cadena hacia arriba\",\n",
    "    \"ğŸ‘ª Descendientes (Descendants)\": \"Todos los elementos contenidos (hijos, nietos, etc.)\",\n",
    "    \"ğŸ” Selector\": \"PatrÃ³n usado para encontrar elementos especÃ­ficos\",\n",
    "    \"ğŸ·ï¸ Atributo\": \"InformaciÃ³n adicional sobre un elemento (id, class, etc.)\"\n",
    "}\n",
    "\n",
    "print(\"\\nğŸ”‘ Conceptos Clave del DOM:\")\n",
    "print(\"â•\" * 70)\n",
    "for concepto, descripcion in conceptos_dom.items():\n",
    "    print(f\"{concepto}: {descripcion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Atributos HTML Cruciales para Web Scraping ğŸ¯\n",
    "\n",
    "Los atributos son caracterÃ­sticas adicionales de los elementos HTML. Son tu **herramienta principal** para localizar elementos especÃ­ficos al hacer scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los atributos mÃ¡s importantes para web scraping\n",
    "atributos_scraping = {\n",
    "    'ğŸ†” id': {\n",
    "        'descripcion': 'Identificador Ãºnico del elemento en toda la pÃ¡gina',\n",
    "        'ejemplo': '<div id=\"contenido-principal\">',\n",
    "        'uso_scraping': 'â­â­â­ Perfecto para seleccionar UN elemento especÃ­fico',\n",
    "        'ventaja': 'Garantiza unicidad, muy confiable',\n",
    "        'selector_css': '#mi-id',\n",
    "        'selector_xpath': '//*[@id=\"mi-id\"]'\n",
    "    },\n",
    "    'ğŸ·ï¸ class': {\n",
    "        'descripcion': 'Clase(s) CSS para agrupar elementos similares',\n",
    "        'ejemplo': '<p class=\"texto importante destacado\">',\n",
    "        'uso_scraping': 'â­â­â­ Ideal para seleccionar GRUPOS de elementos',\n",
    "        'ventaja': 'Muy comÃºn, puede tener mÃºltiples valores',\n",
    "        'selector_css': '.mi-clase',\n",
    "        'selector_xpath': '//*[@class=\"mi-clase\"]'\n",
    "    },\n",
    "    'ğŸ”— href': {\n",
    "        'descripcion': 'URL de destino en enlaces',\n",
    "        'ejemplo': '<a href=\"https://ejemplo.com\">Enlace</a>',\n",
    "        'uso_scraping': 'â­â­ Para extraer URLs y navegar entre pÃ¡ginas',\n",
    "        'ventaja': 'Esencial para crawling y navegaciÃ³n',\n",
    "        'selector_css': 'a[href]',\n",
    "        'selector_xpath': '//a[@href]'\n",
    "    },\n",
    "    'ğŸ–¼ï¸ src': {\n",
    "        'descripcion': 'Fuente de recursos (imÃ¡genes, scripts, etc.)',\n",
    "        'ejemplo': '<img src=\"imagen.jpg\" alt=\"DescripciÃ³n\">',\n",
    "        'uso_scraping': 'â­â­ Para descargar imÃ¡genes o identificar recursos',\n",
    "        'ventaja': 'Ãštil para multimedia y assets',\n",
    "        'selector_css': 'img[src]',\n",
    "        'selector_xpath': '//img[@src]'\n",
    "    },\n",
    "    'ğŸ“Š data-*': {\n",
    "        'descripcion': 'Atributos personalizados para almacenar datos',\n",
    "        'ejemplo': '<div data-producto-id=\"12345\" data-precio=\"99.99\">',\n",
    "        'uso_scraping': 'â­â­â­ Contienen informaciÃ³n estructurada MUY valiosa',\n",
    "        'ventaja': 'Datos limpios y estructurados',\n",
    "        'selector_css': '[data-producto-id]',\n",
    "        'selector_xpath': '//*[@data-producto-id]'\n",
    "    },\n",
    "    'ğŸ“ name': {\n",
    "        'descripcion': 'Nombre de elementos de formulario',\n",
    "        'ejemplo': '<input name=\"email\" type=\"email\">',\n",
    "        'uso_scraping': 'â­ Para identificar campos de formulario',\n",
    "        'ventaja': 'Ãštil para automatizar formularios',\n",
    "        'selector_css': '[name=\"email\"]',\n",
    "        'selector_xpath': '//*[@name=\"email\"]'\n",
    "    },\n",
    "    'ğŸ·ï¸ title': {\n",
    "        'descripcion': 'Texto que aparece al hacer hover',\n",
    "        'ejemplo': '<img src=\"foto.jpg\" title=\"Mi foto favorita\">',\n",
    "        'uso_scraping': 'â­ Para obtener informaciÃ³n adicional',\n",
    "        'ventaja': 'InformaciÃ³n extra no siempre visible',\n",
    "        'selector_css': '[title]',\n",
    "        'selector_xpath': '//*[@title]'\n",
    "    },\n",
    "    'ğŸ¨ style': {\n",
    "        'descripcion': 'Estilos CSS inline',\n",
    "        'ejemplo': '<div style=\"color: red; font-size: 16px;\">',\n",
    "        'uso_scraping': 'â­ Ocasionalmente Ãºtil para filtrar elementos',\n",
    "        'ventaja': 'Puede contener informaciÃ³n de estado',\n",
    "        'selector_css': '[style]',\n",
    "        'selector_xpath': '//*[@style]'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Mostrar informaciÃ³n detallada sobre cada atributo\n",
    "for atributo, info in atributos_scraping.items():\n",
    "    print(f\"\\n{atributo}\")\n",
    "    print(\"â•\" * 80)\n",
    "    for key, value in info.items():\n",
    "        emoji_key = {\n",
    "            'descripcion': 'ğŸ“‹',\n",
    "            'ejemplo': 'ğŸ’»',\n",
    "            'uso_scraping': 'ğŸ¯',\n",
    "            'ventaja': 'âœ¨',\n",
    "            'selector_css': 'ğŸ¨',\n",
    "            'selector_xpath': 'ğŸ›¤ï¸'\n",
    "        }.get(key, 'â€¢')\n",
    "        print(f\"  {emoji_key} {key.replace('_', ' ').title()}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tu Primer Script de Web Scraping ğŸš€\n",
    "\n",
    "Â¡Ahora viene la parte emocionante! Vamos a escribir nuestro primer script de web scraping usando las bibliotecas mÃ¡s populares de Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# InstalaciÃ³n de bibliotecas necesarias\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def instalar_librerias():\n",
    "    \"\"\"Instala las bibliotecas necesarias para web scraping\"\"\"\n",
    "    librerias = ['requests', 'beautifulsoup4', 'lxml']\n",
    "    \n",
    "    for libreria in librerias:\n",
    "        try:\n",
    "            __import__(libreria.replace('-', '_').replace('4', ''))\n",
    "            print(f\"âœ… {libreria} ya estÃ¡ instalada\")\n",
    "        except ImportError:\n",
    "            print(f\"ğŸ“¦ Instalando {libreria}...\")\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', libreria, '--quiet'])\n",
    "            print(f\"âœ… {libreria} instalada correctamente\")\n",
    "\n",
    "# Ejecutar instalaciÃ³n\n",
    "instalar_librerias()\n",
    "print(\"\\nğŸ‰ Â¡Todas las bibliotecas estÃ¡n listas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Mi primer scraper: Extraer citas inspiradoras\n",
    "def mi_primer_scraping():\n",
    "    \"\"\"\n",
    "    FunciÃ³n que realiza web scraping de quotes.toscrape.com\n",
    "    Extrae citas, autores y tags de forma estructurada\n",
    "    \"\"\"\n",
    "    print(\"ğŸ•·ï¸ INICIANDO MI PRIMER WEB SCRAPING\")\n",
    "    print(\"â•\" * 50)\n",
    "    \n",
    "    # PASO 1: Configurar la solicitud\n",
    "    url = \"http://quotes.toscrape.com/\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    print(f\"ğŸŒ Conectando a: {url}\")\n",
    "    print(f\"ğŸ¤– User-Agent: {headers['User-Agent'][:50]}...\")\n",
    "    \n",
    "    # PASO 2: Hacer la solicitud HTTP\n",
    "    try:\n",
    "        inicio = time.time()\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        tiempo_respuesta = time.time() - inicio\n",
    "        \n",
    "        # PASO 3: Verificar el estado de la respuesta\n",
    "        if response.status_code == 200:\n",
    "            print(f\"âœ… ConexiÃ³n exitosa!\")\n",
    "            print(f\"ğŸ“Š CÃ³digo de estado: {response.status_code}\")\n",
    "            print(f\"â±ï¸ Tiempo de respuesta: {tiempo_respuesta:.2f} segundos\")\n",
    "            print(f\"ğŸ“„ TamaÃ±o del HTML: {len(response.text):,} caracteres\")\n",
    "        else:\n",
    "            print(f\"âŒ Error: CÃ³digo de estado {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"âŒ Error: Timeout en la conexiÃ³n\")\n",
    "        return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"âŒ Error en la solicitud: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # PASO 4: Parsear el HTML con Beautiful Soup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    print(f\"\\nğŸ² HTML parseado con Beautiful Soup\")\n",
    "    print(f\"ğŸ“‹ TÃ­tulo de la pÃ¡gina: {soup.find('title').text}\")\n",
    "    \n",
    "    # PASO 5: Extraer las citas\n",
    "    print(\"\\nğŸ“š EXTRAYENDO CITAS INSPIRADORAS\")\n",
    "    print(\"â”€\" * 50)\n",
    "    \n",
    "    # Encontrar todos los contenedores de citas\n",
    "    quotes_containers = soup.find_all('div', class_='quote')\n",
    "    print(f\"ğŸ” Encontradas {len(quotes_containers)} citas en la pÃ¡gina\")\n",
    "    \n",
    "    # Lista para almacenar las citas extraÃ­das\n",
    "    citas_extraidas = []\n",
    "    \n",
    "    for i, quote_container in enumerate(quotes_containers[:5], 1):  # Primeras 5 citas\n",
    "        try:\n",
    "            # Extraer componentes de cada cita\n",
    "            texto_element = quote_container.find('span', class_='text')\n",
    "            autor_element = quote_container.find('small', class_='author')\n",
    "            tags_elements = quote_container.find_all('a', class_='tag')\n",
    "            \n",
    "            # Verificar que los elementos existen\n",
    "            if texto_element and autor_element:\n",
    "                texto = texto_element.text.strip()\n",
    "                autor = autor_element.text.strip()\n",
    "                tags = [tag.text.strip() for tag in tags_elements]\n",
    "                \n",
    "                # Crear diccionario con la informaciÃ³n\n",
    "                cita_info = {\n",
    "                    'numero': i,\n",
    "                    'texto': texto,\n",
    "                    'autor': autor,\n",
    "                    'tags': tags,\n",
    "                    'num_palabras': len(texto.split()),\n",
    "                    'fecha_extraccion': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                }\n",
    "                \n",
    "                citas_extraidas.append(cita_info)\n",
    "                \n",
    "                # Mostrar informaciÃ³n extraÃ­da\n",
    "                print(f\"\\nğŸ“– Cita #{i}:\")\n",
    "                print(f\"   ğŸ“ Texto: {texto[:60]}{'...' if len(texto) > 60 else ''}\")\n",
    "                print(f\"   âœï¸  Autor: {autor}\")\n",
    "                print(f\"   ğŸ·ï¸  Tags: {', '.join(tags) if tags else 'Sin tags'}\")\n",
    "                print(f\"   ğŸ“Š Palabras: {cita_info['num_palabras']}\")\n",
    "            else:\n",
    "                print(f\"âš ï¸ Cita #{i}: Algunos elementos no encontrados\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error procesando cita #{i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # PASO 6: EstadÃ­sticas finales\n",
    "    print(\"\\n\" + \"â•\" * 60)\n",
    "    print(\"ğŸ“Š ESTADÃSTICAS FINALES\")\n",
    "    print(\"â•\" * 60)\n",
    "    print(f\"âœ… Citas extraÃ­das exitosamente: {len(citas_extraidas)}\")\n",
    "    if citas_extraidas:\n",
    "        total_palabras = sum(cita['num_palabras'] for cita in citas_extraidas)\n",
    "        promedio_palabras = total_palabras / len(citas_extraidas)\n",
    "        todos_los_tags = []\n",
    "        for cita in citas_extraidas:\n",
    "            todos_los_tags.extend(cita['tags'])\n",
    "        tags_unicos = set(todos_los_tags)\n",
    "        \n",
    "        print(f\"ğŸ“ Total de palabras: {total_palabras}\")\n",
    "        print(f\"ğŸ“ˆ Promedio de palabras por cita: {promedio_palabras:.1f}\")\n",
    "        print(f\"ğŸ·ï¸ Tags Ãºnicos encontrados: {len(tags_unicos)}\")\n",
    "        print(f\"ğŸ¯ Tags mÃ¡s comunes: {', '.join(list(tags_unicos)[:5])}\")\n",
    "    \n",
    "    return citas_extraidas\n",
    "\n",
    "# Â¡Ejecutar nuestro primer web scraping!\n",
    "print(\"ğŸš€ Â¡VAMOS A HACER NUESTRO PRIMER WEB SCRAPING!\\n\")\n",
    "resultados = mi_primer_scraping()\n",
    "\n",
    "if resultados:\n",
    "    print(\"\\nğŸ‰ Â¡FELICIDADES! Has completado tu primer web scraping exitosamente.\")\n",
    "    print(\"ğŸ“ Ahora tienes los fundamentos para scraping mÃ¡s avanzado.\")\n",
    "else:\n",
    "    print(\"\\nğŸ¤” Algo saliÃ³ mal, pero no te preocupes. Â¡Es parte del aprendizaje!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Beautiful Soup en Profundidad ğŸ²\n",
    "\n",
    "Beautiful Soup es la biblioteca mÃ¡s popular para parsing HTML en Python. Es intuitiva, poderosa y perfecta para principiantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trabajando con nuestro HTML de ejemplo usando Beautiful Soup\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Crear objeto BeautifulSoup con nuestro HTML de ejemplo\n",
    "soup = BeautifulSoup(html_ejemplo, 'html.parser')\n",
    "\n",
    "print(\"ğŸ” MÃ‰TODOS ESENCIALES DE BEAUTIFUL SOUP\\n\")\n",
    "print(\"â•\" * 70)\n",
    "\n",
    "# 1. find() - El mÃ©todo mÃ¡s usado\n",
    "print(\"\\n1ï¸âƒ£ MÃ©todo find() - Encuentra el PRIMER elemento que coincida:\")\n",
    "print(\"â”€\" * 50)\n",
    "\n",
    "primer_h2 = soup.find('h2')\n",
    "print(f\"   ğŸ¯ Primer <h2>: '{primer_h2.text.strip()}'\")\n",
    "\n",
    "primer_parrafo_destacado = soup.find('p', class_='texto-destacado')\n",
    "print(f\"   ğŸ¨ Primer pÃ¡rrafo destacado: '{primer_parrafo_destacado.text.strip()}'\")\n",
    "\n",
    "elemento_con_id = soup.find(id='titulo-principal')\n",
    "print(f\"   ğŸ†” Elemento con ID 'titulo-principal': '{elemento_con_id.text.strip()}'\")\n",
    "\n",
    "# 2. find_all() - Para mÃºltiples elementos\n",
    "print(\"\\n2ï¸âƒ£ MÃ©todo find_all() - Encuentra TODOS los elementos que coincidan:\")\n",
    "print(\"â”€\" * 50)\n",
    "\n",
    "todos_los_h2 = soup.find_all('h2')\n",
    "print(f\"   ğŸ“ Total de <h2> encontrados: {len(todos_los_h2)}\")\n",
    "for i, h2 in enumerate(todos_los_h2, 1):\n",
    "    print(f\"      H2 #{i}: '{h2.text.strip()}'\")\n",
    "\n",
    "todos_los_parrafos = soup.find_all('p')\n",
    "print(f\"\\n   ğŸ“„ Total de pÃ¡rrafos <p>: {len(todos_los_parrafos)}\")\n",
    "for i, p in enumerate(todos_los_parrafos, 1):\n",
    "    texto = p.text.strip()[:40] + '...' if len(p.text.strip()) > 40 else p.text.strip()\n",
    "    print(f\"      PÃ¡rrafo #{i}: '{texto}'\")\n",
    "\n",
    "# 3. BÃºsqueda por mÃºltiples clases\n",
    "print(\"\\n3ï¸âƒ£ BÃºsqueda por clase especÃ­fica:\")\n",
    "print(\"â”€\" * 50)\n",
    "\n",
    "elementos_contenido = soup.find_all(class_='contenido')\n",
    "print(f\"   ğŸ¨ Elementos con clase 'contenido': {len(elementos_contenido)}\")\n",
    "for i, elem in enumerate(elementos_contenido, 1):\n",
    "    id_elem = elem.get('id', 'sin ID')\n",
    "    print(f\"      Elemento #{i}: <{elem.name}> con ID '{id_elem}'\")\n",
    "\n",
    "# 4. BÃºsqueda por atributos personalizados\n",
    "print(\"\\n4ï¸âƒ£ BÃºsqueda por atributos data-* (muy Ãºtil):\")\n",
    "print(\"â”€\" * 50)\n",
    "\n",
    "articulos_con_data = soup.find_all('article', attrs={'data-tema': True})\n",
    "print(f\"   ğŸ“Š Articles con atributo data-tema: {len(articulos_con_data)}\")\n",
    "for articulo in articulos_con_data:\n",
    "    tema = articulo.get('data-tema')\n",
    "    titulo = articulo.find('h3').text.strip()\n",
    "    dificultad = articulo.find('span', class_='dificultad').text.strip()\n",
    "    print(f\"      ğŸ“‹ Tema: {tema} | TÃ­tulo: '{titulo}' | {dificultad}\")\n",
    "\n",
    "# 5. NavegaciÃ³n del Ã¡rbol DOM\n",
    "print(\"\\n5ï¸âƒ£ NavegaciÃ³n del Ã¡rbol DOM (Relaciones familiares):\")\n",
    "print(\"â”€\" * 50)\n",
    "\n",
    "seccion1 = soup.find('section', id='seccion1')\n",
    "print(f\"   ğŸ‘¨â€ğŸ‘§â€ğŸ‘¦ Padre de seccion1: <{seccion1.parent.name}>\")\n",
    "print(f\"   ğŸ‘¶ Hijos directos de seccion1:\")\n",
    "for child in seccion1.children:\n",
    "    if child.name:  # Solo elementos, no texto\n",
    "        texto_preview = child.text.strip()[:30] + '...' if len(child.text.strip()) > 30 else child.text.strip()\n",
    "        print(f\"      â€¢ <{child.name}>: '{texto_preview}'\")\n",
    "\n",
    "# 6. Hermanos (elementos al mismo nivel)\n",
    "primer_h2 = soup.find('h2')\n",
    "siguiente_hermano = primer_h2.find_next_sibling()\n",
    "if siguiente_hermano:\n",
    "    print(f\"   ğŸ‘« Siguiente hermano de primer H2: <{siguiente_hermano.name}>\")\n",
    "\n",
    "# 7. ExtracciÃ³n de atributos especÃ­ficos\n",
    "print(\"\\n6ï¸âƒ£ ExtracciÃ³n de atributos especÃ­ficos:\")\n",
    "print(\"â”€\" * 50)\n",
    "\n",
    "todos_los_enlaces = soup.find_all('a')\n",
    "print(f\"   ğŸ”— Total de enlaces encontrados: {len(todos_los_enlaces)}\")\n",
    "for i, enlace in enumerate(todos_los_enlaces, 1):\n",
    "    href = enlace.get('href', 'Sin href')\n",
    "    texto = enlace.text.strip() or '[Sin texto]'\n",
    "    target = enlace.get('target', 'Misma ventana')\n",
    "    print(f\"      Enlace #{i}: '{texto}' â†’ {href} ({target})\")\n",
    "\n",
    "# 8. BÃºsqueda con expresiones regulares\n",
    "print(\"\\n7ï¸âƒ£ BÃºsqueda avanzada con expresiones regulares:\")\n",
    "print(\"â”€\" * 50)\n",
    "\n",
    "import re\n",
    "elementos_con_data = soup.find_all(attrs={'data-tema': re.compile(r'.*')})\n",
    "print(f\"   ğŸ” Elementos con cualquier atributo data-tema: {len(elementos_con_data)}\")\n",
    "\n",
    "clases_que_contienen_contenido = soup.find_all(class_=re.compile(r'contenido'))\n",
    "print(f\"   ğŸ¨ Elementos con clases que contienen 'contenido': {len(clases_que_contienen_contenido)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Selectores CSS: Tu Arma Secreta ğŸ¨\n",
    "\n",
    "Los selectores CSS son una forma muy poderosa y elegante de encontrar elementos. Son especialmente Ãºtiles cuando ya conoces CSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ¯ SELECTORES CSS: PRECISIÃ“N Y ELEGANCIA\\n\")\n",
    "print(\"â•\" * 70)\n",
    "\n",
    "# Ejemplos completos de selectores CSS\n",
    "selectores_css = [\n",
    "    # Selectores bÃ¡sicos\n",
    "    ('h1', 'ğŸ·ï¸ Seleccionar todos los elementos h1'),\n",
    "    ('p', 'ğŸ“ Todos los pÃ¡rrafos'),\n",
    "    \n",
    "    # Selectores por ID y clase\n",
    "    ('#titulo-principal', 'ğŸ†” Elemento con ID especÃ­fico'),\n",
    "    ('.contenido', 'ğŸ¨ Elementos con clase contenido'),\n",
    "    ('.texto-destacado', 'â­ Elementos con clase texto-destacado'),\n",
    "    \n",
    "    # Selectores combinados\n",
    "    ('section.contenido', 'ğŸ“¦ Elementos section CON clase contenido'),\n",
    "    ('h2.titulo-grande', 'ğŸ¯ H2 con clase especÃ­fica'),\n",
    "    ('p.texto-destacado', 'ğŸ“ PÃ¡rrafos destacados'),\n",
    "    \n",
    "    # Selectores de descendientes\n",
    "    ('nav ul li', 'ğŸ—‚ï¸ Li dentro de ul dentro de nav'),\n",
    "    ('section article h3', 'ğŸ“‹ H3 dentro de article dentro de section'),\n",
    "    ('div.conceptos-grid article', 'ğŸ² Articles dentro de div con clase especÃ­fica'),\n",
    "    \n",
    "    # Selectores de hijos directos\n",
    "    ('section > h2', 'ğŸ‘¶ H2 hijos DIRECTOS de section'),\n",
    "    ('ul > li', 'ğŸ“‹ Li hijos directos de ul'),\n",
    "    \n",
    "    # Selectores de atributos\n",
    "    ('a[href]', 'ğŸ”— Enlaces que tienen atributo href'),\n",
    "    ('article[data-tema]', 'ğŸ“Š Articles con atributo data-tema'),\n",
    "    ('article[data-tema=\"html\"]', 'ğŸ¯ Article con data-tema=\"html\" exacto'),\n",
    "    ('span[data-nivel^=\"fa\"]', 'ğŸ” Span con data-nivel que empiece con \"fa\"'),\n",
    "    ('a[href*=\"scrape\"]', 'ğŸŒ Enlaces cuyo href contenga \"scrape\"'),\n",
    "    \n",
    "    # Pseudo-selectores\n",
    "    ('li:first-child', 'ğŸ¥‡ Primer li hijo'),\n",
    "    ('li:last-child', 'ğŸ Ãšltimo li hijo'),\n",
    "    ('tr:nth-child(2)', '2ï¸âƒ£ Segunda fila de tabla'),\n",
    "    \n",
    "    # Selectores de hermanos\n",
    "    ('h2 + p', 'ğŸ‘« PÃ¡rrafo inmediatamente despuÃ©s de h2'),\n",
    "    ('h2 ~ p', 'ğŸ‘¥ Todos los pÃ¡rrafos hermanos despuÃ©s de h2'),\n",
    "    \n",
    "    # Selectores mÃºltiples\n",
    "    ('h1, h2, h3', 'ğŸ“Š Todos los tÃ­tulos principales'),\n",
    "    ('.contenido, .importante', 'ğŸ¨ Elementos con cualquiera de estas clases'),\n",
    "]\n",
    "\n",
    "# Ejecutar cada selector y mostrar resultados\n",
    "for selector, descripcion in selectores_css:\n",
    "    try:\n",
    "        elementos = soup.select(selector)\n",
    "        print(f\"\\n{descripcion}\")\n",
    "        print(f\"   ğŸ¯ Selector: {selector}\")\n",
    "        print(f\"   ğŸ“Š Elementos encontrados: {len(elementos)}\")\n",
    "        \n",
    "        if elementos:\n",
    "            # Mostrar informaciÃ³n del primer resultado\n",
    "            primer_elemento = elementos[0]\n",
    "            texto = primer_elemento.text.strip()[:50]\n",
    "            if len(primer_elemento.text.strip()) > 50:\n",
    "                texto += '...'\n",
    "            \n",
    "            # InformaciÃ³n adicional del elemento\n",
    "            tag_name = primer_elemento.name\n",
    "            elem_id = primer_elemento.get('id', '')\n",
    "            elem_class = primer_elemento.get('class', [])\n",
    "            \n",
    "            print(f\"   ğŸ” Primer resultado: <{tag_name}>\")\n",
    "            if elem_id:\n",
    "                print(f\"      ğŸ†” ID: {elem_id}\")\n",
    "            if elem_class:\n",
    "                print(f\"      ğŸ¨ Clases: {', '.join(elem_class)}\")\n",
    "            print(f\"      ğŸ“ Texto: '{texto}'\")\n",
    "            \n",
    "            # Si hay mÃºltiples resultados, mostrar resumen\n",
    "            if len(elementos) > 1:\n",
    "                tags_encontrados = [elem.name for elem in elementos]\n",
    "                from collections import Counter\n",
    "                conteo_tags = Counter(tags_encontrados)\n",
    "                print(f\"      ğŸ“ˆ DistribuciÃ³n: {dict(conteo_tags)}\")\n",
    "        else:\n",
    "            print(f\"   âŒ No se encontraron elementos\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸ Error con selector '{selector}': {e}\")\n",
    "\n",
    "print(\"\\n\" + \"â•\" * 70)\n",
    "print(\"ğŸ’¡ CONSEJO: Los selectores CSS son muy poderosos y expresivos.\")\n",
    "print(\"ğŸ’¡ Practica combinÃ¡ndolos para encontrar exactamente lo que necesitas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ExtracciÃ³n de Datos de Tablas ğŸ“Š\n",
    "\n",
    "Las tablas HTML son una mina de oro para los web scrapers. Contienen datos estructurados que son perfectos para anÃ¡lisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“Š EXTRACCIÃ“N PROFESIONAL DE DATOS DE TABLA\\n\")\n",
    "print(\"â•\" * 70)\n",
    "\n",
    "# Encontrar la tabla en nuestro HTML\n",
    "tabla = soup.find('table', class_='tabla-datos')\n",
    "\n",
    "if tabla:\n",
    "    print(\"âœ… Tabla encontrada exitosamente\\n\")\n",
    "    \n",
    "    # PASO 1: Extraer encabezados de la tabla\n",
    "    print(\"1ï¸âƒ£ EXTRAYENDO ENCABEZADOS:\")\n",
    "    print(\"â”€\" * 30)\n",
    "    \n",
    "    thead = tabla.find('thead')\n",
    "    if thead:\n",
    "        encabezados_elementos = thead.find_all('th')\n",
    "        encabezados = [th.text.strip() for th in encabezados_elementos]\n",
    "        print(f\"   ğŸ“‹ Encabezados encontrados: {len(encabezados)}\")\n",
    "        print(f\"   ğŸ·ï¸ Columnas: {' | '.join(encabezados)}\")\n",
    "    else:\n",
    "        print(\"   âŒ No se encontrÃ³ thead\")\n",
    "        encabezados = []\n",
    "    \n",
    "    # PASO 2: Extraer filas de datos\n",
    "    print(\"\\n2ï¸âƒ£ EXTRAYENDO FILAS DE DATOS:\")\n",
    "    print(\"â”€\" * 30)\n",
    "    \n",
    "    tbody = tabla.find('tbody')\n",
    "    if tbody:\n",
    "        filas_elementos = tbody.find_all('tr')\n",
    "        print(f\"   ğŸ“Š Filas de datos encontradas: {len(filas_elementos)}\")\n",
    "        \n",
    "        datos_tabla = []\n",
    "        for i, fila_elemento in enumerate(filas_elementos, 1):\n",
    "            # Extraer celdas de datos (td)\n",
    "            celdas_elementos = fila_elemento.find_all('td')\n",
    "            celdas_datos = [td.text.strip() for td in celdas_elementos]\n",
    "            \n",
    "            # Extraer atributos adicionales\n",
    "            fila_id = fila_elemento.get('data-id', f'fila-{i}')\n",
    "            \n",
    "            # Almacenar datos de la fila\n",
    "            fila_completa = {\n",
    "                'id': fila_id,\n",
    "                'datos': celdas_datos,\n",
    "                'num_celdas': len(celdas_datos)\n",
    "            }\n",
    "            datos_tabla.append(fila_completa)\n",
    "            \n",
    "            # Mostrar fila procesada\n",
    "            print(f\"   ğŸ“ Fila {i} (ID: {fila_id}): {' | '.join(celdas_datos)}\")\n",
    "    else:\n",
    "        print(\"   âŒ No se encontrÃ³ tbody\")\n",
    "        datos_tabla = []\n",
    "    \n",
    "    # PASO 3: Convertir a estructura mÃ¡s Ãºtil (diccionarios)\n",
    "    if encabezados and datos_tabla:\n",
    "        print(\"\\n3ï¸âƒ£ CONVIRTIENDO A ESTRUCTURA DE DATOS:\")\n",
    "        print(\"â”€\" * 30)\n",
    "        \n",
    "        datos_estructurados = []\n",
    "        for fila in datos_tabla:\n",
    "            if len(fila['datos']) == len(encabezados):\n",
    "                # Crear diccionario combinando encabezados con datos\n",
    "                fila_dict = dict(zip(encabezados, fila['datos']))\n",
    "                fila_dict['_id'] = fila['id']  # Preservar ID original\n",
    "                datos_estructurados.append(fila_dict)\n",
    "                \n",
    "                print(f\"   ğŸ“‹ Registro: {fila_dict}\")\n",
    "            else:\n",
    "                print(f\"   âš ï¸ Fila {fila['id']}: Desajuste de columnas ({len(fila['datos'])} vs {len(encabezados)})\")\n",
    "    \n",
    "    # PASO 4: AnÃ¡lisis adicional de la tabla\n",
    "    print(\"\\n4ï¸âƒ£ ANÃLISIS ADICIONAL DE LA TABLA:\")\n",
    "    print(\"â”€\" * 30)\n",
    "    \n",
    "    # Extraer informaciÃ³n de clases CSS en celdas\n",
    "    celdas_con_clase = tabla.find_all('td', class_=True)\n",
    "    if celdas_con_clase:\n",
    "        print(f\"   ğŸ¨ Celdas con clases CSS: {len(celdas_con_clase)}\")\n",
    "        for celda in celdas_con_clase:\n",
    "            clases = celda.get('class', [])\n",
    "            texto = celda.text.strip()\n",
    "            print(f\"      â€¢ '{texto}' â†’ Clases: {', '.join(clases)}\")\n",
    "    \n",
    "    # EstadÃ­sticas finales\n",
    "    print(\"\\nğŸ“ˆ ESTADÃSTICAS DE LA TABLA:\")\n",
    "    print(\"â”€\" * 30)\n",
    "    print(f\"   ğŸ”¢ Total de columnas: {len(encabezados)}\")\n",
    "    print(f\"   ğŸ“Š Total de filas de datos: {len(datos_tabla)}\")\n",
    "    print(f\"   ğŸ“‹ Registros estructurados: {len(datos_estructurados) if 'datos_estructurados' in locals() else 0}\")\n",
    "    print(f\"   ğŸ¨ Celdas con estilos: {len(celdas_con_clase)}\")\n",
    "    \n",
    "    # BONUS: Crear DataFrame si pandas estÃ¡ disponible\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        if 'datos_estructurados' in locals() and datos_estructurados:\n",
    "            df = pd.DataFrame(datos_estructurados)\n",
    "            print(\"\\nğŸ“Š DATAFRAME DE PANDAS CREADO:\")\n",
    "            print(\"â”€\" * 30)\n",
    "            print(df)\n",
    "            print(f\"\\n   ğŸ’¾ Forma del DataFrame: {df.shape}\")\n",
    "            print(f\"   ğŸ“ Columnas: {list(df.columns)}\")\n",
    "    except ImportError:\n",
    "        print(\"\\nğŸ’¡ SUGERENCIA: Instala pandas con 'pip install pandas' para anÃ¡lisis avanzado\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No se encontrÃ³ ninguna tabla en el HTML\")\n",
    "\n",
    "print(\"\\n\" + \"â•\" * 70)\n",
    "print(\"ğŸ† Â¡ExtracciÃ³n de tabla completada exitosamente!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Manejo de Errores y Scraping Robusto ğŸ›¡ï¸\n",
    "\n",
    "Un buen web scraper debe manejar errores graciosamente. Las conexiones fallan, los sitios cambian, y los servidores se caen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "class ScraperRobusto:\n",
    "    \"\"\"\n",
    "    Clase para web scraping robusto con manejo de errores,\n",
    "    reintentos, rate limiting y mejores prÃ¡cticas.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_reintentos=3, delay_base=1, timeout=10):\n",
    "        self.max_reintentos = max_reintentos\n",
    "        self.delay_base = delay_base\n",
    "        self.timeout = timeout\n",
    "        self.session = self._crear_session_con_reintentos()\n",
    "        \n",
    "        # Headers realistas\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "            'Accept-Language': 'es-ES,es;q=0.8,en-US;q=0.5,en;q=0.3',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Connection': 'keep-alive',\n",
    "        }\n",
    "        \n",
    "        # EstadÃ­sticas\n",
    "        self.stats = {\n",
    "            'solicitudes_exitosas': 0,\n",
    "            'errores_totales': 0,\n",
    "            'reintentos_usados': 0,\n",
    "            'tiempo_total': 0\n",
    "        }\n",
    "    \n",
    "    def _crear_session_con_reintentos(self):\n",
    "        \"\"\"Crear sesiÃ³n con estrategia de reintentos automÃ¡ticos\"\"\"\n",
    "        session = requests.Session()\n",
    "        \n",
    "        # Configurar estrategia de reintentos\n",
    "        retry_strategy = Retry(\n",
    "            total=self.max_reintentos,\n",
    "            backoff_factor=1,\n",
    "            status_forcelist=[429, 500, 502, 503, 504],\n",
    "            allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"]\n",
    "        )\n",
    "        \n",
    "        adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "        session.mount(\"http://\", adapter)\n",
    "        session.mount(\"https://\", adapter)\n",
    "        \n",
    "        return session\n",
    "    \n",
    "    def _delay_aleatorio(self, base_delay=None):\n",
    "        \"\"\"Implementar delay aleatorio para parecer mÃ¡s humano\"\"\"\n",
    "        if base_delay is None:\n",
    "            base_delay = self.delay_base\n",
    "        \n",
    "        # Delay aleatorio entre base_delay y base_delay * 2\n",
    "        delay = random.uniform(base_delay, base_delay * 2)\n",
    "        print(f\"   â³ Esperando {delay:.2f} segundos...\")\n",
    "        time.sleep(delay)\n",
    "    \n",
    "    def scrape_url(self, url, parser='html.parser'):\n",
    "        \"\"\"\n",
    "        Realizar scraping robusto de una URL\n",
    "        \n",
    "        Args:\n",
    "            url: URL a scrapear\n",
    "            parser: Parser de BeautifulSoup a usar\n",
    "            \n",
    "        Returns:\n",
    "            BeautifulSoup object o None si falla\n",
    "        \"\"\"\n",
    "        print(f\"\\nğŸš€ INICIANDO SCRAPING ROBUSTO\")\n",
    "        print(f\"ğŸ¯ URL: {url}\")\n",
    "        print(f\"âš™ï¸ Parser: {parser}\")\n",
    "        print(\"â”€\" * 50)\n",
    "        \n",
    "        inicio_tiempo = time.time()\n",
    "        \n",
    "        for intento in range(self.max_reintentos + 1):\n",
    "            try:\n",
    "                print(f\"\\nğŸ”„ Intento {intento + 1} de {self.max_reintentos + 1}\")\n",
    "                \n",
    "                # Hacer la solicitud\n",
    "                response = self.session.get(\n",
    "                    url, \n",
    "                    headers=self.headers, \n",
    "                    timeout=self.timeout\n",
    "                )\n",
    "                \n",
    "                # Verificar cÃ³digo de estado\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # Verificar que tenemos contenido HTML\n",
    "                content_type = response.headers.get('content-type', '').lower()\n",
    "                if 'text/html' not in content_type:\n",
    "                    raise ValueError(f\"Contenido no HTML: {content_type}\")\n",
    "                \n",
    "                # Parsear con Beautiful Soup\n",
    "                soup = BeautifulSoup(response.content, parser)\n",
    "                \n",
    "                # Validar que el HTML es vÃ¡lido\n",
    "                if not soup.find():\n",
    "                    raise ValueError(\"HTML vacÃ­o o invÃ¡lido\")\n",
    "                \n",
    "                # Ã‰xito!\n",
    "                tiempo_transcurrido = time.time() - inicio_tiempo\n",
    "                self.stats['solicitudes_exitosas'] += 1\n",
    "                self.stats['tiempo_total'] += tiempo_transcurrido\n",
    "                \n",
    "                print(f\"âœ… Â¡Scraping exitoso!\")\n",
    "                print(f\"ğŸ“Š CÃ³digo de estado: {response.status_code}\")\n",
    "                print(f\"ğŸ“„ TamaÃ±o del HTML: {len(response.content):,} bytes\")\n",
    "                print(f\"â±ï¸ Tiempo total: {tiempo_transcurrido:.2f} segundos\")\n",
    "                print(f\"ğŸŒ Servidor: {urlparse(url).netloc}\")\n",
    "                \n",
    "                # InformaciÃ³n adicional del HTML\n",
    "                title = soup.find('title')\n",
    "                if title:\n",
    "                    print(f\"ğŸ“‹ TÃ­tulo: {title.text.strip()}\")\n",
    "                \n",
    "                return soup\n",
    "                \n",
    "            except requests.exceptions.Timeout:\n",
    "                print(f\"â±ï¸ Timeout en intento {intento + 1}\")\n",
    "                self.stats['errores_totales'] += 1\n",
    "                \n",
    "            except requests.exceptions.ConnectionError:\n",
    "                print(f\"ğŸ”Œ Error de conexiÃ³n en intento {intento + 1}\")\n",
    "                self.stats['errores_totales'] += 1\n",
    "                \n",
    "            except requests.exceptions.HTTPError as e:\n",
    "                status_code = e.response.status_code\n",
    "                print(f\"âŒ Error HTTP {status_code} en intento {intento + 1}\")\n",
    "                self.stats['errores_totales'] += 1\n",
    "                \n",
    "                # Algunos errores no valen la pena reintentar\n",
    "                if status_code in [404, 403, 401]:\n",
    "                    print(f\"   ğŸš« Error {status_code} - No reintentando\")\n",
    "                    break\n",
    "                    \n",
    "            except ValueError as e:\n",
    "                print(f\"â— Error de validaciÃ³n: {e}\")\n",
    "                self.stats['errores_totales'] += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"ğŸ’¥ Error inesperado: {type(e).__name__}: {e}\")\n",
    "                self.stats['errores_totales'] += 1\n",
    "            \n",
    "            # Si no es el Ãºltimo intento, esperar antes de reintentar\n",
    "            if intento < self.max_reintentos:\n",
    "                self.stats['reintentos_usados'] += 1\n",
    "                delay = self.delay_base * (2 ** intento)  # Backoff exponencial\n",
    "                print(f\"â³ Esperando {delay} segundos antes del siguiente intento...\")\n",
    "                time.sleep(delay)\n",
    "        \n",
    "        # Si llegamos aquÃ­, todos los intentos fallaron\n",
    "        tiempo_total = time.time() - inicio_tiempo\n",
    "        self.stats['tiempo_total'] += tiempo_total\n",
    "        \n",
    "        print(f\"\\nğŸ’¥ SCRAPING FALLIDO despuÃ©s de {self.max_reintentos + 1} intentos\")\n",
    "        print(f\"â±ï¸ Tiempo total gastado: {tiempo_total:.2f} segundos\")\n",
    "        return None\n",
    "    \n",
    "    def mostrar_estadisticas(self):\n",
    "        \"\"\"Mostrar estadÃ­sticas del scraper\"\"\"\n",
    "        print(\"\\nğŸ“Š ESTADÃSTICAS DEL SCRAPER ROBUSTO\")\n",
    "        print(\"â•\" * 50)\n",
    "        print(f\"âœ… Solicitudes exitosas: {self.stats['solicitudes_exitosas']}\")\n",
    "        print(f\"âŒ Errores totales: {self.stats['errores_totales']}\")\n",
    "        print(f\"ğŸ”„ Reintentos usados: {self.stats['reintentos_usados']}\")\n",
    "        print(f\"â±ï¸ Tiempo total: {self.stats['tiempo_total']:.2f} segundos\")\n",
    "        \n",
    "        if self.stats['solicitudes_exitosas'] > 0:\n",
    "            promedio = self.stats['tiempo_total'] / self.stats['solicitudes_exitosas']\n",
    "            print(f\"ğŸ“ˆ Tiempo promedio por solicitud: {promedio:.2f} segundos\")\n",
    "\n",
    "# DemostraciÃ³n del scraper robusto\n",
    "print(\"ğŸ›¡ï¸ DEMOSTRACIÃ“N DE SCRAPER ROBUSTO\\n\")\n",
    "print(\"â•\" * 60)\n",
    "\n",
    "# Crear instancia del scraper\n",
    "scraper = ScraperRobusto(max_reintentos=2, delay_base=0.5)\n",
    "\n",
    "# URLs para probar\n",
    "urls_prueba = [\n",
    "    'http://quotes.toscrape.com/',  # URL vÃ¡lida\n",
    "    'http://quotes.toscrape.com/page/999/',  # URL que darÃ¡ 404\n",
    "]\n",
    "\n",
    "resultados = []\n",
    "for url in urls_prueba:\n",
    "    resultado = scraper.scrape_url(url)\n",
    "    resultados.append({\n",
    "        'url': url,\n",
    "        'exitoso': resultado is not None,\n",
    "        'soup': resultado\n",
    "    })\n",
    "\n",
    "# Mostrar estadÃ­sticas finales\n",
    "scraper.mostrar_estadisticas()\n",
    "\n",
    "# Resumen de resultados\n",
    "print(\"\\nğŸ“‹ RESUMEN DE RESULTADOS\")\n",
    "print(\"â•\" * 50)\n",
    "for i, resultado in enumerate(resultados, 1):\n",
    "    status = \"âœ… Ã‰xito\" if resultado['exitoso'] else \"âŒ Fallo\"\n",
    "    print(f\"{i}. {resultado['url']} â†’ {status}\")\n",
    "\n",
    "print(\"\\nğŸ¯ El manejo robusto de errores es ESENCIAL para scrapers en producciÃ³n.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Ejercicio PrÃ¡ctico Avanzado: Tu Turno ğŸ’ª\n",
    "\n",
    "Â¡Hora de poner en prÃ¡ctica todo lo aprendido! Completa este ejercicio desafiante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EJERCICIO AVANZADO: Scraper de Noticias TecnolÃ³gicas\n",
    "# Objetivo: Crear un scraper completo que extraiga mÃºltiples tipos de datos\n",
    "\n",
    "# HTML de ejemplo: Portal de noticias tech\n",
    "html_noticias = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"es\">\n",
    "<head>\n",
    "    <title>TechNews - Las Ãšltimas Noticias de TecnologÃ­a</title>\n",
    "    <meta name=\"description\" content=\"Portal de noticias tecnolÃ³gicas\">\n",
    "</head>\n",
    "<body>\n",
    "    <header>\n",
    "        <h1>ğŸš€ TechNews</h1>\n",
    "        <nav class=\"main-nav\">\n",
    "            <ul>\n",
    "                <li><a href=\"/ia\">Inteligencia Artificial</a></li>\n",
    "                <li><a href=\"/web\">Desarrollo Web</a></li>\n",
    "                <li><a href=\"/mobile\">Apps MÃ³viles</a></li>\n",
    "            </ul>\n",
    "        </nav>\n",
    "    </header>\n",
    "    \n",
    "    <main>\n",
    "        <section id=\"noticias-destacadas\">\n",
    "            <h2>ğŸ“° Noticias Destacadas</h2>\n",
    "            \n",
    "            <article class=\"noticia destacada\" data-id=\"1\" data-categoria=\"ia\">\n",
    "                <header class=\"noticia-header\">\n",
    "                    <h3 class=\"titulo\">ChatGPT revoluciona la programaciÃ³n en 2024</h3>\n",
    "                    <div class=\"metadatos\">\n",
    "                        <span class=\"autor\" data-autor-id=\"123\">MarÃ­a GarcÃ­a</span>\n",
    "                        <time class=\"fecha\" datetime=\"2024-01-15T10:30:00\">15 de Enero, 2024</time>\n",
    "                        <span class=\"categoria\">ğŸ¤– IA</span>\n",
    "                        <div class=\"estadisticas\">\n",
    "                            <span class=\"vistas\" data-count=\"1250\">1,250 vistas</span>\n",
    "                            <span class=\"comentarios\" data-count=\"45\">45 comentarios</span>\n",
    "                        </div>\n",
    "                    </div>\n",
    "                </header>\n",
    "                <div class=\"contenido\">\n",
    "                    <p class=\"resumen\">La nueva versiÃ³n de ChatGPT introduce capacidades avanzadas para generar cÃ³digo de alta calidad, transformando la manera en que los desarrolladores trabajan.</p>\n",
    "                    <div class=\"tags\">\n",
    "                        <span class=\"tag\">IA</span>\n",
    "                        <span class=\"tag\">programaciÃ³n</span>\n",
    "                        <span class=\"tag\">productividad</span>\n",
    "                    </div>\n",
    "                </div>\n",
    "                <footer class=\"noticia-footer\">\n",
    "                    <a href=\"/noticia/1\" class=\"leer-mas\">Leer artÃ­culo completo</a>\n",
    "                    <div class=\"acciones\">\n",
    "                        <button class=\"like\" data-likes=\"89\">â¤ï¸ 89</button>\n",
    "                        <button class=\"share\">ğŸ”— Compartir</button>\n",
    "                    </div>\n",
    "                </footer>\n",
    "            </article>\n",
    "            \n",
    "            <article class=\"noticia\" data-id=\"2\" data-categoria=\"web\">\n",
    "                <header class=\"noticia-header\">\n",
    "                    <h3 class=\"titulo\">React 19: Nuevas caracterÃ­sticas que cambiarÃ¡n todo</h3>\n",
    "                    <div class=\"metadatos\">\n",
    "                        <span class=\"autor\" data-autor-id=\"456\">Carlos Ruiz</span>\n",
    "                        <time class=\"fecha\" datetime=\"2024-01-14T16:45:00\">14 de Enero, 2024</time>\n",
    "                        <span class=\"categoria\">ğŸ’» Desarrollo Web</span>\n",
    "                        <div class=\"estadisticas\">\n",
    "                            <span class=\"vistas\" data-count=\"890\">890 vistas</span>\n",
    "                            <span class=\"comentarios\" data-count=\"23\">23 comentarios</span>\n",
    "                        </div>\n",
    "                    </div>\n",
    "                </header>\n",
    "                <div class=\"contenido\">\n",
    "                    <p class=\"resumen\">React 19 incluye mejoras significativas en rendimiento y nuevas APIs que simplifican el desarrollo de aplicaciones complejas.</p>\n",
    "                    <div class=\"tags\">\n",
    "                        <span class=\"tag\">React</span>\n",
    "                        <span class=\"tag\">JavaScript</span>\n",
    "                        <span class=\"tag\">frontend</span>\n",
    "                    </div>\n",
    "                </div>\n",
    "                <footer class=\"noticia-footer\">\n",
    "                    <a href=\"/noticia/2\" class=\"leer-mas\">Leer artÃ­culo completo</a>\n",
    "                    <div class=\"acciones\">\n",
    "                        <button class=\"like\" data-likes=\"156\">â¤ï¸ 156</button>\n",
    "                        <button class=\"share\">ğŸ”— Compartir</button>\n",
    "                    </div>\n",
    "                </footer>\n",
    "            </article>\n",
    "            \n",
    "            <article class=\"noticia\" data-id=\"3\" data-categoria=\"mobile\">\n",
    "                <header class=\"noticia-header\">\n",
    "                    <h3 class=\"titulo\">Flutter vs React Native: Comparativa 2024</h3>\n",
    "                    <div class=\"metadatos\">\n",
    "                        <span class=\"autor\" data-autor-id=\"789\">Ana LÃ³pez</span>\n",
    "                        <time class=\"fecha\" datetime=\"2024-01-13T09:15:00\">13 de Enero, 2024</time>\n",
    "                        <span class=\"categoria\">ğŸ“± Apps MÃ³viles</span>\n",
    "                        <div class=\"estadisticas\">\n",
    "                            <span class=\"vistas\" data-count=\"2100\">2,100 vistas</span>\n",
    "                            <span class=\"comentarios\" data-count=\"78\">78 comentarios</span>\n",
    "                        </div>\n",
    "                    </div>\n",
    "                </header>\n",
    "                <div class=\"contenido\">\n",
    "                    <p class=\"resumen\">AnÃ¡lisis completo de las dos tecnologÃ­as mÃ¡s populares para desarrollo mÃ³vil multiplataforma, con ejemplos prÃ¡cticos y casos de uso.</p>\n",
    "                    <div class=\"tags\">\n",
    "                        <span class=\"tag\">Flutter</span>\n",
    "                        <span class=\"tag\">React Native</span>\n",
    "                        <span class=\"tag\">mobile</span>\n",
    "                    </div>\n",
    "                </div>\n",
    "                <footer class=\"noticia-footer\">\n",
    "                    <a href=\"/noticia/3\" class=\"leer-mas\">Leer artÃ­culo completo</a>\n",
    "                    <div class=\"acciones\">\n",
    "                        <button class=\"like\" data-likes=\"203\">â¤ï¸ 203</button>\n",
    "                        <button class=\"share\">ğŸ”— Compartir</button>\n",
    "                    </div>\n",
    "                </footer>\n",
    "            </article>\n",
    "        </section>\n",
    "        \n",
    "        <aside id=\"trending\">\n",
    "            <h2>ğŸ”¥ Trending Topics</h2>\n",
    "            <ul class=\"trending-list\">\n",
    "                <li data-trend=\"1\">#IA <span class=\"count\">(1,250 menciones)</span></li>\n",
    "                <li data-trend=\"2\">#React19 <span class=\"count\">(890 menciones)</span></li>\n",
    "                <li data-trend=\"3\">#Python <span class=\"count\">(650 menciones)</span></li>\n",
    "            </ul>\n",
    "        </aside>\n",
    "    </main>\n",
    "    \n",
    "    <footer>\n",
    "        <p>&copy; 2024 TechNews - Mantente actualizado</p>\n",
    "        <div class=\"stats-globales\">\n",
    "            <span>ğŸ‘¥ 15,430 lectores activos</span>\n",
    "            <span>ğŸ“° 1,250 artÃ­culos publicados</span>\n",
    "        </div>\n",
    "    </footer>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "print(\"ğŸ‹ï¸ EJERCICIO AVANZADO: SCRAPER DE NOTICIAS TECH\\n\")\n",
    "print(\"â•\" * 60)\n",
    "print(\"Instrucciones:\")\n",
    "print(\"1. Extrae TODA la informaciÃ³n de cada artÃ­culo\")\n",
    "print(\"2. Procesa los metadatos y estadÃ­sticas\")\n",
    "print(\"3. Extrae trending topics\")\n",
    "print(\"4. Calcula estadÃ­sticas avanzadas\")\n",
    "print(\"5. Estructura todo en formato JSON\")\n",
    "\n",
    "def scraper_noticias_avanzado(html):\n",
    "    \"\"\"\n",
    "    Scraper avanzado para portal de noticias.\n",
    "    Debe extraer TODA la informaciÃ³n disponible.\n",
    "    \"\"\"\n",
    "    # PARSEAR HTML\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # ESTRUCTURA DE DATOS COMPLETA\n",
    "    portal_data = {\n",
    "        'info_portal': {},\n",
    "        'navegacion': [],\n",
    "        'noticias': [],\n",
    "        'trending': [],\n",
    "        'estadisticas_globales': {},\n",
    "        'resumen_scraping': {}\n",
    "    }\n",
    "    \n",
    "    # 1. INFORMACIÃ“N DEL PORTAL\n",
    "    title = soup.find('title')\n",
    "    description = soup.find('meta', attrs={'name': 'description'})\n",
    "    \n",
    "    portal_data['info_portal'] = {\n",
    "        'titulo': title.text if title else 'Sin tÃ­tulo',\n",
    "        'descripcion': description.get('content', 'Sin descripciÃ³n') if description else 'Sin descripciÃ³n',\n",
    "        'nombre_sitio': soup.find('h1').text if soup.find('h1') else 'Desconocido'\n",
    "    }\n",
    "    \n",
    "    # 2. NAVEGACIÃ“N\n",
    "    nav = soup.find('nav', class_='main-nav')\n",
    "    if nav:\n",
    "        enlaces_nav = nav.find_all('a')\n",
    "        portal_data['navegacion'] = [\n",
    "            {\n",
    "                'texto': enlace.text.strip(),\n",
    "                'href': enlace.get('href', ''),\n",
    "                'seccion': enlace.get('href', '').replace('/', '')\n",
    "            } for enlace in enlaces_nav\n",
    "        ]\n",
    "    \n",
    "    # 3. EXTRACCIÃ“N COMPLETA DE NOTICIAS\n",
    "    noticias = soup.find_all('article', class_='noticia')\n",
    "    \n",
    "    for noticia in noticias:\n",
    "        # Datos bÃ¡sicos\n",
    "        noticia_data = {\n",
    "            'id': noticia.get('data-id'),\n",
    "            'categoria': noticia.get('data-categoria'),\n",
    "            'es_destacada': 'destacada' in noticia.get('class', [])\n",
    "        }\n",
    "        \n",
    "        # TÃ­tulo\n",
    "        titulo_elem = noticia.find('h3', class_='titulo')\n",
    "        noticia_data['titulo'] = titulo_elem.text.strip() if titulo_elem else 'Sin tÃ­tulo'\n",
    "        \n",
    "        # Metadatos del autor\n",
    "        autor_elem = noticia.find('span', class_='autor')\n",
    "        if autor_elem:\n",
    "            noticia_data['autor'] = {\n",
    "                'nombre': autor_elem.text.strip(),\n",
    "                'id': autor_elem.get('data-autor-id')\n",
    "            }\n",
    "        \n",
    "        # Fecha\n",
    "        fecha_elem = noticia.find('time', class_='fecha')\n",
    "        if fecha_elem:\n",
    "            noticia_data['fecha'] = {\n",
    "                'texto': fecha_elem.text.strip(),\n",
    "                'datetime': fecha_elem.get('datetime'),\n",
    "                'timestamp': fecha_elem.get('datetime')\n",
    "            }\n",
    "        \n",
    "        # CategorÃ­a mostrada\n",
    "        categoria_elem = noticia.find('span', class_='categoria')\n",
    "        noticia_data['categoria_display'] = categoria_elem.text.strip() if categoria_elem else 'Sin categorÃ­a'\n",
    "        \n",
    "        # EstadÃ­sticas de engagement\n",
    "        vistas_elem = noticia.find('span', class_='vistas')\n",
    "        comentarios_elem = noticia.find('span', class_='comentarios')\n",
    "        likes_elem = noticia.find('button', class_='like')\n",
    "        \n",
    "        noticia_data['estadisticas'] = {\n",
    "            'vistas': {\n",
    "                'texto': vistas_elem.text.strip() if vistas_elem else '0',\n",
    "                'count': int(vistas_elem.get('data-count', 0)) if vistas_elem else 0\n",
    "            },\n",
    "            'comentarios': {\n",
    "                'texto': comentarios_elem.text.strip() if comentarios_elem else '0',\n",
    "                'count': int(comentarios_elem.get('data-count', 0)) if comentarios_elem else 0\n",
    "            },\n",
    "            'likes': {\n",
    "                'texto': likes_elem.text.strip() if likes_elem else '0',\n",
    "                'count': int(likes_elem.get('data-likes', 0)) if likes_elem else 0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Contenido\n",
    "        resumen_elem = noticia.find('p', class_='resumen')\n",
    "        noticia_data['resumen'] = resumen_elem.text.strip() if resumen_elem else 'Sin resumen'\n",
    "        \n",
    "        # Tags\n",
    "        tags_container = noticia.find('div', class_='tags')\n",
    "        if tags_container:\n",
    "            tags = [tag.text.strip() for tag in tags_container.find_all('span', class_='tag')]\n",
    "            noticia_data['tags'] = tags\n",
    "        else:\n",
    "            noticia_data['tags'] = []\n",
    "        \n",
    "        # Enlaces\n",
    "        leer_mas = noticia.find('a', class_='leer-mas')\n",
    "        noticia_data['enlace_completo'] = leer_mas.get('href') if leer_mas else None\n",
    "        \n",
    "        # MÃ©tricas calculadas\n",
    "        noticia_data['metricas_calculadas'] = {\n",
    "            'engagement_score': (\n",
    "                noticia_data['estadisticas']['likes']['count'] * 3 +\n",
    "                noticia_data['estadisticas']['comentarios']['count'] * 2 +\n",
    "                noticia_data['estadisticas']['vistas']['count'] * 0.01\n",
    "            ),\n",
    "            'palabras_titulo': len(noticia_data['titulo'].split()),\n",
    "            'palabras_resumen': len(noticia_data['resumen'].split()),\n",
    "            'num_tags': len(noticia_data['tags'])\n",
    "        }\n",
    "        \n",
    "        portal_data['noticias'].append(noticia_data)\n",
    "    \n",
    "    # 4. TRENDING TOPICS\n",
    "    trending_list = soup.find('ul', class_='trending-list')\n",
    "    if trending_list:\n",
    "        trending_items = trending_list.find_all('li')\n",
    "        for item in trending_items:\n",
    "            trend_data = {\n",
    "                'posicion': item.get('data-trend'),\n",
    "                'hashtag': item.text.split('(')[0].strip(),\n",
    "                'menciones_texto': item.find('span', class_='count').text.strip() if item.find('span', class_='count') else '0'\n",
    "            }\n",
    "            # Extraer nÃºmero de menciones\n",
    "            import re\n",
    "            menciones_match = re.search(r'\\((\\d+[,\\d]*)', trend_data['menciones_texto'])\n",
    "            if menciones_match:\n",
    "                menciones_num = menciones_match.group(1).replace(',', '')\n",
    "                trend_data['menciones_count'] = int(menciones_num)\n",
    "            else:\n",
    "                trend_data['menciones_count'] = 0\n",
    "            \n",
    "            portal_data['trending'].append(trend_data)\n",
    "    \n",
    "    # 5. ESTADÃSTICAS GLOBALES\n",
    "    stats_footer = soup.find('div', class_='stats-globales')\n",
    "    if stats_footer:\n",
    "        stats_text = stats_footer.text\n",
    "        import re\n",
    "        \n",
    "        # Extraer nÃºmeros de las estadÃ­sticas\n",
    "        lectores_match = re.search(r'(\\d+[,\\d]*) lectores', stats_text)\n",
    "        articulos_match = re.search(r'(\\d+[,\\d]*) artÃ­culos', stats_text)\n",
    "        \n",
    "        portal_data['estadisticas_globales'] = {\n",
    "            'lectores_activos': int(lectores_match.group(1).replace(',', '')) if lectores_match else 0,\n",
    "            'articulos_publicados': int(articulos_match.group(1).replace(',', '')) if articulos_match else 0\n",
    "        }\n",
    "    \n",
    "    # 6. RESUMEN DEL SCRAPING\n",
    "    total_vistas = sum(n['estadisticas']['vistas']['count'] for n in portal_data['noticias'])\n",
    "    total_likes = sum(n['estadisticas']['likes']['count'] for n in portal_data['noticias'])\n",
    "    total_comentarios = sum(n['estadisticas']['comentarios']['count'] for n in portal_data['noticias'])\n",
    "    \n",
    "    todos_tags = []\n",
    "    for noticia in portal_data['noticias']:\n",
    "        todos_tags.extend(noticia['tags'])\n",
    "    \n",
    "    from collections import Counter\n",
    "    tags_mas_comunes = Counter(todos_tags).most_common(5)\n",
    "    \n",
    "    portal_data['resumen_scraping'] = {\n",
    "        'total_noticias': len(portal_data['noticias']),\n",
    "        'noticias_destacadas': len([n for n in portal_data['noticias'] if n['es_destacada']]),\n",
    "        'categorias_unicas': len(set(n['categoria'] for n in portal_data['noticias'])),\n",
    "        'autores_unicos': len(set(n['autor']['nombre'] for n in portal_data['noticias'] if 'autor' in n)),\n",
    "        'engagement_total': {\n",
    "            'vistas': total_vistas,\n",
    "            'likes': total_likes,\n",
    "            'comentarios': total_comentarios\n",
    "        },\n",
    "        'tags_mas_populares': tags_mas_comunes,\n",
    "        'fecha_scraping': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    \n",
    "    return portal_data\n",
    "\n",
    "# Â¡EJECUTAR EL SCRAPER AVANZADO!\n",
    "print(\"\\nğŸš€ EJECUTANDO SCRAPER AVANZADO...\\n\")\n",
    "\n",
    "datos_extraidos = scraper_noticias_avanzado(html_noticias)\n",
    "\n",
    "# MOSTRAR RESULTADOS DE FORMA ORGANIZADA\n",
    "import json\n",
    "\n",
    "print(\"ğŸ“Š RESULTADOS DEL SCRAPING AVANZADO\")\n",
    "print(\"â•\" * 60)\n",
    "\n",
    "# Info del portal\n",
    "print(f\"\\nğŸŒ PORTAL: {datos_extraidos['info_portal']['nombre_sitio']}\")\n",
    "print(f\"ğŸ“‹ TÃ­tulo: {datos_extraidos['info_portal']['titulo']}\")\n",
    "print(f\"ğŸ“ DescripciÃ³n: {datos_extraidos['info_portal']['descripcion']}\")\n",
    "\n",
    "# NavegaciÃ³n\n",
    "print(f\"\\nğŸ§­ NAVEGACIÃ“N ({len(datos_extraidos['navegacion'])} secciones):\")\n",
    "for nav in datos_extraidos['navegacion']:\n",
    "    print(f\"   â€¢ {nav['texto']} â†’ {nav['href']}\")\n",
    "\n",
    "# Noticias\n",
    "print(f\"\\nğŸ“° NOTICIAS EXTRAÃDAS ({len(datos_extraidos['noticias'])} artÃ­culos):\")\n",
    "for i, noticia in enumerate(datos_extraidos['noticias'], 1):\n",
    "    destacada = \"â­\" if noticia['es_destacada'] else \"  \"\n",
    "    print(f\"\\n{destacada} {i}. {noticia['titulo']}\")\n",
    "    print(f\"      ğŸ‘¤ Autor: {noticia.get('autor', {}).get('nombre', 'Desconocido')}\")\n",
    "    print(f\"      ğŸ“… Fecha: {noticia.get('fecha', {}).get('texto', 'Sin fecha')}\")\n",
    "    print(f\"      ğŸ·ï¸ CategorÃ­a: {noticia['categoria_display']}\")\n",
    "    print(f\"      ğŸ“Š Engagement: {noticia['estadisticas']['vistas']['count']} vistas, {noticia['estadisticas']['likes']['count']} likes\")\n",
    "    print(f\"      ğŸ·ï¸ Tags: {', '.join(noticia['tags']) if noticia['tags'] else 'Sin tags'}\")\n",
    "    print(f\"      ğŸ’¯ Score: {noticia['metricas_calculadas']['engagement_score']:.1f}\")\n",
    "\n",
    "# Trending\n",
    "print(f\"\\nğŸ”¥ TRENDING TOPICS ({len(datos_extraidos['trending'])} temas):\")\n",
    "for trend in datos_extraidos['trending']:\n",
    "    print(f\"   {trend['posicion']}. {trend['hashtag']} - {trend['menciones_count']:,} menciones\")\n",
    "\n",
    "# EstadÃ­sticas globales\n",
    "print(f\"\\nğŸ“ˆ ESTADÃSTICAS GLOBALES:\")\n",
    "print(f\"   ğŸ‘¥ Lectores activos: {datos_extraidos['estadisticas_globales']['lectores_activos']:,}\")\n",
    "print(f\"   ğŸ“° ArtÃ­culos publicados: {datos_extraidos['estadisticas_globales']['articulos_publicados']:,}\")\n",
    "\n",
    "# Resumen del scraping\n",
    "resumen = datos_extraidos['resumen_scraping']\n",
    "print(f\"\\nğŸ“‹ RESUMEN DEL SCRAPING:\")\n",
    "print(f\"   ğŸ“° Total de noticias: {resumen['total_noticias']}\")\n",
    "print(f\"   â­ Noticias destacadas: {resumen['noticias_destacadas']}\")\n",
    "print(f\"   ğŸ—‚ï¸ CategorÃ­as Ãºnicas: {resumen['categorias_unicas']}\")\n",
    "print(f\"   âœï¸ Autores Ãºnicos: {resumen['autores_unicos']}\")\n",
    "print(f\"   ğŸ‘€ Total de vistas: {resumen['engagement_total']['vistas']:,}\")\n",
    "print(f\"   â¤ï¸ Total de likes: {resumen['engagement_total']['likes']:,}\")\n",
    "print(f\"   ğŸ’¬ Total de comentarios: {resumen['engagement_total']['comentarios']:,}\")\n",
    "print(f\"   ğŸ·ï¸ Tags mÃ¡s populares: {', '.join([f'{tag} ({count})' for tag, count in resumen['tags_mas_populares']])}\")\n",
    "\n",
    "print(\"\\n\" + \"â•\" * 60)\n",
    "print(\"ğŸ‰ Â¡EJERCICIO AVANZADO COMPLETADO EXITOSAMENTE!\")\n",
    "print(\"ğŸ† Has demostrado dominio avanzado de web scraping.\")\n",
    "print(f\"ğŸ“… Scraping realizado: {resumen['fecha_scraping']}\")\n",
    "\n",
    "# Guardar en JSON para anÃ¡lisis posterior\n",
    "print(\"\\nğŸ’¾ Datos tambiÃ©n disponibles en formato JSON para anÃ¡lisis posterior.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Herramientas del Navegador para Web Scraping ğŸ› ï¸\n",
    "\n",
    "Las herramientas de desarrollo del navegador son tu **mejor amigo** para el web scraping. Te permiten inspeccionar, probar y entender la estructura de cualquier sitio web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸš€ Accesos RÃ¡pidos a DevTools\n",
    "\n",
    "| AcciÃ³n | Windows/Linux | Mac | DescripciÃ³n |\n",
    "|--------|---------------|-----|-------------|\n",
    "| **Abrir DevTools** | `F12` o `Ctrl+Shift+I` | `Cmd+Option+I` | Abre las herramientas de desarrollador |\n",
    "| **Inspeccionar elemento** | `Ctrl+Shift+C` | `Cmd+Shift+C` | Activa el selector de elementos |\n",
    "| **Consola JavaScript** | `Ctrl+Shift+J` | `Cmd+Option+J` | Abre directamente la consola |\n",
    "| **Ver cÃ³digo fuente** | `Ctrl+U` | `Cmd+U` | Muestra el HTML completo |\n",
    "| **Buscar en pÃ¡gina** | `Ctrl+F` | `Cmd+F` | Buscar texto en la pÃ¡gina actual |\n",
    "\n",
    "### ğŸ•µï¸ TÃ©cnicas de InspecciÃ³n Avanzadas\n",
    "\n",
    "#### 1. **Inspector de Elementos**\n",
    "```\n",
    "â€¢ Click derecho en cualquier elemento â†’ \"Inspeccionar\"\n",
    "â€¢ Navega por la jerarquÃ­a HTML\n",
    "â€¢ Ve estilos CSS aplicados en tiempo real\n",
    "â€¢ Observa cambios dinÃ¡micos en el DOM\n",
    "```\n",
    "\n",
    "#### 2. **Consola JavaScript para Testing**\n",
    "```javascript\n",
    "// Probar selectores CSS\n",
    "document.querySelectorAll('.clase')\n",
    "document.querySelector('#mi-id')\n",
    "\n",
    "// Probar XPath\n",
    "$x('//div[@class=\"ejemplo\"]')\n",
    "$x('//a[contains(@href, \"scrape\")]')\n",
    "\n",
    "// Obtener texto de elementos\n",
    "document.querySelector('h1').textContent\n",
    "\n",
    "// Ver atributos\n",
    "document.querySelector('img').getAttribute('src')\n",
    "```\n",
    "\n",
    "#### 3. **PestaÃ±a Network (Red)**\n",
    "```\n",
    "âœ… Ve todas las solicitudes HTTP\n",
    "âœ… Examina headers y cookies\n",
    "âœ… Identifica APIs y endpoints\n",
    "âœ… Analiza tiempos de respuesta\n",
    "âœ… Detecta llamadas AJAX/fetch\n",
    "```\n",
    "\n",
    "#### 4. **Copiar Selectores AutomÃ¡ticamente**\n",
    "```\n",
    "â€¢ Click derecho en elemento en Inspector\n",
    "â€¢ Copy â†’ Copy selector (CSS)\n",
    "â€¢ Copy â†’ Copy XPath\n",
    "â€¢ Copy â†’ Copy full XPath\n",
    "```\n",
    "\n",
    "### ğŸ¯ Trucos Profesionales\n",
    "\n",
    "#### **Buscar en todo el HTML**\n",
    "- `Ctrl+F` en la pestaÃ±a Elements\n",
    "- Busca por: texto, selectores CSS, XPath\n",
    "- Ejemplo: buscar `class=\"noticia\"` encuentra todos los elementos con esa clase\n",
    "\n",
    "#### **Variables especiales en consola**\n",
    "- `$0`: Ãšltimo elemento seleccionado en Inspector\n",
    "- `$1`, `$2`, etc.: Elementos seleccionados anteriormente\n",
    "- `$$('selector')`: Equivale a `querySelectorAll`\n",
    "- `$('selector')`: Equivale a `querySelector`\n",
    "\n",
    "#### **Monitorear cambios dinÃ¡micos**\n",
    "- Click derecho en elemento â†’ \"Break on...\" â†’ \"Subtree modifications\"\n",
    "- Pausa cuando JavaScript modifica el contenido\n",
    "- Ãštil para sitios con contenido que cambia dinÃ¡micamente\n",
    "\n",
    "### ğŸ” Estrategias de InvestigaciÃ³n\n",
    "\n",
    "#### **Para encontrar el selector perfecto:**\n",
    "1. **Inspecciona** el elemento que te interesa\n",
    "2. **Mira los atributos** disponibles (`id`, `class`, `data-*`)\n",
    "3. **Prueba selectores** en la consola\n",
    "4. **Verifica unicidad** - Â¿selecciona solo lo que quieres?\n",
    "5. **Considera la estabilidad** - Â¿seguirÃ¡ funcionando si el diseÃ±o cambia?\n",
    "\n",
    "#### **Para sitios dinÃ¡micos:**\n",
    "1. **Network tab** para ver llamadas AJAX\n",
    "2. **Buscar APIs JSON** en lugar de scraping HTML\n",
    "3. **Observar patrones** en URLs de datos\n",
    "4. **Identificar headers necesarios** para las peticiones\n",
    "\n",
    "### âš¡ Workflow Eficiente\n",
    "\n",
    "```\n",
    "1. ğŸŒ Abrir pÃ¡gina objetivo\n",
    "2. ğŸ” Inspeccionar elementos de interÃ©s\n",
    "3. ğŸ“ Anotar selectores y estructura\n",
    "4. ğŸ§ª Probar selectores en consola\n",
    "5. ğŸ“Š Verificar en Network si hay APIs\n",
    "6. ğŸ’» Implementar en Python\n",
    "7. ğŸ› Debuggear y refinar\n",
    "```\n",
    "\n",
    "### ğŸ’¡ Consejos Pro\n",
    "\n",
    "- **Usa el modo Responsive** para ver cÃ³mo cambia el HTML en mÃ³vil\n",
    "- **Simula conexiones lentas** en Network para probar timeouts\n",
    "- **Deshabilita JavaScript** para ver el HTML bÃ¡sico\n",
    "- **Usa Screenshots** para documentar elementos importantes\n",
    "- **Guarda HAR files** para analizar trÃ¡fico de red offline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Resumen y PrÃ³ximos Pasos ğŸ¯\n",
    "\n",
    "### ğŸ“ Lo que has dominado en esta lecciÃ³n:\n",
    "\n",
    "#### âœ… **Conceptos Fundamentales**\n",
    "- âœ¨ QuÃ© es el web scraping y sus aplicaciones en el mundo real\n",
    "- ğŸ—ï¸ Estructura completa de HTML y el DOM\n",
    "- ğŸ¯ Elementos y atributos HTML mÃ¡s importantes para scraping\n",
    "- ğŸŒ³ NavegaciÃ³n por la jerarquÃ­a del DOM\n",
    "\n",
    "#### âœ… **Habilidades TÃ©cnicas Avanzadas**\n",
    "- ğŸŒ Realizar solicitudes HTTP robustas con `requests`\n",
    "- ğŸ² Dominio completo de Beautiful Soup\n",
    "- ğŸ¨ Selectores CSS avanzados y precisos\n",
    "- ğŸ“Š ExtracciÃ³n profesional de datos de tablas\n",
    "- ğŸ›¡ï¸ Manejo de errores y scraping robusto\n",
    "- ğŸ§ª Testing y debugging con DevTools\n",
    "\n",
    "#### âœ… **Herramientas Profesionales**\n",
    "- ğŸ“¦ Beautiful Soup para parsing HTML/XML\n",
    "- ğŸŒ Requests para comunicaciÃ³n HTTP\n",
    "- ğŸ› ï¸ DevTools del navegador para investigaciÃ³n\n",
    "- ğŸ” TÃ©cnicas de inspecciÃ³n y anÃ¡lisis\n",
    "\n",
    "#### âœ… **Mejores PrÃ¡cticas**\n",
    "- ğŸ¤– Headers realistas y User-Agents\n",
    "- â±ï¸ Rate limiting y delays apropiados\n",
    "- ğŸ”„ Estrategias de reintentos\n",
    "- ğŸ“Š EstructuraciÃ³n de datos extraÃ­dos\n",
    "\n",
    "### ğŸš€ PrÃ³xima LecciÃ³n: HTTP Requests y Beautiful Soup Avanzado\n",
    "\n",
    "En la **LecciÃ³n 2** profundizaremos en:\n",
    "\n",
    "#### ğŸŒ **HTTP Avanzado**\n",
    "- ğŸ“‹ Headers HTTP personalizados y cookies\n",
    "- ğŸ” Sesiones persistentes y autenticaciÃ³n\n",
    "- ğŸ“¤ POST requests y manejo de formularios\n",
    "- ğŸ”„ Redirects y cÃ³digos de estado\n",
    "\n",
    "#### ğŸ² **Beautiful Soup Profesional**\n",
    "- ğŸ¯ TÃ©cnicas de parsing avanzadas\n",
    "- ğŸ” BÃºsquedas complejas con expresiones regulares\n",
    "- ğŸŒ³ NavegaciÃ³n avanzada del Ã¡rbol DOM\n",
    "- âš¡ OptimizaciÃ³n de rendimiento\n",
    "\n",
    "#### ğŸ“„ **Manejo de Contenido Complejo**\n",
    "- ğŸ“‹ PaginaciÃ³n y navegaciÃ³n entre pÃ¡ginas\n",
    "- ğŸ”— Crawling y seguimiento de enlaces\n",
    "- ğŸ“Š Procesamiento de diferentes formatos\n",
    "- ğŸ›¡ï¸ DetecciÃ³n y manejo de anti-scraping\n",
    "\n",
    "### ğŸ’ª DesafÃ­os para Practicar\n",
    "\n",
    "#### ğŸ¥‰ **Nivel BÃ¡sico**\n",
    "1. Scraping de un blog personal simple\n",
    "2. ExtracciÃ³n de productos de una tienda online bÃ¡sica\n",
    "3. RecopilaciÃ³n de noticias de un portal sencillo\n",
    "\n",
    "#### ğŸ¥ˆ **Nivel Intermedio**\n",
    "1. Comparador de precios entre mÃºltiples tiendas\n",
    "2. Agregador de ofertas de empleo\n",
    "3. Monitor de cambios en pÃ¡ginas web\n",
    "\n",
    "#### ğŸ¥‡ **Nivel Avanzado**\n",
    "1. Scraper de redes sociales (datos pÃºblicos)\n",
    "2. AnÃ¡lisis de sentimientos de reseÃ±as de productos\n",
    "3. Sistema de alertas en tiempo real\n",
    "\n",
    "### ğŸ“š Recursos para Seguir Aprendiendo\n",
    "\n",
    "#### ğŸ“– **DocumentaciÃ³n Oficial**\n",
    "- [Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Requests Documentation](https://docs.python-requests.org/)\n",
    "- [Python HTML Parsing Libraries](https://docs.python.org/3/library/html.parser.html)\n",
    "\n",
    "#### ğŸŒ **Sitios de PrÃ¡ctica**\n",
    "- [Quotes to Scrape](http://quotes.toscrape.com/) - Perfecto para principiantes\n",
    "- [Books to Scrape](http://books.toscrape.com/) - CatÃ¡logo con paginaciÃ³n\n",
    "- [Scrape This Site](https://scrapethissite.com/) - DesafÃ­os progresivos\n",
    "\n",
    "#### ğŸ› ï¸ **Herramientas Ãštiles**\n",
    "- [XPath Tester](https://www.freeformatter.com/xpath-tester.html)\n",
    "- [CSS Selector Tester](https://www.w3schools.com/cssref/trysel.asp)\n",
    "- [Regex101](https://regex101.com/) - Testing de expresiones regulares\n",
    "- [Postman](https://www.postman.com/) - Testing de APIs\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ† Â¡Felicidades por Completar la LecciÃ³n 1!\n",
    "\n",
    "Has dado el primer paso importante en tu viaje hacia el dominio del web scraping. Los fundamentos que has aprendido aquÃ­ son la base sÃ³lida sobre la cual construirÃ¡s habilidades mÃ¡s avanzadas.\n",
    "\n",
    "#### ğŸ¯ **Recuerda siempre:**\n",
    "- ğŸ¤ **SÃ© respetuoso** con los sitios web y sus recursos\n",
    "- ğŸ“œ **Lee los tÃ©rminos de servicio** antes de hacer scraping\n",
    "- ğŸ¤– **Revisa robots.txt** para conocer las reglas\n",
    "- â±ï¸ **Implementa delays** apropiados entre requests\n",
    "- ğŸ›¡ï¸ **Maneja errores** graciosamente\n",
    "\n",
    "#### ğŸš€ **Tu prÃ³xima misiÃ³n:**\n",
    "1. ğŸ’» **Practica** con los ejercicios de esta lecciÃ³n\n",
    "2. ğŸ” **Explora** sitios web simples por tu cuenta\n",
    "3. ğŸ§ª **Experimenta** con diferentes selectores\n",
    "4. ğŸ“ **Documenta** tus hallazgos y aprendizajes\n",
    "5. â¡ï¸ **ContinÃºa** con la LecciÃ³n 2 cuando te sientas cÃ³modo\n",
    "\n",
    "**Â¡El mundo de los datos web te estÃ¡ esperando! ğŸŒŸ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}