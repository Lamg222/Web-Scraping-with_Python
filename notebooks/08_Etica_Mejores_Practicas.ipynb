{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ°Ô∏è Lecci√≥n 8: √âtica, Robots.txt y Mejores Pr√°cticas\n",
    "\n",
    "## üéØ Objetivos Finales\n",
    "\n",
    "- Entender los aspectos legales y √©ticos del web scraping\n",
    "- Implementar respeto por robots.txt y pol√≠ticas\n",
    "- Dominar rate limiting y cortes√≠a web\n",
    "- Crear sistemas de scraping responsables\n",
    "- Aplicar mejores pr√°cticas industriales\n",
    "- Desarrollar scrapers sostenibles y √©ticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import urllib.robotparser\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import json\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "print(\"üõ°Ô∏è √âTICA Y MEJORES PR√ÅCTICAS EN WEB SCRAPING\")\n",
    "print(\"=\" * 55)\n",
    "print(\"‚úÖ M√≥dulos importados correctamente\")\n",
    "print(\"üìñ Preparados para aprender scraping responsable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Aspectos Legales y √âticos\n",
    "\n",
    "### üìú Marco Legal del Web Scraping\n",
    "\n",
    "El web scraping existe en una zona gris legal que depende de varios factores:\n",
    "\n",
    "#### ‚úÖ Generalmente Legal:\n",
    "- Datos p√∫blicos sin autenticaci√≥n\n",
    "- Informaci√≥n factual (precios, horarios)\n",
    "- Datos para uso personal/educativo\n",
    "- Respeto a robots.txt y t√©rminos de servicio\n",
    "- Rate limiting apropiado\n",
    "\n",
    "#### ‚ùå Potencialmente Problem√°tico:\n",
    "- Datos detr√°s de autenticaci√≥n\n",
    "- Contenido con copyright\n",
    "- Informaci√≥n personal/privada\n",
    "- Violaci√≥n de t√©rminos de servicio\n",
    "- Sobrecarga de servidores\n",
    "- Uso comercial no autorizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EthicalScrapingChecker:\n",
    "    \"\"\"Verificador de pr√°cticas √©ticas de scraping\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.ethical_guidelines = {\n",
    "            'respect_robots_txt': True,\n",
    "            'rate_limit_enabled': True,\n",
    "            'user_agent_identified': True,\n",
    "            'public_data_only': True,\n",
    "            'terms_of_service_reviewed': False,  # Usuario debe confirmar\n",
    "            'commercial_use_authorized': False,  # Usuario debe confirmar\n",
    "            'data_minimization': True,  # Solo extraer lo necesario\n",
    "            'privacy_respected': True   # No datos personales\n",
    "        }\n",
    "        \n",
    "        self.risk_factors = {\n",
    "            'authentication_required': 'high',\n",
    "            'personal_data_present': 'high',\n",
    "            'copyright_content': 'medium',\n",
    "            'high_frequency_requests': 'medium',\n",
    "            'commercial_competitor': 'medium',\n",
    "            'no_robots_txt': 'low'\n",
    "        }\n",
    "    \n",
    "    def check_url_ethics(self, url: str, purpose: str = 'research') -> Dict[str, any]:\n",
    "        \"\"\"Evaluar aspectos √©ticos de scrapear una URL\"\"\"\n",
    "        domain = urlparse(url).netloc\n",
    "        \n",
    "        ethics_report = {\n",
    "            'url': url,\n",
    "            'domain': domain,\n",
    "            'purpose': purpose,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'ethical_score': 0,\n",
    "            'recommendations': [],\n",
    "            'warnings': [],\n",
    "            'legal_considerations': []\n",
    "        }\n",
    "        \n",
    "        # Verificar robots.txt\n",
    "        robots_check = self.check_robots_txt(url)\n",
    "        if robots_check['allowed']:\n",
    "            ethics_report['ethical_score'] += 20\n",
    "        else:\n",
    "            ethics_report['warnings'].append(f\"üö´ Robots.txt proh√≠be el acceso: {robots_check['rule']}\")\n",
    "            ethics_report['ethical_score'] -= 30\n",
    "        \n",
    "        # Verificar si es un sitio p√∫blico\n",
    "        if 'login' in url.lower() or 'auth' in url.lower():\n",
    "            ethics_report['warnings'].append(\"üîê URL parece requerir autenticaci√≥n\")\n",
    "            ethics_report['legal_considerations'].append(\"Datos detr√°s de autenticaci√≥n pueden requerir autorizaci√≥n\")\n",
    "            ethics_report['ethical_score'] -= 25\n",
    "        else:\n",
    "            ethics_report['ethical_score'] += 15\n",
    "        \n",
    "        # Evaluar prop√≥sito\n",
    "        purpose_scores = {\n",
    "            'research': 20,\n",
    "            'education': 20,\n",
    "            'personal': 15,\n",
    "            'commercial': -10,\n",
    "            'competitive_analysis': -5\n",
    "        }\n",
    "        \n",
    "        ethics_report['ethical_score'] += purpose_scores.get(purpose, 0)\n",
    "        \n",
    "        if purpose == 'commercial':\n",
    "            ethics_report['legal_considerations'].append(\n",
    "                \"Uso comercial requiere revisar t√©rminos de servicio y posibles licencias\"\n",
    "            )\n",
    "        \n",
    "        # Verificar dominio conocido\n",
    "        sensitive_domains = ['facebook.com', 'linkedin.com', 'twitter.com', 'instagram.com']\n",
    "        if any(sensitive in domain for sensitive in sensitive_domains):\n",
    "            ethics_report['warnings'].append(\"‚ö†Ô∏è Dominio con pol√≠ticas estrictas sobre scraping\")\n",
    "            ethics_report['legal_considerations'].append(\n",
    "                \"Redes sociales tienen t√©rminos de servicio muy restrictivos\"\n",
    "            )\n",
    "            ethics_report['ethical_score'] -= 15\n",
    "        \n",
    "        # Generar recomendaciones\n",
    "        if ethics_report['ethical_score'] >= 50:\n",
    "            ethics_report['recommendations'].append(\"‚úÖ Pr√°ctica generalmente √©tica\")\n",
    "        elif ethics_report['ethical_score'] >= 20:\n",
    "            ethics_report['recommendations'].append(\"‚ö†Ô∏è Proceder con precauci√≥n\")\n",
    "        else:\n",
    "            ethics_report['recommendations'].append(\"‚ùå Alto riesgo √©tico/legal\")\n",
    "        \n",
    "        # Recomendaciones generales\n",
    "        ethics_report['recommendations'].extend([\n",
    "            \"üìñ Revisar t√©rminos de servicio del sitio\",\n",
    "            \"ü§ñ Respetar robots.txt\",\n",
    "            \"‚è±Ô∏è Implementar rate limiting\",\n",
    "            \"üÜî Usar User-Agent identificable\",\n",
    "            \"üìä Extraer solo datos necesarios\",\n",
    "            \"üîí Evitar datos personales/privados\"\n",
    "        ])\n",
    "        \n",
    "        return ethics_report\n",
    "    \n",
    "    def check_robots_txt(self, url: str) -> Dict[str, any]:\n",
    "        \"\"\"Verificar robots.txt\"\"\"\n",
    "        try:\n",
    "            parsed_url = urlparse(url)\n",
    "            robots_url = f\"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt\"\n",
    "            \n",
    "            rp = urllib.robotparser.RobotFileParser()\n",
    "            rp.set_url(robots_url)\n",
    "            rp.read()\n",
    "            \n",
    "            allowed = rp.can_fetch('*', url)\n",
    "            \n",
    "            return {\n",
    "                'robots_url': robots_url,\n",
    "                'allowed': allowed,\n",
    "                'rule': f\"{'Permitido' if allowed else 'No permitido'} para user-agent '*'\"\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'robots_url': 'N/A',\n",
    "                'allowed': True,  # Si no hay robots.txt, asumir permitido\n",
    "                'rule': f'No se pudo leer robots.txt: {e}'\n",
    "            }\n",
    "\n",
    "# Ejemplo de evaluaci√≥n √©tica\n",
    "print(\"‚öñÔ∏è EVALUACI√ìN √âTICA DE URLs\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "ethics_checker = EthicalScrapingChecker()\n",
    "\n",
    "# URLs de ejemplo para evaluar\n",
    "test_urls = [\n",
    "    ('http://quotes.toscrape.com', 'education'),\n",
    "    ('https://example-store.com/products', 'research'),\n",
    "    ('https://facebook.com/profile/123', 'commercial'),\n",
    "    ('https://api.private-site.com/data', 'competitive_analysis')\n",
    "]\n",
    "\n",
    "for url, purpose in test_urls:\n",
    "    print(f\"\\nüîç Evaluando: {url} (Prop√≥sito: {purpose})\")\n",
    "    \n",
    "    report = ethics_checker.check_url_ethics(url, purpose)\n",
    "    \n",
    "    print(f\"üìä Puntuaci√≥n √©tica: {report['ethical_score']}/100\")\n",
    "    \n",
    "    if report['warnings']:\n",
    "        print(\"‚ö†Ô∏è Advertencias:\")\n",
    "        for warning in report['warnings']:\n",
    "            print(f\"   {warning}\")\n",
    "    \n",
    "    print(f\"üí° Recomendaci√≥n principal: {report['recommendations'][0]}\")\n",
    "    \n",
    "    if report['legal_considerations']:\n",
    "        print(f\"‚öñÔ∏è Consideraci√≥n legal: {report['legal_considerations'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Robots.txt: Respeto y Cumplimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobotsTxtManager:\n",
    "    \"\"\"Gestor completo de robots.txt\"\"\"\n",
    "    \n",
    "    def __init__(self, user_agent: str = '*'):\n",
    "        self.user_agent = user_agent\n",
    "        self.robots_cache = {}  # Cache de robots.txt\n",
    "        self.cache_expiry = 3600  # 1 hora\n",
    "    \n",
    "    def get_robots_txt(self, domain: str) -> Optional[urllib.robotparser.RobotFileParser]:\n",
    "        \"\"\"Obtener y cachear robots.txt\"\"\"\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # Verificar cache\n",
    "        if domain in self.robots_cache:\n",
    "            cached_data = self.robots_cache[domain]\n",
    "            if current_time - cached_data['timestamp'] < self.cache_expiry:\n",
    "                return cached_data['parser']\n",
    "        \n",
    "        try:\n",
    "            robots_url = f\"https://{domain}/robots.txt\"\n",
    "            \n",
    "            rp = urllib.robotparser.RobotFileParser()\n",
    "            rp.set_url(robots_url)\n",
    "            rp.read()\n",
    "            \n",
    "            # Cachear resultado\n",
    "            self.robots_cache[domain] = {\n",
    "                'parser': rp,\n",
    "                'timestamp': current_time\n",
    "            }\n",
    "            \n",
    "            return rp\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error leyendo robots.txt de {domain}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def can_fetch(self, url: str) -> Tuple[bool, str]:\n",
    "        \"\"\"Verificar si se puede hacer scraping de una URL\"\"\"\n",
    "        parsed_url = urlparse(url)\n",
    "        domain = parsed_url.netloc\n",
    "        \n",
    "        rp = self.get_robots_txt(domain)\n",
    "        \n",
    "        if rp is None:\n",
    "            return True, \"No se pudo obtener robots.txt - asumiendo permitido\"\n",
    "        \n",
    "        allowed = rp.can_fetch(self.user_agent, url)\n",
    "        \n",
    "        if allowed:\n",
    "            return True, \"Permitido por robots.txt\"\n",
    "        else:\n",
    "            return False, \"Prohibido por robots.txt\"\n",
    "    \n",
    "    def get_crawl_delay(self, domain: str) -> Optional[float]:\n",
    "        \"\"\"Obtener delay recomendado de robots.txt\"\"\"\n",
    "        rp = self.get_robots_txt(domain)\n",
    "        \n",
    "        if rp is None:\n",
    "            return None\n",
    "        \n",
    "        delay = rp.crawl_delay(self.user_agent)\n",
    "        return delay\n",
    "    \n",
    "    def get_sitemaps(self, domain: str) -> List[str]:\n",
    "        \"\"\"Obtener sitemaps listados en robots.txt\"\"\"\n",
    "        rp = self.get_robots_txt(domain)\n",
    "        \n",
    "        if rp is None:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            return rp.site_maps() or []\n",
    "        except AttributeError:\n",
    "            return []\n",
    "    \n",
    "    def analyze_robots_txt(self, domain: str) -> Dict[str, any]:\n",
    "        \"\"\"An√°lisis completo de robots.txt\"\"\"\n",
    "        analysis = {\n",
    "            'domain': domain,\n",
    "            'robots_url': f\"https://{domain}/robots.txt\",\n",
    "            'accessible': False,\n",
    "            'user_agents': [],\n",
    "            'disallowed_paths': [],\n",
    "            'allowed_paths': [],\n",
    "            'crawl_delay': None,\n",
    "            'sitemaps': [],\n",
    "            'raw_content': ''\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Obtener contenido raw\n",
    "            response = requests.get(analysis['robots_url'], timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            analysis['accessible'] = True\n",
    "            analysis['raw_content'] = response.text\n",
    "            \n",
    "            # Parsear contenido\n",
    "            lines = response.text.split('\\n')\n",
    "            current_user_agent = None\n",
    "            \n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if not line or line.startswith('#'):\n",
    "                    continue\n",
    "                \n",
    "                if line.lower().startswith('user-agent:'):\n",
    "                    current_user_agent = line.split(':', 1)[1].strip()\n",
    "                    if current_user_agent not in analysis['user_agents']:\n",
    "                        analysis['user_agents'].append(current_user_agent)\n",
    "                \n",
    "                elif line.lower().startswith('disallow:'):\n",
    "                    path = line.split(':', 1)[1].strip()\n",
    "                    if path and path not in analysis['disallowed_paths']:\n",
    "                        analysis['disallowed_paths'].append(path)\n",
    "                \n",
    "                elif line.lower().startswith('allow:'):\n",
    "                    path = line.split(':', 1)[1].strip()\n",
    "                    if path and path not in analysis['allowed_paths']:\n",
    "                        analysis['allowed_paths'].append(path)\n",
    "                \n",
    "                elif line.lower().startswith('crawl-delay:'):\n",
    "                    try:\n",
    "                        delay = float(line.split(':', 1)[1].strip())\n",
    "                        analysis['crawl_delay'] = delay\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "                \n",
    "                elif line.lower().startswith('sitemap:'):\n",
    "                    sitemap = line.split(':', 1)[1].strip()\n",
    "                    if sitemap not in analysis['sitemaps']:\n",
    "                        analysis['sitemaps'].append(sitemap)\n",
    "        \n",
    "        except Exception as e:\n",
    "            analysis['error'] = str(e)\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "# Ejemplo de an√°lisis de robots.txt\n",
    "print(\"ü§ñ AN√ÅLISIS DE ROBOTS.TXT\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "robots_manager = RobotsTxtManager(user_agent='EthicalScraper/1.0')\n",
    "\n",
    "# Analizar varios dominios\n",
    "test_domains = ['quotes.toscrape.com', 'httpbin.org', 'example.com']\n",
    "\n",
    "for domain in test_domains:\n",
    "    print(f\"\\nüåê Analizando robots.txt de {domain}:\")\n",
    "    \n",
    "    analysis = robots_manager.analyze_robots_txt(domain)\n",
    "    \n",
    "    if analysis['accessible']:\n",
    "        print(f\"‚úÖ Robots.txt accesible\")\n",
    "        print(f\"üë• User agents: {len(analysis['user_agents'])}\")\n",
    "        print(f\"üö´ Rutas prohibidas: {len(analysis['disallowed_paths'])}\")\n",
    "        print(f\"‚úÖ Rutas permitidas: {len(analysis['allowed_paths'])}\")\n",
    "        \n",
    "        if analysis['crawl_delay']:\n",
    "            print(f\"‚è±Ô∏è Delay recomendado: {analysis['crawl_delay']}s\")\n",
    "        \n",
    "        if analysis['sitemaps']:\n",
    "            print(f\"üó∫Ô∏è Sitemaps: {len(analysis['sitemaps'])}\")\n",
    "        \n",
    "        # Verificar URLs espec√≠ficas\n",
    "        test_url = f\"https://{domain}/\"\n",
    "        can_fetch, reason = robots_manager.can_fetch(test_url)\n",
    "        print(f\"üîç Scraping de ra√≠z: {'‚úÖ Permitido' if can_fetch else '‚ùå Prohibido'} ({reason})\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"‚ùå No se pudo acceder a robots.txt\")\n",
    "        if 'error' in analysis:\n",
    "            print(f\"Error: {analysis['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚è±Ô∏è Rate Limiting y Cortes√≠a Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoliteWebScraper:\n",
    "    \"\"\"Scraper que implementa todas las mejores pr√°cticas de cortes√≠a\"\"\"\n",
    "    \n",
    "    def __init__(self, user_agent: str, base_delay: float = 1.0):\n",
    "        self.user_agent = user_agent\n",
    "        self.base_delay = base_delay\n",
    "        \n",
    "        # Gesti√≥n de delays por dominio\n",
    "        self.domain_delays = defaultdict(lambda: base_delay)\n",
    "        self.last_requests = defaultdict(float)\n",
    "        self.request_counts = defaultdict(int)\n",
    "        \n",
    "        # Configuraci√≥n de cortes√≠a\n",
    "        self.politeness_config = {\n",
    "            'max_requests_per_minute': 30,\n",
    "            'max_requests_per_hour': 1000,\n",
    "            'adaptive_delay': True,\n",
    "            'respect_server_load': True,\n",
    "            'exponential_backoff': True\n",
    "        }\n",
    "        \n",
    "        # Estad√≠sticas\n",
    "        self.stats = {\n",
    "            'requests_made': 0,\n",
    "            'total_delay_time': 0,\n",
    "            'errors_encountered': 0,\n",
    "            'domains_accessed': set(),\n",
    "            'start_time': time.time()\n",
    "        }\n",
    "        \n",
    "        # Robots.txt manager\n",
    "        self.robots_manager = RobotsTxtManager(user_agent=user_agent)\n",
    "        \n",
    "        # Setup logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def calculate_delay(self, domain: str, response_time: float = None, status_code: int = 200) -> float:\n",
    "        \"\"\"Calcular delay apropiado basado en m√∫ltiples factores\"\"\"\n",
    "        base_delay = self.domain_delays[domain]\n",
    "        \n",
    "        # Factor 1: Robots.txt crawl-delay\n",
    "        robots_delay = self.robots_manager.get_crawl_delay(domain)\n",
    "        if robots_delay:\n",
    "            base_delay = max(base_delay, robots_delay)\n",
    "        \n",
    "        # Factor 2: Carga del servidor (basado en tiempo de respuesta)\n",
    "        if response_time and self.politeness_config['respect_server_load']:\n",
    "            if response_time > 5.0:  # Servidor lento\n",
    "                base_delay *= 2.0\n",
    "            elif response_time > 2.0:  # Servidor moderadamente lento\n",
    "                base_delay *= 1.5\n",
    "        \n",
    "        # Factor 3: C√≥digos de estado (backoff exponencial)\n",
    "        if self.politeness_config['exponential_backoff']:\n",
    "            if status_code == 429:  # Too Many Requests\n",
    "                base_delay *= 4.0\n",
    "            elif status_code >= 500:  # Server errors\n",
    "                base_delay *= 2.0\n",
    "        \n",
    "        # Factor 4: Frecuencia de requests\n",
    "        current_time = time.time()\n",
    "        requests_this_minute = self.request_counts[f\"{domain}_minute\"]\n",
    "        \n",
    "        if requests_this_minute > self.politeness_config['max_requests_per_minute']:\n",
    "            base_delay *= 3.0  # Slow down significantly\n",
    "        \n",
    "        # Factor 5: Delay adaptativo\n",
    "        if self.politeness_config['adaptive_delay']:\n",
    "            # Aumentar delay gradualmente durante la sesi√≥n\n",
    "            session_duration = current_time - self.stats['start_time']\n",
    "            if session_duration > 3600:  # Despu√©s de 1 hora\n",
    "                base_delay *= 1.2\n",
    "        \n",
    "        return min(base_delay, 30.0)  # M√°ximo 30 segundos\n",
    "    \n",
    "    def wait_for_politeness(self, domain: str, response_time: float = None, status_code: int = 200):\n",
    "        \"\"\"Esperar tiempo apropiado entre requests\"\"\"\n",
    "        current_time = time.time()\n",
    "        last_request_time = self.last_requests.get(domain, 0)\n",
    "        \n",
    "        # Calcular delay necesario\n",
    "        required_delay = self.calculate_delay(domain, response_time, status_code)\n",
    "        \n",
    "        # Tiempo transcurrido desde √∫ltimo request\n",
    "        time_since_last = current_time - last_request_time\n",
    "        \n",
    "        # Esperar si es necesario\n",
    "        if time_since_last < required_delay:\n",
    "            wait_time = required_delay - time_since_last\n",
    "            \n",
    "            self.logger.info(f\"‚è±Ô∏è Esperando {wait_time:.2f}s para {domain} (cortes√≠a web)\")\n",
    "            \n",
    "            time.sleep(wait_time)\n",
    "            self.stats['total_delay_time'] += wait_time\n",
    "        \n",
    "        # Actualizar registros\n",
    "        self.last_requests[domain] = time.time()\n",
    "        self.update_request_counts(domain)\n",
    "    \n",
    "    def update_request_counts(self, domain: str):\n",
    "        \"\"\"Actualizar contadores de requests\"\"\"\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # Limpiar contadores antiguos\n",
    "        minute_key = f\"{domain}_minute\"\n",
    "        hour_key = f\"{domain}_hour\"\n",
    "        \n",
    "        # Resetear contadores si ha pasado el tiempo\n",
    "        if current_time - self.last_requests.get(f\"{minute_key}_reset\", 0) > 60:\n",
    "            self.request_counts[minute_key] = 0\n",
    "            self.last_requests[f\"{minute_key}_reset\"] = current_time\n",
    "        \n",
    "        if current_time - self.last_requests.get(f\"{hour_key}_reset\", 0) > 3600:\n",
    "            self.request_counts[hour_key] = 0\n",
    "            self.last_requests[f\"{hour_key}_reset\"] = current_time\n",
    "        \n",
    "        # Incrementar contadores\n",
    "        self.request_counts[minute_key] += 1\n",
    "        self.request_counts[hour_key] += 1\n",
    "    \n",
    "    def polite_request(self, url: str, **kwargs) -> Optional[requests.Response]:\n",
    "        \"\"\"Realizar request con todas las consideraciones de cortes√≠a\"\"\"\n",
    "        domain = urlparse(url).netloc\n",
    "        \n",
    "        # Verificar robots.txt\n",
    "        can_fetch, reason = self.robots_manager.can_fetch(url)\n",
    "        if not can_fetch:\n",
    "            self.logger.warning(f\"üö´ Robots.txt proh√≠be el acceso a {url}: {reason}\")\n",
    "            return None\n",
    "        \n",
    "        # Verificar l√≠mites de rate\n",
    "        if not self.check_rate_limits(domain):\n",
    "            self.logger.warning(f\"‚è±Ô∏è L√≠mites de rate excedidos para {domain}\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Esperar tiempo de cortes√≠a\n",
    "            self.wait_for_politeness(domain)\n",
    "            \n",
    "            # Configurar headers apropiados\n",
    "            headers = kwargs.get('headers', {})\n",
    "            headers.update({\n",
    "                'User-Agent': self.user_agent,\n",
    "                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "                'Accept-Language': 'en-US,en;q=0.5',\n",
    "                'Accept-Encoding': 'gzip, deflate',\n",
    "                'Connection': 'keep-alive',\n",
    "                'Upgrade-Insecure-Requests': '1'\n",
    "            })\n",
    "            kwargs['headers'] = headers\n",
    "            \n",
    "            # Realizar request\n",
    "            start_time = time.time()\n",
    "            response = requests.get(url, timeout=30, **kwargs)\n",
    "            response_time = time.time() - start_time\n",
    "            \n",
    "            # Actualizar estad√≠sticas\n",
    "            self.stats['requests_made'] += 1\n",
    "            self.stats['domains_accessed'].add(domain)\n",
    "            \n",
    "            # Log del request\n",
    "            self.logger.info(\n",
    "                f\"üì° {response.status_code} {url} ({response_time:.2f}s, {len(response.content)} bytes)\"\n",
    "            )\n",
    "            \n",
    "            # Actualizar delay para pr√≥ximo request\n",
    "            self.wait_for_politeness(domain, response_time, response.status_code)\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except requests.RequestException as e:\n",
    "            self.stats['errors_encountered'] += 1\n",
    "            self.logger.error(f\"‚ùå Error en request a {url}: {e}\")\n",
    "            \n",
    "            # Aumentar delay despu√©s de error\n",
    "            self.domain_delays[domain] *= 1.5\n",
    "            \n",
    "            return None\n",
    "    \n",
    "    def check_rate_limits(self, domain: str) -> bool:\n",
    "        \"\"\"Verificar si se pueden hacer m√°s requests\"\"\"\n",
    "        minute_count = self.request_counts.get(f\"{domain}_minute\", 0)\n",
    "        hour_count = self.request_counts.get(f\"{domain}_hour\", 0)\n",
    "        \n",
    "        if minute_count >= self.politeness_config['max_requests_per_minute']:\n",
    "            return False\n",
    "        \n",
    "        if hour_count >= self.politeness_config['max_requests_per_hour']:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def get_politeness_report(self) -> Dict[str, any]:\n",
    "        \"\"\"Obtener reporte de cortes√≠a\"\"\"\n",
    "        current_time = time.time()\n",
    "        session_duration = current_time - self.stats['start_time']\n",
    "        \n",
    "        return {\n",
    "            'session_duration': session_duration,\n",
    "            'requests_made': self.stats['requests_made'],\n",
    "            'domains_accessed': len(self.stats['domains_accessed']),\n",
    "            'total_delay_time': self.stats['total_delay_time'],\n",
    "            'average_delay': self.stats['total_delay_time'] / max(self.stats['requests_made'], 1),\n",
    "            'requests_per_minute': self.stats['requests_made'] / (session_duration / 60),\n",
    "            'errors_encountered': self.stats['errors_encountered'],\n",
    "            'politeness_ratio': self.stats['total_delay_time'] / session_duration * 100,\n",
    "            'domains_list': list(self.stats['domains_accessed'])\n",
    "        }\n",
    "\n",
    "# Ejemplo de scraping cort√©s\n",
    "print(\"‚è±Ô∏è SCRAPING CORT√âS Y RESPONSABLE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Crear scraper cort√©s\n",
    "polite_scraper = PoliteWebScraper(\n",
    "    user_agent='EthicalScraper/1.0 (+http://example.com/bot-info)',\n",
    "    base_delay=2.0\n",
    ")\n",
    "\n",
    "# URLs de ejemplo\n",
    "test_urls = [\n",
    "    'http://httpbin.org/delay/1',\n",
    "    'http://httpbin.org/status/200',\n",
    "    'http://httpbin.org/json'\n",
    "]\n",
    "\n",
    "print(\"üåê Realizando requests corteses...\")\n",
    "responses = []\n",
    "\n",
    "for url in test_urls:\n",
    "    print(f\"\\nüì° Requesting: {url}\")\n",
    "    response = polite_scraper.polite_request(url)\n",
    "    \n",
    "    if response:\n",
    "        responses.append({\n",
    "            'url': url,\n",
    "            'status': response.status_code,\n",
    "            'size': len(response.content)\n",
    "        })\n",
    "        print(f\"‚úÖ Success: {response.status_code} ({len(response.content)} bytes)\")\n",
    "    else:\n",
    "        print(f\"‚ùå Failed to fetch {url}\")\n",
    "\n",
    "# Reporte de cortes√≠a\n",
    "report = polite_scraper.get_politeness_report()\n",
    "\n",
    "print(f\"\\nüìä REPORTE DE CORTES√çA:\")\n",
    "print(f\"  ‚è±Ô∏è Duraci√≥n sesi√≥n: {report['session_duration']:.2f}s\")\n",
    "print(f\"  üì° Requests realizados: {report['requests_made']}\")\n",
    "print(f\"  üåê Dominios accedidos: {report['domains_accessed']}\")\n",
    "print(f\"  ‚è≥ Tiempo total de espera: {report['total_delay_time']:.2f}s\")\n",
    "print(f\"  üìä Delay promedio: {report['average_delay']:.2f}s\")\n",
    "print(f\"  üéØ Requests por minuto: {report['requests_per_minute']:.2f}\")\n",
    "print(f\"  üõ°Ô∏è Ratio de cortes√≠a: {report['politeness_ratio']:.1f}%\")\n",
    "print(f\"  ‚ùå Errores: {report['errors_encountered']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Framework Completo de Scraping √âtico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EthicalScrapingFramework:\n",
    "    \"\"\"Framework completo para web scraping √©tico y responsable\"\"\"\n",
    "    \n",
    "    def __init__(self, project_name: str, contact_info: str):\n",
    "        self.project_name = project_name\n",
    "        self.contact_info = contact_info\n",
    "        \n",
    "        # Configurar User-Agent identificable\n",
    "        self.user_agent = f\"{project_name}/1.0 (+{contact_info})\"\n",
    "        \n",
    "        # Componentes del framework\n",
    "        self.ethics_checker = EthicalScrapingChecker()\n",
    "        self.polite_scraper = PoliteWebScraper(self.user_agent)\n",
    "        \n",
    "        # Configuraci√≥n √©tica\n",
    "        self.ethical_config = {\n",
    "            'max_pages_per_site': 1000,\n",
    "            'max_concurrent_requests': 1,  # Conservador\n",
    "            'minimum_delay': 3.0,  # 3 segundos m√≠nimo\n",
    "            'respect_robots_txt': True,\n",
    "            'log_all_requests': True,\n",
    "            'data_minimization': True,  # Solo extraer datos necesarios\n",
    "            'auto_detect_limits': True,\n",
    "            'stop_on_errors': True\n",
    "        }\n",
    "        \n",
    "        # Log de auditor√≠a\n",
    "        self.audit_log = {\n",
    "            'session_id': f\"{project_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "            'start_time': datetime.now().isoformat(),\n",
    "            'requests_log': [],\n",
    "            'ethics_checks': [],\n",
    "            'errors_log': [],\n",
    "            'data_extracted': 0\n",
    "        }\n",
    "        \n",
    "        # Setup logging\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(f'{project_name}_ethical_scraping.log'),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        self.logger.info(f\"üõ°Ô∏è Iniciando framework de scraping √©tico para '{project_name}'\")\n",
    "    \n",
    "    def pre_scraping_check(self, urls: List[str], purpose: str) -> Dict[str, any]:\n",
    "        \"\"\"Verificaci√≥n √©tica previa al scraping\"\"\"\n",
    "        self.logger.info(f\"üîç Realizando verificaci√≥n √©tica previa para {len(urls)} URLs\")\n",
    "        \n",
    "        check_results = {\n",
    "            'total_urls': len(urls),\n",
    "            'ethical_urls': 0,\n",
    "            'problematic_urls': 0,\n",
    "            'blocked_urls': [],\n",
    "            'warnings': [],\n",
    "            'approved_urls': [],\n",
    "            'overall_ethics_score': 0\n",
    "        }\n",
    "        \n",
    "        total_score = 0\n",
    "        \n",
    "        for url in urls:\n",
    "            ethics_report = self.ethics_checker.check_url_ethics(url, purpose)\n",
    "            \n",
    "            # Log en auditor√≠a\n",
    "            self.audit_log['ethics_checks'].append({\n",
    "                'url': url,\n",
    "                'score': ethics_report['ethical_score'],\n",
    "                'warnings': ethics_report['warnings'],\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            })\n",
    "            \n",
    "            total_score += ethics_report['ethical_score']\n",
    "            \n",
    "            if ethics_report['ethical_score'] >= 50:\n",
    "                check_results['ethical_urls'] += 1\n",
    "                check_results['approved_urls'].append(url)\n",
    "            elif ethics_report['ethical_score'] >= 20:\n",
    "                check_results['warnings'].append(f\"‚ö†Ô∏è {url}: Proceder con precauci√≥n\")\n",
    "                check_results['approved_urls'].append(url)\n",
    "            else:\n",
    "                check_results['problematic_urls'] += 1\n",
    "                check_results['blocked_urls'].append(url)\n",
    "                self.logger.warning(f\"üö´ URL bloqueada por razones √©ticas: {url}\")\n",
    "        \n",
    "        check_results['overall_ethics_score'] = total_score / len(urls) if urls else 0\n",
    "        \n",
    "        # Decisi√≥n final\n",
    "        if check_results['overall_ethics_score'] < 30:\n",
    "            self.logger.error(f\"‚ùå Proyecto rechazado: Score √©tico muy bajo ({check_results['overall_ethics_score']:.1f})\")\n",
    "            check_results['approved'] = False\n",
    "        else:\n",
    "            self.logger.info(f\"‚úÖ Proyecto aprobado con score √©tico de {check_results['overall_ethics_score']:.1f}\")\n",
    "            check_results['approved'] = True\n",
    "        \n",
    "        return check_results\n",
    "    \n",
    "    def ethical_scrape(self, url: str, extract_function=None) -> Optional[Dict[str, any]]:\n",
    "        \"\"\"Realizar scraping √©tico de una URL\"\"\"\n",
    "        self.logger.info(f\"üåê Iniciando scraping √©tico de {url}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Realizar request cort√©s\n",
    "            response = self.polite_scraper.polite_request(url)\n",
    "            \n",
    "            if not response:\n",
    "                self.logger.error(f\"‚ùå No se pudo obtener respuesta de {url}\")\n",
    "                return None\n",
    "            \n",
    "            # Extraer datos si se proporciona funci√≥n\n",
    "            extracted_data = None\n",
    "            if extract_function:\n",
    "                try:\n",
    "                    extracted_data = extract_function(response)\n",
    "                    if extracted_data:\n",
    "                        self.audit_log['data_extracted'] += 1\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"‚ùå Error extrayendo datos de {url}: {e}\")\n",
    "            \n",
    "            # Log de auditor√≠a del request\n",
    "            request_log = {\n",
    "                'url': url,\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'status_code': response.status_code,\n",
    "                'response_size': len(response.content),\n",
    "                'response_time': time.time() - start_time,\n",
    "                'user_agent': self.user_agent,\n",
    "                'data_extracted': extracted_data is not None\n",
    "            }\n",
    "            \n",
    "            self.audit_log['requests_log'].append(request_log)\n",
    "            \n",
    "            return {\n",
    "                'response': response,\n",
    "                'data': extracted_data,\n",
    "                'metadata': request_log\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"‚ùå Error en scraping √©tico de {url}: {e}\")\n",
    "            \n",
    "            # Log de error\n",
    "            self.audit_log['errors_log'].append({\n",
    "                'url': url,\n",
    "                'error': str(e),\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            })\n",
    "            \n",
    "            return None\n",
    "    \n",
    "    def batch_ethical_scrape(self, urls: List[str], extract_function=None, max_pages: int = None) -> List[Dict[str, any]]:\n",
    "        \"\"\"Scraping √©tico en lote\"\"\"\n",
    "        if max_pages:\n",
    "            urls = urls[:max_pages]\n",
    "        \n",
    "        self.logger.info(f\"üöÄ Iniciando scraping √©tico en lote de {len(urls)} URLs\")\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for i, url in enumerate(urls, 1):\n",
    "            self.logger.info(f\"üìÑ Procesando {i}/{len(urls)}: {url}\")\n",
    "            \n",
    "            result = self.ethical_scrape(url, extract_function)\n",
    "            \n",
    "            if result:\n",
    "                results.append(result)\n",
    "            \n",
    "            # Verificar si debemos parar por errores\n",
    "            if self.ethical_config['stop_on_errors'] and len(self.audit_log['errors_log']) > 5:\n",
    "                self.logger.warning(\"‚èπÔ∏è Deteniendo scraping por exceso de errores\")\n",
    "                break\n",
    "            \n",
    "            # Progress update cada 10 URLs\n",
    "            if i % 10 == 0:\n",
    "                success_rate = len(results) / i * 100\n",
    "                self.logger.info(f\"üìä Progreso: {i}/{len(urls)} URLs ({success_rate:.1f}% √©xito)\")\n",
    "        \n",
    "        self.logger.info(f\"‚úÖ Scraping en lote completado: {len(results)} URLs exitosas\")\n",
    "        return results\n",
    "    \n",
    "    def generate_compliance_report(self) -> Dict[str, any]:\n",
    "        \"\"\"Generar reporte de cumplimiento √©tico\"\"\"\n",
    "        current_time = datetime.now()\n",
    "        \n",
    "        # Estad√≠sticas del scraper cort√©s\n",
    "        politeness_stats = self.polite_scraper.get_politeness_report()\n",
    "        \n",
    "        compliance_report = {\n",
    "            'project_info': {\n",
    "                'name': self.project_name,\n",
    "                'contact': self.contact_info,\n",
    "                'user_agent': self.user_agent,\n",
    "                'session_id': self.audit_log['session_id']\n",
    "            },\n",
    "            'session_summary': {\n",
    "                'start_time': self.audit_log['start_time'],\n",
    "                'end_time': current_time.isoformat(),\n",
    "                'duration_hours': (current_time - datetime.fromisoformat(self.audit_log['start_time'])).total_seconds() / 3600,\n",
    "                'total_requests': len(self.audit_log['requests_log']),\n",
    "                'successful_requests': len([r for r in self.audit_log['requests_log'] if r['status_code'] == 200]),\n",
    "                'failed_requests': len(self.audit_log['errors_log']),\n",
    "                'data_points_extracted': self.audit_log['data_extracted']\n",
    "            },\n",
    "            'ethical_compliance': {\n",
    "                'ethics_checks_performed': len(self.audit_log['ethics_checks']),\n",
    "                'average_ethics_score': sum(c['score'] for c in self.audit_log['ethics_checks']) / len(self.audit_log['ethics_checks']) if self.audit_log['ethics_checks'] else 0,\n",
    "                'robots_txt_respected': True,  # Nuestro framework siempre respeta robots.txt\n",
    "                'rate_limiting_applied': True,\n",
    "                'average_delay_seconds': politeness_stats.get('average_delay', 0),\n",
    "                'politeness_ratio': politeness_stats.get('politeness_ratio', 0)\n",
    "            },\n",
    "            'performance_metrics': politeness_stats,\n",
    "            'errors_encountered': self.audit_log['errors_log'],\n",
    "            'recommendations': self.generate_recommendations()\n",
    "        }\n",
    "        \n",
    "        return compliance_report\n",
    "    \n",
    "    def generate_recommendations(self) -> List[str]:\n",
    "        \"\"\"Generar recomendaciones basadas en la sesi√≥n\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        politeness_stats = self.polite_scraper.get_politeness_report()\n",
    "        \n",
    "        # Recomendaciones basadas en m√©tricas\n",
    "        if politeness_stats.get('requests_per_minute', 0) > 10:\n",
    "            recommendations.append(\"‚ö†Ô∏è Considerar reducir la velocidad de requests\")\n",
    "        \n",
    "        if politeness_stats.get('errors_encountered', 0) > 5:\n",
    "            recommendations.append(\"üîß Revisar manejo de errores y reintentos\")\n",
    "        \n",
    "        if len(self.audit_log['errors_log']) > len(self.audit_log['requests_log']) * 0.1:\n",
    "            recommendations.append(\"üìä Alta tasa de errores - verificar targets\")\n",
    "        \n",
    "        # Recomendaciones generales\n",
    "        recommendations.extend([\n",
    "            \"‚úÖ Continuar respetando robots.txt\",\n",
    "            \"‚è±Ô∏è Mantener delays apropiados\",\n",
    "            \"üìù Documentar prop√≥sito de scraping\",\n",
    "            \"üîÑ Revisar peri√≥dicamente t√©rminos de servicio\"\n",
    "        ])\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def save_compliance_report(self, filename: str = None) -> str:\n",
    "        \"\"\"Guardar reporte de cumplimiento\"\"\"\n",
    "        if not filename:\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            filename = f'{self.project_name}_compliance_report_{timestamp}.json'\n",
    "        \n",
    "        report = self.generate_compliance_report()\n",
    "        \n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(report, f, indent=2, default=str)\n",
    "        \n",
    "        self.logger.info(f\"üìÑ Reporte de cumplimiento guardado: {filename}\")\n",
    "        return filename\n",
    "\n",
    "# Funci√≥n de extracci√≥n de ejemplo\n",
    "def extract_quote_data(response):\n",
    "    \"\"\"Extraer datos de quotes.toscrape.com\"\"\"\n",
    "    from bs4 import BeautifulSoup\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    quotes = []\n",
    "    \n",
    "    for quote_elem in soup.select('.quote'):\n",
    "        quote_data = {\n",
    "            'text': quote_elem.select_one('.text').text if quote_elem.select_one('.text') else '',\n",
    "            'author': quote_elem.select_one('.author').text if quote_elem.select_one('.author') else '',\n",
    "            'tags': [tag.text for tag in quote_elem.select('.tag')]\n",
    "        }\n",
    "        quotes.append(quote_data)\n",
    "    \n",
    "    return quotes\n",
    "\n",
    "# Ejemplo de uso del framework completo\n",
    "print(\"üéØ FRAMEWORK COMPLETO DE SCRAPING √âTICO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Crear framework\n",
    "framework = EthicalScrapingFramework(\n",
    "    project_name=\"EthicalQuoteScraper\",\n",
    "    contact_info=\"http://example.com/contact\"\n",
    ")\n",
    "\n",
    "# URLs de ejemplo\n",
    "test_urls = [\n",
    "    'http://quotes.toscrape.com/',\n",
    "    'http://quotes.toscrape.com/page/2/'\n",
    "]\n",
    "\n",
    "# Verificaci√≥n √©tica previa\n",
    "print(\"\\nüîç Realizando verificaci√≥n √©tica previa...\")\n",
    "ethics_check = framework.pre_scraping_check(test_urls, 'education')\n",
    "\n",
    "print(f\"üìä Resultado de verificaci√≥n √©tica:\")\n",
    "print(f\"  ‚úÖ URLs aprobadas: {ethics_check['ethical_urls']}/{ethics_check['total_urls']}\")\n",
    "print(f\"  ‚ö†Ô∏è URLs problem√°ticas: {ethics_check['problematic_urls']}\")\n",
    "print(f\"  üéØ Score √©tico general: {ethics_check['overall_ethics_score']:.1f}/100\")\n",
    "print(f\"  üìã Proyecto aprobado: {'S√≠' if ethics_check['approved'] else 'No'}\")\n",
    "\n",
    "if ethics_check['approved']:\n",
    "    print(\"\\nüöÄ Iniciando scraping √©tico...\")\n",
    "    \n",
    "    # Realizar scraping √©tico\n",
    "    results = framework.batch_ethical_scrape(\n",
    "        ethics_check['approved_urls'], \n",
    "        extract_function=extract_quote_data,\n",
    "        max_pages=2\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìä Resultados del scraping:\")\n",
    "    print(f\"  üìÑ P√°ginas procesadas: {len(results)}\")\n",
    "    \n",
    "    total_quotes = sum(len(r['data']) for r in results if r['data'])\n",
    "    print(f\"  üí¨ Citas extra√≠das: {total_quotes}\")\n",
    "    \n",
    "    # Generar reporte de cumplimiento\n",
    "    print(\"\\nüìã Generando reporte de cumplimiento...\")\n",
    "    compliance_report = framework.generate_compliance_report()\n",
    "    \n",
    "    print(f\"\\nüõ°Ô∏è REPORTE DE CUMPLIMIENTO √âTICO:\")\n",
    "    print(f\"  üìä Requests totales: {compliance_report['session_summary']['total_requests']}\")\n",
    "    print(f\"  ‚úÖ Requests exitosos: {compliance_report['session_summary']['successful_requests']}\")\n",
    "    print(f\"  ‚è±Ô∏è Delay promedio: {compliance_report['ethical_compliance']['average_delay_seconds']:.2f}s\")\n",
    "    print(f\"  üéØ Score √©tico promedio: {compliance_report['ethical_compliance']['average_ethics_score']:.1f}\")\n",
    "    print(f\"  üõ°Ô∏è Ratio de cortes√≠a: {compliance_report['ethical_compliance']['politeness_ratio']:.1f}%\")\n",
    "    \n",
    "    # Guardar reporte\n",
    "    report_file = framework.save_compliance_report()\n",
    "    print(f\"\\nüíæ Reporte completo guardado en: {report_file}\")\n",
    "    \n",
    "    print(\"\\nüí° Recomendaciones principales:\")\n",
    "    for rec in compliance_report['recommendations'][:3]:\n",
    "        print(f\"  {rec}\")\n",
    "\nelse:\n",
    "    print(\"‚ùå Proyecto no aprobado por razones √©ticas\")\n",
    "\nprint(\"\\n‚úÖ Framework de scraping √©tico completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Checklist de Mejores Pr√°cticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_best_practices_checklist():\n",
    "    \"\"\"Generar checklist completo de mejores pr√°cticas\"\"\"\n",
    "    \n",
    "    checklist = {\n",
    "        \"üìñ Aspectos Legales y √âticos\": [\n",
    "            \"‚úÖ Revisar t√©rminos de servicio del sitio web\",\n",
    "            \"‚úÖ Verificar que los datos sean p√∫blicamente accesibles\",\n",
    "            \"‚úÖ Evitar scraping de datos personales o privados\",\n",
    "            \"‚úÖ Obtener autorizaci√≥n para uso comercial si es necesario\",\n",
    "            \"‚úÖ Respetar derechos de autor y propiedad intelectual\",\n",
    "            \"‚úÖ Documentar el prop√≥sito del scraping\",\n",
    "            \"‚úÖ Considerar alternativas como APIs oficiales\"\n",
    "        ],\n",
    "        \n",
    "        \"ü§ñ Respeto a Robots.txt\": [\n",
    "            \"‚úÖ Verificar y respetar robots.txt antes de scrapear\",\n",
    "            \"‚úÖ Implementar parser de robots.txt en el c√≥digo\",\n",
    "            \"‚úÖ Respetar directivas de Crawl-delay\",\n",
    "            \"‚úÖ Revisar sitemaps indicados en robots.txt\",\n",
    "            \"‚úÖ Actualizar cache de robots.txt peri√≥dicamente\"\n",
    "        ],\n",
    "        \n",
    "        \"‚è±Ô∏è Rate Limiting y Cortes√≠a\": [\n",
    "            \"‚úÖ Implementar delays entre requests (m√≠nimo 1-3 segundos)\",\n",
    "            \"‚úÖ Usar delays variables para parecer m√°s humano\",\n",
    "            \"‚úÖ Limitar requests concurrentes (m√°ximo 1-2 por dominio)\",\n",
    "            \"‚úÖ Implementar backoff exponencial en errores\",\n",
    "            \"‚úÖ Monitorear tiempo de respuesta del servidor\",\n",
    "            \"‚úÖ Pausar o reducir velocidad si el servidor est√° lento\",\n",
    "            \"‚úÖ Evitar hacer scraping en horas pico del sitio\"\n",
    "        ],\n",
    "        \n",
    "        \"üÜî Identificaci√≥n y Headers\": [\n",
    "            \"‚úÖ Usar User-Agent identificable con informaci√≥n de contacto\",\n",
    "            \"‚úÖ Incluir headers HTTP realistas\",\n",
    "            \"‚úÖ Rotar User-Agents si es necesario (pero mantener identificaci√≥n)\",\n",
    "            \"‚úÖ Incluir Accept, Accept-Language y otros headers comunes\",\n",
    "            \"‚úÖ Evitar headers que revelen herramientas de scraping\"\n",
    "        ],\n",
    "        \n",
    "        \"üîß Aspectos T√©cnicos\": [\n",
    "            \"‚úÖ Manejar errores gracefully (404, 500, timeouts)\",\n",
    "            \"‚úÖ Implementar reintentos con backoff\",\n",
    "            \"‚úÖ Usar timeouts apropiados para requests\",\n",
    "            \"‚úÖ Manejar diferentes encodings de texto\",\n",
    "            \"‚úÖ Limpiar y validar datos extra√≠dos\",\n",
    "            \"‚úÖ Implementar detecci√≥n de cambios en estructura\",\n",
    "            \"‚úÖ Usar proxies rotativos si es necesario\"\n",
    "        ],\n",
    "        \n",
    "        \"üìä Monitoreo y Logging\": [\n",
    "            \"‚úÖ Loggear todos los requests realizados\",\n",
    "            \"‚úÖ Monitorear tasas de error y bloqueos\",\n",
    "            \"‚úÖ Implementar alertas para problemas\",\n",
    "            \"‚úÖ Registrar m√©tricas de rendimiento\",\n",
    "            \"‚úÖ Mantener audit trail de todas las actividades\",\n",
    "            \"‚úÖ Generar reportes de cumplimiento regulares\"\n",
    "        ],\n",
    "        \n",
    "        \"üõ°Ô∏è Seguridad y Privacidad\": [\n",
    "            \"‚úÖ No almacenar credenciales en c√≥digo\",\n",
    "            \"‚úÖ Usar HTTPS cuando est√© disponible\",\n",
    "            \"‚úÖ Anonimizar datos personales si se encuentran\",\n",
    "            \"‚úÖ Implementar cifrado para datos sensibles\",\n",
    "            \"‚úÖ Seguir principios de minimizaci√≥n de datos\",\n",
    "            \"‚úÖ Implementar pol√≠ticas de retenci√≥n de datos\"\n",
    "        ],\n",
    "        \n",
    "        \"üíæ Almacenamiento y Procesamiento\": [\n",
    "            \"‚úÖ Evitar duplicados de datos\",\n",
    "            \"‚úÖ Implementar versionado de datos\",\n",
    "            \"‚úÖ Documentar esquema de datos\",\n",
    "            \"‚úÖ Implementar backups regulares\",\n",
    "            \"‚úÖ Optimizar consultas de base de datos\",\n",
    "            \"‚úÖ Limpiar datos obsoletos regularmente\"\n",
    "        ],\n",
    "        \n",
    "        \"üöÄ Mantenimiento y Escalabilidad\": [\n",
    "            \"‚úÖ Monitorear cambios en sitios web target\",\n",
    "            \"‚úÖ Mantener c√≥digo modular y configurable\",\n",
    "            \"‚úÖ Implementar tests automatizados\",\n",
    "            \"‚úÖ Documentar configuraci√≥n y uso\",\n",
    "            \"‚úÖ Planificar escalabilidad horizontal\",\n",
    "            \"‚úÖ Implementar m√©tricas de negocio relevantes\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return checklist\n",
    "\n",
    "def print_checklist():\n",
    "    \"\"\"Imprimir checklist formateado\"\"\"\n",
    "    checklist = generate_best_practices_checklist()\n",
    "    \n",
    "    print(\"üìã CHECKLIST COMPLETO DE MEJORES PR√ÅCTICAS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for category, items in checklist.items():\n",
    "        print(f\"\\n{category}\")\n",
    "        print(\"-\" * len(category))\n",
    "        \n",
    "        for item in items:\n",
    "            print(f\"  {item}\")\n",
    "    \n",
    "    total_items = sum(len(items) for items in checklist.values())\n",
    "    print(f\"\\nüìä Total de mejores pr√°cticas: {total_items}\")\n",
    "    \n",
    "    return checklist\n",
    "\n",
    "# Generar y mostrar checklist\n",
    "practices_checklist = print_checklist()\n",
    "\n",
    "# Funci√≥n para evaluar cumplimiento\n",
    "def evaluate_project_compliance(project_checklist: Dict[str, bool]) -> Dict[str, any]:\n",
    "    \"\"\"Evaluar cumplimiento de un proyecto\"\"\"\n",
    "    \n",
    "    total_practices = sum(len(items) for items in practices_checklist.values())\n",
    "    completed_practices = sum(1 for completed in project_checklist.values() if completed)\n",
    "    \n",
    "    compliance_percentage = (completed_practices / total_practices) * 100\n",
    "    \n",
    "    # Determinar nivel de cumplimiento\n",
    "    if compliance_percentage >= 90:\n",
    "        compliance_level = \"Excelente üèÜ\"\n",
    "    elif compliance_percentage >= 75:\n",
    "        compliance_level = \"Bueno üëç\"\n",
    "    elif compliance_percentage >= 60:\n",
    "        compliance_level = \"Aceptable ‚ö†Ô∏è\"\n",
    "    else:\n",
    "        compliance_level = \"Necesita Mejoras ‚ùå\"\n",
    "    \n",
    "    return {\n",
    "        'compliance_percentage': compliance_percentage,\n",
    "        'completed_practices': completed_practices,\n",
    "        'total_practices': total_practices,\n",
    "        'compliance_level': compliance_level,\n",
    "        'missing_practices': total_practices - completed_practices\n",
    "    }\n",
    "\n",
    "print(f\"\\n\\nüí° CONSEJOS FINALES:\")\n",
    "print(\"=\" * 25)\n",
    "print(\"üéØ El scraping √©tico no es solo sobre cumplir reglas\")\n",
    "print(\"ü§ù Es sobre ser un buen ciudadano digital\")\n",
    "print(\"‚öñÔ∏è Siempre preg√∫ntate: ¬øEs esto lo correcto?\")\n",
    "print(\"üìö Mantente actualizado sobre cambios legales\")\n",
    "print(\"üîÑ Revisa y actualiza pr√°cticas regularmente\")\n",
    "print(\"üåç Considera el impacto en la comunidad web\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Graduaci√≥n: Eres Ahora un Web Scraper √âtico\n",
    "\n",
    "## üèÜ ¬°Felicidades! Has Completado el Curso\n",
    "\n",
    "### üéØ Lo que Has Dominado\n",
    "\n",
    "1. **Fundamentos S√≥lidos**:\n",
    "   - HTML, CSS, y estructura web\n",
    "   - HTTP, requests, y manejo de respuestas\n",
    "   - Beautiful Soup y parsing de documentos\n",
    "\n",
    "2. **T√©cnicas Avanzadas de Selecci√≥n**:\n",
    "   - XPath completo con predicados y funciones\n",
    "   - CSS Selectors avanzados\n",
    "   - Expresiones regulares para patrones complejos\n",
    "\n",
    "3. **Scrapy Framework Master**:\n",
    "   - Spiders b√°sicos y avanzados\n",
    "   - CrawlSpiders y seguimiento de enlaces\n",
    "   - Pipelines y transformaci√≥n de datos\n",
    "   - Middlewares personalizados\n",
    "\n",
    "4. **Procesamiento de Datos Industrial**:\n",
    "   - Limpieza y validaci√≥n autom√°tica\n",
    "   - Bases de datos y almacenamiento\n",
    "   - An√°lisis y visualizaci√≥n\n",
    "   - Sistemas de monitoreo\n",
    "\n",
    "5. **√âtica y Responsabilidad**:\n",
    "   - Aspectos legales del web scraping\n",
    "   - Robots.txt y pol√≠ticas de cortes√≠a\n",
    "   - Rate limiting inteligente\n",
    "   - Frameworks de scraping √©tico\n",
    "\n",
    "### üõ†Ô∏è Tu Arsenal de Herramientas\n",
    "\n",
    "```python\n",
    "# Toolkit Completo del Web Scraper √âtico\n",
    "requests                    # HTTP requests\n",
    "beautifulsoup4             # HTML parsing\n",
    "scrapy                     # Framework industrial\n",
    "lxml                       # XPath avanzado\n",
    "pandas                     # Procesamiento datos\n",
    "sqlite3                    # Base de datos\n",
    "matplotlib/seaborn         # Visualizaci√≥n\n",
    "selenium                   # JavaScript handling\n",
    "urllib.robotparser         # Robots.txt\n",
    "```\n",
    "\n",
    "### üìà Niveles de Expertise Alcanzados\n",
    "\n",
    "- **üü¢ Principiante ‚Üí Superado**: HTML b√°sico, requests simples\n",
    "- **üü° Intermedio ‚Üí Superado**: Beautiful Soup, CSS selectors, manejo de errores\n",
    "- **üü† Avanzado ‚Üí Superado**: XPath, Scrapy, pipelines, bases de datos\n",
    "- **üî¥ Experto ‚Üí ¬°ALCANZADO!**: Frameworks √©ticos, sistemas industriales, monitoreo\n",
    "\n",
    "### üöÄ Pr√≥ximos Pasos en tu Carrera\n",
    "\n",
    "1. **Proyectos Personales**:\n",
    "   - Construir scrapers para tus intereses\n",
    "   - Crear dashboards de datos\n",
    "   - Automatizar tareas repetitivas\n",
    "\n",
    "2. **Especializaci√≥n Avanzada**:\n",
    "   - Machine Learning para an√°lisis de datos\n",
    "   - APIs y microservicios\n",
    "   - Cloud computing y escalabilidad\n",
    "   - Real-time data processing\n",
    "\n",
    "3. **Contribuci√≥n a la Comunidad**:\n",
    "   - Contribuir a proyectos open source\n",
    "   - Compartir conocimiento en blogs/tutorials\n",
    "   - Mentorizar otros developers\n",
    "\n",
    "### üåü Principios para Recordar Siempre\n",
    "\n",
    "1. **üõ°Ô∏è √âtica Primero**: Siempre considera el impacto de tu scraping\n",
    "2. **ü§ñ Respeta Robots.txt**: Es la ley no escrita de la web\n",
    "3. **‚è±Ô∏è S√© Cort√©s**: Rate limiting salva servidores y tu reputaci√≥n\n",
    "4. **üìä Calidad sobre Cantidad**: Datos limpios valen m√°s que datos abundantes\n",
    "5. **üîÑ Mant√©n y Actualiza**: El web cambia, tu c√≥digo tambi√©n debe hacerlo\n",
    "6. **üìö Nunca Pares de Aprender**: La tecnolog√≠a evoluciona constantemente\n",
    "\n",
    "### üéñÔ∏è Certificado de Completaci√≥n\n",
    "\n",
    "```\n",
    "üèÜ CERTIFICADO DE MAESTR√çA EN WEB SCRAPING √âTICO üèÜ\n",
    "\n",
    "Por la presente se certifica que has completado exitosamente\n",
    "el Curso Completo de Web Scraping con Python\n",
    "\n",
    "Habilidades Certificadas:\n",
    "‚úÖ Web Scraping √âtico y Responsable\n",
    "‚úÖ Frameworks Scrapy Avanzado\n",
    "‚úÖ Procesamiento Industrial de Datos\n",
    "‚úÖ Compliance Legal y T√©cnico\n",
    "‚úÖ Arquitectura de Sistemas Escalables\n",
    "\n",
    "Fecha: {datetime.now().strftime('%B %d, %Y')}\n",
    "Instructor: Claude (Anthropic)\n",
    "Curso: Web Scraping con Python - Nivel Experto\n",
    "```\n",
    "\n",
    "### üíå Mensaje Final\n",
    "\n",
    "Has recorrido un camino incre√≠ble desde los conceptos b√°sicos de HTML hasta la construcci√≥n de sistemas completos de scraping √©tico. No solo has aprendido a extraer datos de la web, sino que has desarrollado una mentalidad responsable y profesional hacia esta poderosa tecnolog√≠a.\n",
    "\n",
    "El web scraping es una herramienta que puede transformar industrias, acelerar investigaci√≥n, y democratizar el acceso a informaci√≥n. Con los conocimientos que ahora posees, tienes la responsabilidad de usar esta herramienta de manera √©tica y constructiva.\n",
    "\n",
    "### üåê La Web Te Espera\n",
    "\n",
    "Ahora ve y construye cosas incre√≠bles. El mundo de los datos est√° esperando a que lo explores de manera responsable y √©tica.\n",
    "\n",
    "**¬°Que tengas un excelente viaje scrapeando! üï∑Ô∏è‚ú®**\n",
    "\n",
    "---\n",
    "\n",
    "*\"Con gran poder viene gran responsabilidad\"* - Aplica esto siempre a tu web scraping. üõ°Ô∏è"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}