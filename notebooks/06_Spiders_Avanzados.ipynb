{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üï∏Ô∏è Lecci√≥n 6: Construcci√≥n de Spiders Avanzados\n",
    "\n",
    "## üéØ Objetivos\n",
    "\n",
    "- Dominar diferentes tipos de spiders\n",
    "- Implementar seguimiento de enlaces autom√°tico\n",
    "- Manejar paginaci√≥n compleja\n",
    "- Optimizar el procesamiento paralelo\n",
    "- Crear middlewares personalizados\n",
    "- Manejar sitios web din√°micos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from scrapy import signals\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üï∏Ô∏è SPIDERS AVANZADOS\")\n",
    "print(\"=\" * 25)\n",
    "print(\"‚úÖ M√≥dulos importados correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üï∑Ô∏è CrawlSpider: Seguimiento Autom√°tico de Enlaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsCrawlSpider(CrawlSpider):\n",
    "    \"\"\"Spider que rastrea autom√°ticamente enlaces de noticias\"\"\"\n",
    "    name = 'news_crawler'\n",
    "    allowed_domains = ['example-news.com']\n",
    "    start_urls = ['http://example-news.com/']\n",
    "    \n",
    "    # Reglas para seguir enlaces\n",
    "    rules = (\n",
    "        # Seguir enlaces de categor√≠as\n",
    "        Rule(\n",
    "            LinkExtractor(\n",
    "                allow=(r'/category/',),\n",
    "                deny=(r'/admin/', r'/login/')\n",
    "            ),\n",
    "            callback='parse_category',\n",
    "            follow=True\n",
    "        ),\n",
    "        \n",
    "        # Seguir enlaces de art√≠culos\n",
    "        Rule(\n",
    "            LinkExtractor(\n",
    "                allow=(r'/article/\\d+/',),\n",
    "                restrict_css='.article-links'\n",
    "            ),\n",
    "            callback='parse_article',\n",
    "            follow=False\n",
    "        ),\n",
    "        \n",
    "        # Seguir paginaci√≥n\n",
    "        Rule(\n",
    "            LinkExtractor(\n",
    "                allow=(r'\\?page=\\d+',),\n",
    "                restrict_css='.pagination'\n",
    "            ),\n",
    "            follow=True\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    def parse_category(self, response):\n",
    "        \"\"\"Procesar p√°ginas de categor√≠as\"\"\"\n",
    "        category_name = response.css('h1::text').get()\n",
    "        articles_count = len(response.css('.article-item'))\n",
    "        \n",
    "        yield {\n",
    "            'type': 'category',\n",
    "            'name': category_name,\n",
    "            'articles_count': articles_count,\n",
    "            'url': response.url\n",
    "        }\n",
    "    \n",
    "    def parse_article(self, response):\n",
    "        \"\"\"Procesar art√≠culos individuales\"\"\"\n",
    "        yield {\n",
    "            'type': 'article',\n",
    "            'title': response.css('h1::text').get(),\n",
    "            'content': response.css('.article-content::text').getall(),\n",
    "            'author': response.css('.author::text').get(),\n",
    "            'date': response.css('.date::text').get(),\n",
    "            'category': response.css('.breadcrumb li:last-child::text').get(),\n",
    "            'url': response.url,\n",
    "            'word_count': len(' '.join(response.css('.article-content::text').getall()).split())\n",
    "        }\n",
    "\n",
    "print(\"üï∑Ô∏è CRAWLSPIDER CREADO\")\n",
    "print(\"Caracter√≠sticas del CrawlSpider:\")\n",
    "print(\"  üîó Seguimiento autom√°tico de enlaces\")\n",
    "print(\"  üìã Reglas configurables\")\n",
    "print(\"  üö´ Filtros allow/deny\")\n",
    "print(\"  üìÑ Paginaci√≥n autom√°tica\")\n",
    "print(\"  üéØ Callbacks espec√≠ficos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Spider con Paginaci√≥n Compleja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaginationSpider(scrapy.Spider):\n",
    "    \"\"\"Spider que maneja diferentes tipos de paginaci√≥n\"\"\"\n",
    "    name = 'pagination_spider'\n",
    "    \n",
    "    def __init__(self, start_page=1, max_pages=10, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.start_page = int(start_page)\n",
    "        self.max_pages = int(max_pages)\n",
    "        self.current_page = self.start_page\n",
    "        \n",
    "        # URLs din√°micas\n",
    "        self.base_url = 'http://example.com/products'\n",
    "        self.start_urls = [f'{self.base_url}?page={self.start_page}']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        \"\"\"M√©todo principal con m√∫ltiples estrategias de paginaci√≥n\"\"\"\n",
    "        \n",
    "        # Extraer productos de la p√°gina actual\n",
    "        products = response.css('.product-item')\n",
    "        \n",
    "        for product in products:\n",
    "            yield {\n",
    "                'name': product.css('.product-name::text').get(),\n",
    "                'price': product.css('.price::text').get(),\n",
    "                'rating': product.css('.rating::attr(data-rating)').get(),\n",
    "                'page': self.current_page,\n",
    "                'url': response.url\n",
    "            }\n",
    "        \n",
    "        # Estrategia 1: Paginaci√≥n con enlaces \"Next\"\n",
    "        next_page = response.css('.pagination .next::attr(href)').get()\n",
    "        if next_page and self.current_page < self.max_pages:\n",
    "            self.current_page += 1\n",
    "            yield response.follow(next_page, self.parse)\n",
    "        \n",
    "        # Estrategia 2: Paginaci√≥n con n√∫meros de p√°gina\n",
    "        elif not next_page and self.current_page < self.max_pages:\n",
    "            next_page_num = self.current_page + 1\n",
    "            next_url = f'{self.base_url}?page={next_page_num}'\n",
    "            \n",
    "            # Verificar si existe la pr√≥xima p√°gina\n",
    "            yield scrapy.Request(\n",
    "                next_url,\n",
    "                callback=self.parse,\n",
    "                errback=self.handle_pagination_error\n",
    "            )\n",
    "            self.current_page += 1\n",
    "        \n",
    "        # Estrategia 3: Paginaci√≥n AJAX (simulada)\n",
    "        if response.css('.load-more-button'):\n",
    "            yield self.make_ajax_request(response)\n",
    "    \n",
    "    def make_ajax_request(self, response):\n",
    "        \"\"\"Simular request AJAX para m√°s contenido\"\"\"\n",
    "        ajax_url = response.urljoin('/api/products')\n",
    "        \n",
    "        return scrapy.Request(\n",
    "            ajax_url,\n",
    "            method='POST',\n",
    "            headers={'Content-Type': 'application/json'},\n",
    "            body=json.dumps({\n",
    "                'page': self.current_page + 1,\n",
    "                'limit': 20\n",
    "            }),\n",
    "            callback=self.parse_ajax_response\n",
    "        )\n",
    "    \n",
    "    def parse_ajax_response(self, response):\n",
    "        \"\"\"Procesar respuesta AJAX JSON\"\"\"\n",
    "        data = json.loads(response.text)\n",
    "        \n",
    "        for item in data.get('products', []):\n",
    "            yield {\n",
    "                'name': item.get('name'),\n",
    "                'price': item.get('price'),\n",
    "                'rating': item.get('rating'),\n",
    "                'page': self.current_page,\n",
    "                'source': 'ajax'\n",
    "            }\n",
    "        \n",
    "        # Continuar si hay m√°s p√°ginas\n",
    "        if data.get('has_next') and self.current_page < self.max_pages:\n",
    "            self.current_page += 1\n",
    "            yield self.make_ajax_request(response)\n",
    "    \n",
    "    def handle_pagination_error(self, failure):\n",
    "        \"\"\"Manejar errores en paginaci√≥n\"\"\"\n",
    "        self.logger.error(f'Error en paginaci√≥n: {failure.value}')\n",
    "        # Detener paginaci√≥n en caso de error 404\n",
    "        if hasattr(failure.value, 'response') and failure.value.response.status == 404:\n",
    "            self.logger.info('Fin de p√°ginas alcanzado (404)')\n",
    "\n",
    "print(\"üîÑ SPIDER DE PAGINACI√ìN CREADO\")\n",
    "print(\"Estrategias de paginaci√≥n:\")\n",
    "print(\"  1. üîó Enlaces 'Next' tradicionales\")\n",
    "print(\"  2. üî¢ N√∫meros de p√°gina secuenciales\")\n",
    "print(\"  3. ‚ö° Carga din√°mica AJAX\")\n",
    "print(\"  4. ‚ùå Manejo de errores y l√≠mites\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Spider con Procesamiento Paralelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelSpider(scrapy.Spider):\n",
    "    \"\"\"Spider optimizado para procesamiento paralelo\"\"\"\n",
    "    name = 'parallel_spider'\n",
    "    \n",
    "    custom_settings = {\n",
    "        'CONCURRENT_REQUESTS': 32,  # Aumentar requests concurrentes\n",
    "        'CONCURRENT_REQUESTS_PER_DOMAIN': 16,\n",
    "        'DOWNLOAD_DELAY': 0.25,  # Delay m√≠nimo\n",
    "        'RANDOMIZE_DOWNLOAD_DELAY': 0.5,\n",
    "        'AUTOTHROTTLE_ENABLED': True,  # Auto-ajuste de velocidad\n",
    "        'AUTOTHROTTLE_START_DELAY': 0.25,\n",
    "        'AUTOTHROTTLE_MAX_DELAY': 10,\n",
    "        'AUTOTHROTTLE_TARGET_CONCURRENCY': 2.0,\n",
    "        'AUTOTHROTTLE_DEBUG': True,\n",
    "    }\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.stats = {\n",
    "            'requests_made': 0,\n",
    "            'responses_received': 0,\n",
    "            'items_scraped': 0,\n",
    "            'errors': 0,\n",
    "            'start_time': time.time()\n",
    "        }\n",
    "    \n",
    "    def start_requests(self):\n",
    "        \"\"\"Generar m√∫ltiples requests iniciales para paralelizaci√≥n\"\"\"\n",
    "        # Generar URLs para m√∫ltiples categor√≠as simult√°neamente\n",
    "        categories = ['electronics', 'books', 'clothing', 'home', 'sports']\n",
    "        \n",
    "        for category in categories:\n",
    "            for page in range(1, 6):  # 5 p√°ginas por categor√≠a\n",
    "                url = f'http://example.com/{category}?page={page}'\n",
    "                self.stats['requests_made'] += 1\n",
    "                \n",
    "                yield scrapy.Request(\n",
    "                    url,\n",
    "                    callback=self.parse_category,\n",
    "                    meta={\n",
    "                        'category': category,\n",
    "                        'page': page,\n",
    "                        'priority': 10 - page  # Prioridad mayor para primeras p√°ginas\n",
    "                    }\n",
    "                )\n",
    "    \n",
    "    def parse_category(self, response):\n",
    "        \"\"\"Procesar p√°ginas de categor√≠a en paralelo\"\"\"\n",
    "        self.stats['responses_received'] += 1\n",
    "        category = response.meta['category']\n",
    "        page = response.meta['page']\n",
    "        \n",
    "        # Extraer productos\n",
    "        products = response.css('.product-item')\n",
    "        \n",
    "        for product in products:\n",
    "            product_url = product.css('a::attr(href)').get()\n",
    "            if product_url:\n",
    "                # Crear request para p√°gina de producto con alta prioridad\n",
    "                yield scrapy.Request(\n",
    "                    response.urljoin(product_url),\n",
    "                    callback=self.parse_product,\n",
    "                    meta={\n",
    "                        'category': category,\n",
    "                        'source_page': page\n",
    "                    },\n",
    "                    priority=15  # Alta prioridad para productos\n",
    "                )\n",
    "    \n",
    "    def parse_product(self, response):\n",
    "        \"\"\"Procesar productos individuales\"\"\"\n",
    "        try:\n",
    "            # Extracci√≥n r√°pida y eficiente\n",
    "            item = {\n",
    "                'name': response.css('h1::text').get(),\n",
    "                'price': self.extract_price(response),\n",
    "                'category': response.meta['category'],\n",
    "                'description': self.extract_description(response),\n",
    "                'rating': response.css('.rating::attr(data-rating)').get(),\n",
    "                'availability': response.css('.availability::text').get(),\n",
    "                'sku': response.css('[data-sku]::attr(data-sku)').get(),\n",
    "                'url': response.url,\n",
    "                'scraped_at': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            self.stats['items_scraped'] += 1\n",
    "            \n",
    "            # Log progreso cada 100 items\n",
    "            if self.stats['items_scraped'] % 100 == 0:\n",
    "                elapsed = time.time() - self.stats['start_time']\n",
    "                rate = self.stats['items_scraped'] / elapsed\n",
    "                self.logger.info(f'Scraped {self.stats[\"items_scraped\"]} items at {rate:.2f} items/sec')\n",
    "            \n",
    "            yield item\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.stats['errors'] += 1\n",
    "            self.logger.error(f'Error parsing product {response.url}: {e}')\n",
    "    \n",
    "    def extract_price(self, response):\n",
    "        \"\"\"Extraer precio con m√∫ltiples selectores\"\"\"\n",
    "        price_selectors = [\n",
    "            '.price::text',\n",
    "            '.current-price::text',\n",
    "            '[data-price]::attr(data-price)',\n",
    "            '.price-now::text'\n",
    "        ]\n",
    "        \n",
    "        for selector in price_selectors:\n",
    "            price = response.css(selector).get()\n",
    "            if price:\n",
    "                return price.strip()\n",
    "        return None\n",
    "    \n",
    "    def extract_description(self, response):\n",
    "        \"\"\"Extraer descripci√≥n limitando longitud\"\"\"\n",
    "        description = response.css('.description::text').getall()\n",
    "        if description:\n",
    "            full_desc = ' '.join(description).strip()\n",
    "            return full_desc[:500] + '...' if len(full_desc) > 500 else full_desc\n",
    "        return None\n",
    "    \n",
    "    def closed(self, reason):\n",
    "        \"\"\"Estad√≠sticas finales cuando termina el spider\"\"\"\n",
    "        elapsed = time.time() - self.stats['start_time']\n",
    "        \n",
    "        self.logger.info('=== ESTAD√çSTICAS FINALES ===')\n",
    "        self.logger.info(f'Tiempo total: {elapsed:.2f} segundos')\n",
    "        self.logger.info(f'Requests realizados: {self.stats[\"requests_made\"]}')\n",
    "        self.logger.info(f'Responses recibidas: {self.stats[\"responses_received\"]}')\n",
    "        self.logger.info(f'Items scrapeados: {self.stats[\"items_scraped\"]}')\n",
    "        self.logger.info(f'Errores: {self.stats[\"errors\"]}')\n",
    "        \n",
    "        if elapsed > 0:\n",
    "            self.logger.info(f'Velocidad promedio: {self.stats[\"items_scraped\"] / elapsed:.2f} items/sec')\n",
    "\n",
    "print(\"üöÄ SPIDER PARALELO CREADO\")\n",
    "print(\"Optimizaciones implementadas:\")\n",
    "print(\"  ‚ö° 32 requests concurrentes\")\n",
    "print(\"  üéØ Sistema de prioridades\")\n",
    "print(\"  üìä Auto-throttling inteligente\")\n",
    "print(\"  üìà M√©tricas en tiempo real\")\n",
    "print(\"  üîß Extracci√≥n optimizada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ°Ô∏è Middleware Avanzado para Anti-Detecci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from scrapy.downloadermiddlewares.useragent import UserAgentMiddleware\n",
    "from scrapy import signals\n",
    "\n",
    "class AntiDetectionMiddleware:\n",
    "    \"\"\"Middleware avanzado para evitar detecci√≥n\"\"\"\n",
    "    \n",
    "    def __init__(self, crawler):\n",
    "        self.crawler = crawler\n",
    "        \n",
    "        # Pool de User Agents realistas\n",
    "        self.user_agents = [\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',\n",
    "            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15',\n",
    "            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "        ]\n",
    "        \n",
    "        # Headers adicionales para parecer m√°s humano\n",
    "        self.common_headers = {\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'DNT': '1',\n",
    "            'Connection': 'keep-alive',\n",
    "        }\n",
    "        \n",
    "        # Estad√≠sticas de requests\n",
    "        self.requests_count = 0\n",
    "        self.blocks_detected = 0\n",
    "    \n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        return cls(crawler)\n",
    "    \n",
    "    def process_request(self, request, spider):\n",
    "        \"\"\"Procesar cada request para evitar detecci√≥n\"\"\"\n",
    "        self.requests_count += 1\n",
    "        \n",
    "        # Rotar User Agent\n",
    "        request.headers['User-Agent'] = random.choice(self.user_agents)\n",
    "        \n",
    "        # Agregar headers comunes\n",
    "        for header, value in self.common_headers.items():\n",
    "            request.headers[header] = value\n",
    "        \n",
    "        # Agregar headers espec√≠ficos del sitio\n",
    "        if 'amazon' in request.url:\n",
    "            request.headers['Accept'] = 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'\n",
    "            request.headers['Upgrade-Insecure-Requests'] = '1'\n",
    "        \n",
    "        # Simular comportamiento humano con delays variables\n",
    "        if hasattr(request, 'meta'):\n",
    "            if not request.meta.get('dont_delay'):\n",
    "                request.meta['download_delay'] = random.uniform(1, 3)\n",
    "        \n",
    "        # Log cada 100 requests\n",
    "        if self.requests_count % 100 == 0:\n",
    "            spider.logger.info(f'Processed {self.requests_count} requests, {self.blocks_detected} blocks detected')\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def process_response(self, request, response, spider):\n",
    "        \"\"\"Analizar respuestas para detectar bloqueos\"\"\"\n",
    "        \n",
    "        # Detectar p√°ginas de bloqueo comunes\n",
    "        block_indicators = [\n",
    "            'access denied',\n",
    "            'blocked',\n",
    "            'captcha',\n",
    "            'robot',\n",
    "            'suspicious activity',\n",
    "            'rate limit'\n",
    "        ]\n",
    "        \n",
    "        response_text = response.text.lower()\n",
    "        \n",
    "        for indicator in block_indicators:\n",
    "            if indicator in response_text:\n",
    "                self.blocks_detected += 1\n",
    "                spider.logger.warning(f'Possible block detected on {request.url}: {indicator}')\n",
    "                \n",
    "                # Crear nuevo request con delay mayor\n",
    "                new_request = request.copy()\n",
    "                new_request.meta['download_delay'] = 30  # Esperar 30 segundos\n",
    "                new_request.meta['retry_count'] = request.meta.get('retry_count', 0) + 1\n",
    "                \n",
    "                if new_request.meta['retry_count'] < 3:\n",
    "                    return new_request\n",
    "                else:\n",
    "                    spider.logger.error(f'Max retries reached for {request.url}')\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def process_exception(self, request, exception, spider):\n",
    "        \"\"\"Manejar excepciones de red\"\"\"\n",
    "        spider.logger.error(f'Exception for {request.url}: {exception}')\n",
    "        return None\n",
    "\n",
    "# Middleware para gesti√≥n inteligente de cookies\n",
    "class SmartCookieMiddleware:\n",
    "    \"\"\"Middleware para manejo inteligente de cookies\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.session_cookies = {}\n",
    "    \n",
    "    def process_request(self, request, spider):\n",
    "        \"\"\"Agregar cookies persistentes\"\"\"\n",
    "        domain = self.get_domain(request.url)\n",
    "        \n",
    "        if domain in self.session_cookies:\n",
    "            request.cookies.update(self.session_cookies[domain])\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def process_response(self, request, response, spider):\n",
    "        \"\"\"Guardar cookies de sesi√≥n\"\"\"\n",
    "        if response.headers.getlist('Set-Cookie'):\n",
    "            domain = self.get_domain(request.url)\n",
    "            \n",
    "            if domain not in self.session_cookies:\n",
    "                self.session_cookies[domain] = {}\n",
    "            \n",
    "            # Extraer y guardar cookies importantes\n",
    "            for cookie_header in response.headers.getlist('Set-Cookie'):\n",
    "                cookie_str = cookie_header.decode('utf-8')\n",
    "                if 'session' in cookie_str.lower() or 'auth' in cookie_str.lower():\n",
    "                    # Parsear y guardar cookie importante\n",
    "                    cookie_parts = cookie_str.split(';')[0].split('=')\n",
    "                    if len(cookie_parts) == 2:\n",
    "                        self.session_cookies[domain][cookie_parts[0]] = cookie_parts[1]\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def get_domain(self, url):\n",
    "        \"\"\"Extraer dominio de URL\"\"\"\n",
    "        from urllib.parse import urlparse\n",
    "        return urlparse(url).netloc\n",
    "\n",
    "print(\"üõ°Ô∏è MIDDLEWARES ANTI-DETECCI√ìN CREADOS\")\n",
    "print(\"Caracter√≠sticas de seguridad:\")\n",
    "print(\"  üé≠ Rotaci√≥n de User Agents\")\n",
    "print(\"  üîç Detecci√≥n de bloqueos\")\n",
    "print(\"  üç™ Gesti√≥n inteligente de cookies\")\n",
    "print(\"  ‚è±Ô∏è Delays variables\")\n",
    "print(\"  üîÑ Sistema de reintentos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üñ•Ô∏è Spider para Sitios con JavaScript (Selenium Integration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulaci√≥n de integraci√≥n con Selenium\n",
    "class JavaScriptSpider(scrapy.Spider):\n",
    "    \"\"\"Spider que maneja contenido JavaScript con Selenium\"\"\"\n",
    "    name = 'javascript_spider'\n",
    "    \n",
    "    custom_settings = {\n",
    "        'DOWNLOAD_DELAY': 3,  # Mayor delay para JS\n",
    "        'DOWNLOADER_MIDDLEWARES': {\n",
    "            'scrapy_selenium.SeleniumMiddleware': 800\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def start_requests(self):\n",
    "        \"\"\"Requests iniciales con configuraci√≥n de Selenium\"\"\"\n",
    "        urls = [\n",
    "            'https://spa-example.com/products',\n",
    "            'https://infinite-scroll.com/items'\n",
    "        ]\n",
    "        \n",
    "        for url in urls:\n",
    "            yield scrapy.Request(\n",
    "                url,\n",
    "                callback=self.parse_spa,\n",
    "                meta={\n",
    "                    'selenium': True,\n",
    "                    'selenium_wait_time': 10,\n",
    "                    'selenium_wait_condition': 'presence_of_element_located',\n",
    "                    'selenium_wait_condition_arg': '.product-item'\n",
    "                }\n",
    "            )\n",
    "    \n",
    "    def parse_spa(self, response):\n",
    "        \"\"\"Procesar Single Page Application\"\"\"\n",
    "        # En este punto, Selenium ya carg√≥ el JavaScript\n",
    "        products = response.css('.product-item')\n",
    "        \n",
    "        for product in products:\n",
    "            # Extraer datos que fueron generados por JavaScript\n",
    "            yield {\n",
    "                'name': product.css('.js-product-name::text').get(),\n",
    "                'price': product.css('[data-price]::attr(data-price)').get(),\n",
    "                'dynamic_rating': product.css('.js-rating::text').get(),\n",
    "                'availability': product.css('.js-stock::text').get(),\n",
    "                'url': response.url\n",
    "            }\n",
    "        \n",
    "        # Manejar infinite scroll\n",
    "        if response.css('.load-more-button'):\n",
    "            yield scrapy.Request(\n",
    "                response.url,\n",
    "                callback=self.handle_infinite_scroll,\n",
    "                meta={\n",
    "                    'selenium': True,\n",
    "                    'selenium_action': 'click_and_wait',\n",
    "                    'selenium_action_target': '.load-more-button',\n",
    "                    'selenium_wait_time': 5\n",
    "                }\n",
    "            )\n",
    "    \n",
    "    def handle_infinite_scroll(self, response):\n",
    "        \"\"\"Manejar carga infinita de contenido\"\"\"\n",
    "        # Procesar nuevos productos cargados\n",
    "        new_products = response.css('.product-item')\n",
    "        \n",
    "        for product in new_products:\n",
    "            yield {\n",
    "                'name': product.css('.js-product-name::text').get(),\n",
    "                'price': product.css('[data-price]::attr(data-price)').get(),\n",
    "                'source': 'infinite_scroll',\n",
    "                'url': response.url\n",
    "            }\n",
    "        \n",
    "        # Continuar scroll si hay m√°s contenido\n",
    "        if response.css('.load-more-button:not(.disabled)'):\n",
    "            yield scrapy.Request(\n",
    "                response.url,\n",
    "                callback=self.handle_infinite_scroll,\n",
    "                meta=response.meta\n",
    "            )\n",
    "\n",
    "# Simulaci√≥n de configuraci√≥n para Selenium\n",
    "SELENIUM_DRIVER_SETTINGS = {\n",
    "    'SELENIUM_DRIVER_NAME': 'chrome',\n",
    "    'SELENIUM_DRIVER_EXECUTABLE_PATH': None,  # Usar webdriver-manager\n",
    "    'SELENIUM_DRIVER_ARGUMENTS': [\n",
    "        '--headless',  # Ejecutar sin ventana\n",
    "        '--no-sandbox',\n",
    "        '--disable-dev-shm-usage',\n",
    "        '--disable-gpu',\n",
    "        '--window-size=1920,1080',\n",
    "        '--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"üñ•Ô∏è SPIDER JAVASCRIPT CREADO\")\n",
    "print(\"Capacidades JavaScript:\")\n",
    "print(\"  ‚ö° Integraci√≥n con Selenium\")\n",
    "print(\"  üîÑ Infinite scroll autom√°tico\")\n",
    "print(\"  ‚è≥ Esperas inteligentes\")\n",
    "print(\"  üñ±Ô∏è Interacciones con botones\")\n",
    "print(\"  üì± SPA (Single Page Apps)\")\n",
    "\n",
    "print(\"\\n‚öôÔ∏è Configuraci√≥n Selenium:\")\n",
    "for key, value in SELENIUM_DRIVER_SETTINGS.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Spider con Monitoreo Avanzado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonitoredSpider(scrapy.Spider):\n",
    "    \"\"\"Spider con sistema de monitoreo y alertas\"\"\"\n",
    "    name = 'monitored_spider'\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        # M√©tricas detalladas\n",
    "        self.metrics = {\n",
    "            'start_time': time.time(),\n",
    "            'pages_scraped': 0,\n",
    "            'items_extracted': 0,\n",
    "            'errors_encountered': 0,\n",
    "            'response_times': [],\n",
    "            'status_codes': {},\n",
    "            'domains_scraped': set(),\n",
    "            'data_quality_issues': 0\n",
    "        }\n",
    "        \n",
    "        # Umbrales para alertas\n",
    "        self.thresholds = {\n",
    "            'max_error_rate': 0.05,  # 5% de error m√°ximo\n",
    "            'min_items_per_minute': 10,\n",
    "            'max_response_time': 30,  # segundos\n",
    "            'max_consecutive_errors': 10\n",
    "        }\n",
    "        \n",
    "        self.consecutive_errors = 0\n",
    "        self.last_report_time = time.time()\n",
    "    \n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler, *args, **kwargs):\n",
    "        spider = super().from_crawler(crawler, *args, **kwargs)\n",
    "        \n",
    "        # Conectar signals\n",
    "        crawler.signals.connect(spider.spider_opened, signal=signals.spider_opened)\n",
    "        crawler.signals.connect(spider.spider_closed, signal=signals.spider_closed)\n",
    "        crawler.signals.connect(spider.request_scheduled, signal=signals.request_scheduled)\n",
    "        crawler.signals.connect(spider.response_received, signal=signals.response_received)\n",
    "        crawler.signals.connect(spider.item_scraped, signal=signals.item_scraped)\n",
    "        \n",
    "        return spider\n",
    "    \n",
    "    def spider_opened(self, spider):\n",
    "        \"\"\"Inicializar monitoreo\"\"\"\n",
    "        self.logger.info(f'üï∑Ô∏è Spider {spider.name} iniciado con monitoreo')\n",
    "        self.logger.info(f'üìä Umbrales configurados: {self.thresholds}')\n",
    "    \n",
    "    def request_scheduled(self, request, spider):\n",
    "        \"\"\"Monitorear requests programados\"\"\"\n",
    "        request.meta['request_start_time'] = time.time()\n",
    "        \n",
    "        # Rastrear dominios\n",
    "        from urllib.parse import urlparse\n",
    "        domain = urlparse(request.url).netloc\n",
    "        self.metrics['domains_scraped'].add(domain)\n",
    "    \n",
    "    def response_received(self, response, request, spider):\n",
    "        \"\"\"Analizar respuestas recibidas\"\"\"\n",
    "        # Calcular tiempo de respuesta\n",
    "        if 'request_start_time' in request.meta:\n",
    "            response_time = time.time() - request.meta['request_start_time']\n",
    "            self.metrics['response_times'].append(response_time)\n",
    "            \n",
    "            # Alertar si el tiempo de respuesta es muy alto\n",
    "            if response_time > self.thresholds['max_response_time']:\n",
    "                self.logger.warning(f'‚ö†Ô∏è Tiempo de respuesta alto: {response_time:.2f}s para {request.url}')\n",
    "        \n",
    "        # Rastrear c√≥digos de estado\n",
    "        status = response.status\n",
    "        self.metrics['status_codes'][status] = self.metrics['status_codes'].get(status, 0) + 1\n",
    "        \n",
    "        # Detectar errores\n",
    "        if status >= 400:\n",
    "            self.metrics['errors_encountered'] += 1\n",
    "            self.consecutive_errors += 1\n",
    "            \n",
    "            if self.consecutive_errors >= self.thresholds['max_consecutive_errors']:\n",
    "                self.send_alert(f'üö® {self.consecutive_errors} errores consecutivos detectados')\n",
    "        else:\n",
    "            self.consecutive_errors = 0\n",
    "        \n",
    "        self.metrics['pages_scraped'] += 1\n",
    "        \n",
    "        # Reporte peri√≥dico cada 5 minutos\n",
    "        if time.time() - self.last_report_time > 300:  # 5 minutos\n",
    "            self.generate_progress_report()\n",
    "            self.last_report_time = time.time()\n",
    "    \n",
    "    def item_scraped(self, item, response, spider):\n",
    "        \"\"\"Monitorear items extra√≠dos\"\"\"\n",
    "        self.metrics['items_extracted'] += 1\n",
    "        \n",
    "        # Verificar calidad de datos\n",
    "        if self.check_data_quality(item):\n",
    "            self.metrics['data_quality_issues'] += 1\n",
    "    \n",
    "    def check_data_quality(self, item):\n",
    "        \"\"\"Verificar calidad de los datos extra√≠dos\"\"\"\n",
    "        issues = 0\n",
    "        \n",
    "        # Verificar campos vac√≠os\n",
    "        for key, value in item.items():\n",
    "            if not value or (isinstance(value, str) and len(value.strip()) == 0):\n",
    "                issues += 1\n",
    "        \n",
    "        # Verificar datos sospechosos\n",
    "        if 'price' in item:\n",
    "            price_str = str(item['price'])\n",
    "            if 'error' in price_str.lower() or 'null' in price_str.lower():\n",
    "                issues += 1\n",
    "        \n",
    "        return issues > 0\n",
    "    \n",
    "    def generate_progress_report(self):\n",
    "        \"\"\"Generar reporte de progreso\"\"\"\n",
    "        elapsed = time.time() - self.metrics['start_time']\n",
    "        \n",
    "        # Calcular m√©tricas\n",
    "        pages_per_minute = (self.metrics['pages_scraped'] / elapsed) * 60\n",
    "        items_per_minute = (self.metrics['items_extracted'] / elapsed) * 60\n",
    "        error_rate = self.metrics['errors_encountered'] / max(self.metrics['pages_scraped'], 1)\n",
    "        avg_response_time = sum(self.metrics['response_times']) / max(len(self.metrics['response_times']), 1)\n",
    "        \n",
    "        # Log reporte\n",
    "        self.logger.info('üìä === REPORTE DE PROGRESO ===')\n",
    "        self.logger.info(f'‚è±Ô∏è Tiempo transcurrido: {elapsed/60:.1f} minutos')\n",
    "        self.logger.info(f'üìÑ P√°ginas scrapeadas: {self.metrics[\"pages_scraped\"]}')\n",
    "        self.logger.info(f'üì¶ Items extra√≠dos: {self.metrics[\"items_extracted\"]}')\n",
    "        self.logger.info(f'üåê Dominios: {len(self.metrics[\"domains_scraped\"])}')\n",
    "        self.logger.info(f'‚ö° Velocidad: {pages_per_minute:.1f} p√°ginas/min, {items_per_minute:.1f} items/min')\n",
    "        self.logger.info(f'‚ùå Tasa de error: {error_rate:.3f} ({self.metrics[\"errors_encountered\"]} errores)')\n",
    "        self.logger.info(f'üïê Tiempo promedio de respuesta: {avg_response_time:.2f}s')\n",
    "        self.logger.info(f'‚ö†Ô∏è Problemas de calidad: {self.metrics[\"data_quality_issues\"]}')\n",
    "        \n",
    "        # Verificar umbrales y enviar alertas\n",
    "        if error_rate > self.thresholds['max_error_rate']:\n",
    "            self.send_alert(f'üö® Tasa de error alta: {error_rate:.3f}')\n",
    "        \n",
    "        if items_per_minute < self.thresholds['min_items_per_minute']:\n",
    "            self.send_alert(f'üêå Velocidad baja: {items_per_minute:.1f} items/min')\n",
    "    \n",
    "    def send_alert(self, message):\n",
    "        \"\"\"Enviar alerta (simulado)\"\"\"\n",
    "        self.logger.error(f'ALERTA: {message}')\n",
    "        # Aqu√≠ se integrar√≠a con Slack, email, webhook, etc.\n",
    "    \n",
    "    def spider_closed(self, spider):\n",
    "        \"\"\"Generar reporte final\"\"\"\n",
    "        elapsed = time.time() - self.metrics['start_time']\n",
    "        \n",
    "        self.logger.info('üèÅ === REPORTE FINAL ===')\n",
    "        self.logger.info(f'‚è±Ô∏è Duraci√≥n total: {elapsed/60:.1f} minutos')\n",
    "        self.logger.info(f'üìä M√©tricas finales: {self.metrics[\"pages_scraped\"]} p√°ginas, {self.metrics[\"items_extracted\"]} items')\n",
    "        self.logger.info(f'üåê Dominios scrapeados: {\", \".join(self.metrics[\"domains_scraped\"])}')\n",
    "        self.logger.info(f'üìà C√≥digos de estado: {self.metrics[\"status_codes\"]}')\n",
    "        \n",
    "        if self.metrics['response_times']:\n",
    "            avg_rt = sum(self.metrics['response_times']) / len(self.metrics['response_times'])\n",
    "            max_rt = max(self.metrics['response_times'])\n",
    "            self.logger.info(f'üïê Tiempo de respuesta: promedio {avg_rt:.2f}s, m√°ximo {max_rt:.2f}s')\n",
    "\n",
    "print(\"üìä SPIDER MONITOREADO CREADO\")\n",
    "print(\"Sistema de monitoreo:\")\n",
    "print(\"  üìà M√©tricas en tiempo real\")\n",
    "print(\"  üö® Alertas autom√°ticas\")\n",
    "print(\"  üìã Reportes peri√≥dicos\")\n",
    "print(\"  üîç Control de calidad de datos\")\n",
    "print(\"  ‚è±Ô∏è An√°lisis de rendimiento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Ejercicio Pr√°ctico Completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ EJERCICIO PR√ÅCTICO: SPIDER MASTER\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "class MasterSpider(scrapy.Spider):\n",
    "    \"\"\"Spider que combina todas las t√©cnicas avanzadas\"\"\"\n",
    "    name = 'master_spider'\n",
    "    allowed_domains = ['example-store.com']\n",
    "    \n",
    "    custom_settings = {\n",
    "        # Configuraci√≥n de rendimiento\n",
    "        'CONCURRENT_REQUESTS': 16,\n",
    "        'CONCURRENT_REQUESTS_PER_DOMAIN': 8,\n",
    "        'DOWNLOAD_DELAY': 1,\n",
    "        'RANDOMIZE_DOWNLOAD_DELAY': 0.5,\n",
    "        \n",
    "        # Auto-throttling\n",
    "        'AUTOTHROTTLE_ENABLED': True,\n",
    "        'AUTOTHROTTLE_START_DELAY': 0.5,\n",
    "        'AUTOTHROTTLE_MAX_DELAY': 10,\n",
    "        'AUTOTHROTTLE_TARGET_CONCURRENCY': 2.0,\n",
    "        \n",
    "        # Middlewares\n",
    "        'DOWNLOADER_MIDDLEWARES': {\n",
    "            '__main__.AntiDetectionMiddleware': 500,\n",
    "            '__main__.SmartCookieMiddleware': 600,\n",
    "        },\n",
    "        \n",
    "        # Pipelines\n",
    "        'ITEM_PIPELINES': {\n",
    "            '__main__.ValidationPipeline': 300,\n",
    "            '__main__.ProcessingPipeline': 400,\n",
    "        },\n",
    "        \n",
    "        # Export\n",
    "        'FEEDS': {\n",
    "            'master_output.json': {\n",
    "                'format': 'json',\n",
    "                'encoding': 'utf8',\n",
    "                'indent': 2\n",
    "            },\n",
    "            'master_output.csv': {\n",
    "                'format': 'csv',\n",
    "                'encoding': 'utf8'\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        # Logging\n",
    "        'LOG_LEVEL': 'INFO'\n",
    "    }\n",
    "    \n",
    "    def __init__(self, category='all', max_pages=5, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.category = category\n",
    "        self.max_pages = int(max_pages)\n",
    "        self.scraped_urls = set()\n",
    "        \n",
    "        # Configurar URLs iniciales\n",
    "        if category == 'all':\n",
    "            self.start_urls = [\n",
    "                'https://example-store.com/categories/electronics',\n",
    "                'https://example-store.com/categories/books',\n",
    "                'https://example-store.com/categories/clothing'\n",
    "            ]\n",
    "        else:\n",
    "            self.start_urls = [f'https://example-store.com/categories/{category}']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        \"\"\"Parse principal con m√∫ltiples estrategias\"\"\"\n",
    "        # Extraer categor√≠a actual\n",
    "        category = self.extract_category(response)\n",
    "        \n",
    "        # Estrategia 1: Productos en p√°gina de categor√≠a\n",
    "        products = response.css('.product-item')\n",
    "        \n",
    "        for product in products:\n",
    "            product_url = product.css('a::attr(href)').get()\n",
    "            if product_url and product_url not in self.scraped_urls:\n",
    "                self.scraped_urls.add(product_url)\n",
    "                \n",
    "                yield response.follow(\n",
    "                    product_url,\n",
    "                    callback=self.parse_product,\n",
    "                    meta={'category': category}\n",
    "                )\n",
    "        \n",
    "        # Estrategia 2: Paginaci√≥n\n",
    "        next_page = response.css('.pagination .next::attr(href)').get()\n",
    "        current_page = self.get_current_page(response)\n",
    "        \n",
    "        if next_page and current_page < self.max_pages:\n",
    "            yield response.follow(next_page, self.parse)\n",
    "        \n",
    "        # Estrategia 3: Subcategor√≠as\n",
    "        subcategories = response.css('.subcategory-link::attr(href)').getall()\n",
    "        for subcat_url in subcategories[:3]:  # Limitar a 3 subcategor√≠as\n",
    "            yield response.follow(subcat_url, self.parse)\n",
    "    \n",
    "    def parse_product(self, response):\n",
    "        \"\"\"Extraer informaci√≥n detallada del producto\"\"\"\n",
    "        try:\n",
    "            # Datos b√°sicos\n",
    "            item = {\n",
    "                'url': response.url,\n",
    "                'name': response.css('h1::text').get(),\n",
    "                'price': self.extract_price(response),\n",
    "                'category': response.meta.get('category'),\n",
    "                'brand': response.css('.brand::text').get(),\n",
    "                'sku': response.css('[data-sku]::attr(data-sku)').get(),\n",
    "                'availability': response.css('.availability::text').get(),\n",
    "                'rating': self.extract_rating(response),\n",
    "                'reviews_count': self.extract_reviews_count(response),\n",
    "                'description': self.extract_description(response),\n",
    "                'images': response.css('.product-images img::attr(src)').getall(),\n",
    "                'specifications': self.extract_specifications(response),\n",
    "                'scraped_at': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            # Validaci√≥n b√°sica\n",
    "            if item['name'] and item['price']:\n",
    "                yield item\n",
    "            else:\n",
    "                self.logger.warning(f'Producto incompleto en {response.url}')\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f'Error parsing product {response.url}: {e}')\n",
    "    \n",
    "    def extract_category(self, response):\n",
    "        \"\"\"Extraer categor√≠a de la p√°gina\"\"\"\n",
    "        breadcrumb = response.css('.breadcrumb li:last-child::text').get()\n",
    "        if breadcrumb:\n",
    "            return breadcrumb.strip()\n",
    "        \n",
    "        # Fallback: extraer de URL\n",
    "        from urllib.parse import urlparse\n",
    "        path_parts = urlparse(response.url).path.split('/')\n",
    "        if 'categories' in path_parts:\n",
    "            idx = path_parts.index('categories')\n",
    "            if idx + 1 < len(path_parts):\n",
    "                return path_parts[idx + 1]\n",
    "        \n",
    "        return 'unknown'\n",
    "    \n",
    "    def extract_price(self, response):\n",
    "        \"\"\"Extraer precio con m√∫ltiples selectores\"\"\"\n",
    "        selectors = [\n",
    "            '.price::text',\n",
    "            '.current-price::text',\n",
    "            '[data-price]::attr(data-price)',\n",
    "            '.price-now::text'\n",
    "        ]\n",
    "        \n",
    "        for selector in selectors:\n",
    "            price = response.css(selector).get()\n",
    "            if price:\n",
    "                # Limpiar precio\n",
    "                import re\n",
    "                price_clean = re.sub(r'[^\\d.,]', '', price)\n",
    "                return price_clean\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def extract_rating(self, response):\n",
    "        \"\"\"Extraer rating del producto\"\"\"\n",
    "        # M√∫ltiples formatos de rating\n",
    "        rating_selectors = [\n",
    "            '.rating::attr(data-rating)',\n",
    "            '.stars::attr(data-stars)',\n",
    "            '.review-score::text'\n",
    "        ]\n",
    "        \n",
    "        for selector in rating_selectors:\n",
    "            rating = response.css(selector).get()\n",
    "            if rating:\n",
    "                try:\n",
    "                    return float(rating)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "        \n",
    "        # Contar estrellas visuales\n",
    "        filled_stars = response.css('.star.filled, .star.active').getall()\n",
    "        if filled_stars:\n",
    "            return len(filled_stars)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def extract_reviews_count(self, response):\n",
    "        \"\"\"Extraer n√∫mero de reviews\"\"\"\n",
    "        reviews_text = response.css('.reviews-count::text').get()\n",
    "        if reviews_text:\n",
    "            import re\n",
    "            match = re.search(r'(\\d+)', reviews_text)\n",
    "            if match:\n",
    "                return int(match.group(1))\n",
    "        return 0\n",
    "    \n",
    "    def extract_description(self, response):\n",
    "        \"\"\"Extraer descripci√≥n del producto\"\"\"\n",
    "        desc_selectors = [\n",
    "            '.product-description::text',\n",
    "            '.description p::text',\n",
    "            '.product-details::text'\n",
    "        ]\n",
    "        \n",
    "        for selector in desc_selectors:\n",
    "            desc_parts = response.css(selector).getall()\n",
    "            if desc_parts:\n",
    "                full_desc = ' '.join(desc_parts).strip()\n",
    "                return full_desc[:1000]  # Limitar longitud\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def extract_specifications(self, response):\n",
    "        \"\"\"Extraer especificaciones t√©cnicas\"\"\"\n",
    "        specs = {}\n",
    "        \n",
    "        # Tabla de especificaciones\n",
    "        spec_rows = response.css('.specifications tr')\n",
    "        for row in spec_rows:\n",
    "            label = row.css('td:first-child::text').get()\n",
    "            value = row.css('td:last-child::text').get()\n",
    "            if label and value:\n",
    "                specs[label.strip().lower().replace(' ', '_')] = value.strip()\n",
    "        \n",
    "        # Lista de caracter√≠sticas\n",
    "        features = response.css('.features li::text').getall()\n",
    "        if features:\n",
    "            specs['features'] = features\n",
    "        \n",
    "        return specs\n",
    "    \n",
    "    def get_current_page(self, response):\n",
    "        \"\"\"Obtener n√∫mero de p√°gina actual\"\"\"\n",
    "        # Extraer de URL\n",
    "        from urllib.parse import urlparse, parse_qs\n",
    "        query_params = parse_qs(urlparse(response.url).query)\n",
    "        page = query_params.get('page', ['1'])[0]\n",
    "        \n",
    "        try:\n",
    "            return int(page)\n",
    "        except ValueError:\n",
    "            return 1\n",
    "\n",
    "print(\"üèÜ MASTER SPIDER CREADO\")\n",
    "print(\"T√©cnicas integradas:\")\n",
    "print(\"  üï∑Ô∏è CrawlSpider avanzado\")\n",
    "print(\"  üîÑ Paginaci√≥n m√∫ltiple\")\n",
    "print(\"  üöÄ Procesamiento paralelo\")\n",
    "print(\"  üõ°Ô∏è Anti-detecci√≥n\")\n",
    "print(\"  üìä Monitoreo completo\")\n",
    "print(\"  üéØ Extracci√≥n robusta\")\n",
    "\n",
    "print(\"\\nüéØ COMANDOS DE EJECUCI√ìN:\")\n",
    "print(\"  scrapy crawl master_spider\")\n",
    "print(\"  scrapy crawl master_spider -a category=electronics\")\n",
    "print(\"  scrapy crawl master_spider -a max_pages=10\")\n",
    "print(\"  scrapy crawl master_spider -s LOG_LEVEL=DEBUG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ Resumen de la Lecci√≥n\n",
    "\n",
    "### üéØ T√©cnicas Dominadas\n",
    "\n",
    "1. **CrawlSpider Avanzado**:\n",
    "   - Seguimiento autom√°tico de enlaces con reglas\n",
    "   - LinkExtractor con filtros allow/deny\n",
    "   - Callbacks espec√≠ficos para diferentes tipos de p√°ginas\n",
    "   - Restricciones por CSS y XPath\n",
    "\n",
    "2. **Paginaci√≥n Compleja**:\n",
    "   - Enlaces \"Next\" tradicionales\n",
    "   - Paginaci√≥n num√©rica secuencial\n",
    "   - Infinite scroll con AJAX\n",
    "   - Manejo de errores 404\n",
    "\n",
    "3. **Procesamiento Paralelo**:\n",
    "   - Configuraci√≥n de concurrencia optimizada\n",
    "   - Sistema de prioridades para requests\n",
    "   - Auto-throttling inteligente\n",
    "   - M√©tricas de rendimiento en tiempo real\n",
    "\n",
    "4. **Middlewares Anti-Detecci√≥n**:\n",
    "   - Rotaci√≥n de User Agents\n",
    "   - Gesti√≥n inteligente de cookies\n",
    "   - Detecci√≥n autom√°tica de bloqueos\n",
    "   - Delays variables y headers realistas\n",
    "\n",
    "5. **Integraci√≥n con JavaScript**:\n",
    "   - Selenium middleware\n",
    "   - Esperas condicionales\n",
    "   - Interacciones con elementos din√°micos\n",
    "   - Manejo de SPAs\n",
    "\n",
    "6. **Monitoreo Avanzado**:\n",
    "   - M√©tricas detalladas en tiempo real\n",
    "   - Sistema de alertas autom√°tico\n",
    "   - Control de calidad de datos\n",
    "   - Reportes peri√≥dicos y finales\n",
    "\n",
    "### üöÄ Pr√≥xima Lecci√≥n: Procesamiento y Almacenamiento\n",
    "\n",
    "En la siguiente lecci√≥n aprenderemos:\n",
    "- Limpieza y validaci√≥n avanzada de datos\n",
    "- Integraci√≥n con bases de datos (SQL y NoSQL)\n",
    "- Pipelines de transformaci√≥n complejos\n",
    "- Export a m√∫ltiples formatos\n",
    "- Data warehousing y ETL\n",
    "\n",
    "### üí° Mejores Pr√°cticas Aprendidas\n",
    "\n",
    "1. **Dise√±o Escalable**: Usar configuraciones flexibles y parametrizables\n",
    "2. **Monitoreo Proactivo**: Implementar alertas y m√©tricas desde el inicio\n",
    "3. **Robustez**: Manejar errores gracefully y tener fallbacks\n",
    "4. **Eficiencia**: Optimizar concurrencia sin sobrecargar servidores\n",
    "5. **Mantenibilidad**: C√≥digo modular y bien documentado\n",
    "\n",
    "### üõ†Ô∏è Herramientas Master\n",
    "\n",
    "```python\n",
    "# Configuraci√≥n de alto rendimiento\n",
    "CONCURRENT_REQUESTS = 32\n",
    "AUTOTHROTTLE_ENABLED = True\n",
    "DOWNLOAD_DELAY = 0.25\n",
    "\n",
    "# Middlewares esenciales\n",
    "AntiDetectionMiddleware\n",
    "SmartCookieMiddleware\n",
    "MonitoringMiddleware\n",
    "\n",
    "# T√©cnicas de extracci√≥n\n",
    "multiple_selectors_fallback()\n",
    "intelligent_data_validation()\n",
    "robust_error_handling()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "¬°Incre√≠ble! üéâ Ahora eres un experto en spiders avanzados y puedes manejar los sitios web m√°s complejos con confianza."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}