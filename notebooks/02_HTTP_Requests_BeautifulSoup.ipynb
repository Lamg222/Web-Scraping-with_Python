{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåê Lecci√≥n 2: HTTP Requests y Beautiful Soup Avanzado\n",
    "\n",
    "## üéØ Objetivos de Aprendizaje\n",
    "\n",
    "Al finalizar esta lecci√≥n, ser√°s capaz de:\n",
    "- ‚úÖ Dominar el protocolo HTTP y sus m√©todos\n",
    "- ‚úÖ Manejar headers, cookies y sesiones avanzadas\n",
    "- ‚úÖ Trabajar con formularios y POST requests\n",
    "- ‚úÖ Implementar t√©cnicas avanzadas de Beautiful Soup\n",
    "- ‚úÖ Manejar diferentes tipos de contenido y encodings\n",
    "- ‚úÖ Crear scrapers robustos para sitios complejos\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Protocolo HTTP en Profundidad üîç\n",
    "\n",
    "HTTP (HyperText Transfer Protocol) es el fundamento de toda comunicaci√≥n web. Para hacer web scraping avanzado, necesitas entender c√≥mo funciona realmente.\n",
    "\n",
    "### üîÑ M√©todos HTTP Principales\n",
    "\n",
    "| M√©todo | Prop√≥sito | Idempotente | Cacheable | Tiene Body |\n",
    "|--------|-----------|-------------|-----------|------------|\n",
    "| **GET** | üì• Obtener datos | ‚úÖ | ‚úÖ | ‚ùå |\n",
    "| **POST** | üì§ Enviar datos | ‚ùå | ‚ùå | ‚úÖ |\n",
    "| **PUT** | üîÑ Actualizar/crear | ‚úÖ | ‚ùå | ‚úÖ |\n",
    "| **DELETE** | üóëÔ∏è Eliminar recurso | ‚úÖ | ‚ùå | ‚ùå |\n",
    "| **HEAD** | üìã Solo headers | ‚úÖ | ‚úÖ | ‚ùå |\n",
    "| **OPTIONS** | üîß Opciones disponibles | ‚úÖ | ‚úÖ | ‚ùå |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from pprint import pprint\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import time\n",
    "\n",
    "def explorar_metodos_http():\n",
    "    \"\"\"Demonstrar diferentes m√©todos HTTP\"\"\"\n",
    "    print(\"üåê EXPLORANDO M√âTODOS HTTP\\n\")\n",
    "    print(\"‚ïê\" * 60)\n",
    "    \n",
    "    # URL de ejemplo que acepta diferentes m√©todos\n",
    "    base_url = \"https://httpbin.org\"\n",
    "    \n",
    "    # 1. GET Request b√°sico\n",
    "    print(\"\\n1Ô∏è‚É£ GET Request:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    response = requests.get(f\"{base_url}/get\", params={'param1': 'valor1', 'param2': 'valor2'})\n",
    "    print(f\"Status Code: {response.status_code}\")\n",
    "    print(f\"URL final: {response.url}\")\n",
    "    \n",
    "    data = response.json()\n",
    "    print(f\"Par√°metros enviados: {data['args']}\")\n",
    "    print(f\"Headers enviados: {len(data['headers'])} headers\")\n",
    "    \n",
    "    # 2. POST Request con datos\n",
    "    print(\"\\n2Ô∏è‚É£ POST Request:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    post_data = {\n",
    "        'usuario': 'scraper_usuario',\n",
    "        'email': 'usuario@ejemplo.com',\n",
    "        'mensaje': 'Datos enviados via POST'\n",
    "    }\n",
    "    \n",
    "    response = requests.post(f\"{base_url}/post\", data=post_data)\n",
    "    print(f\"Status Code: {response.status_code}\")\n",
    "    \n",
    "    data = response.json()\n",
    "    print(f\"Datos enviados: {data['form']}\")\n",
    "    print(f\"Content-Type: {data['headers'].get('Content-Type')}\")\n",
    "    \n",
    "    # 3. HEAD Request (solo headers)\n",
    "    print(\"\\n3Ô∏è‚É£ HEAD Request:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    response = requests.head(\"https://httpbin.org/html\")\n",
    "    print(f\"Status Code: {response.status_code}\")\n",
    "    print(f\"Content-Type: {response.headers.get('content-type')}\")\n",
    "    print(f\"Content-Length: {response.headers.get('content-length')}\")\n",
    "    print(f\"Body length: {len(response.content)} (deber√≠a ser 0)\")\n",
    "    \n",
    "    # 4. PUT Request\n",
    "    print(\"\\n4Ô∏è‚É£ PUT Request:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    put_data = {'recurso': 'actualizado', 'version': '2.0'}\n",
    "    response = requests.put(f\"{base_url}/put\", json=put_data)\n",
    "    \n",
    "    data = response.json()\n",
    "    print(f\"Status Code: {response.status_code}\")\n",
    "    print(f\"JSON enviado: {data['json']}\")\n",
    "    \n",
    "    # 5. DELETE Request\n",
    "    print(\"\\n5Ô∏è‚É£ DELETE Request:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    response = requests.delete(f\"{base_url}/delete\")\n",
    "    print(f\"Status Code: {response.status_code}\")\n",
    "    print(f\"M√©todo confirmado: {response.json()['url']}\")\n",
    "    \n",
    "# Ejecutar demostraci√≥n\n",
    "explorar_metodos_http()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Headers HTTP: Tu Identidad en la Web üïµÔ∏è\n",
    "\n",
    "Los headers HTTP son metadatos que acompa√±an cada request y response. Son cruciales para un scraping exitoso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analizar_headers_http():\n",
    "    \"\"\"An√°lisis completo de headers HTTP\"\"\"\n",
    "    print(\"üïµÔ∏è AN√ÅLISIS COMPLETO DE HEADERS HTTP\\n\")\n",
    "    print(\"‚ïê\" * 70)\n",
    "    \n",
    "    # Headers b√°sicos vs avanzados\n",
    "    headers_basicos = {\n",
    "        'User-Agent': 'Python-requests/2.28.0'\n",
    "    }\n",
    "    \n",
    "    headers_avanzados = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "        'Accept-Language': 'es-ES,es;q=0.9,en;q=0.8',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'DNT': '1',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Upgrade-Insecure-Requests': '1',\n",
    "        'Cache-Control': 'max-age=0',\n",
    "        'Sec-Fetch-Dest': 'document',\n",
    "        'Sec-Fetch-Mode': 'navigate',\n",
    "        'Sec-Fetch-Site': 'none',\n",
    "        'Sec-Fetch-User': '?1'\n",
    "    }\n",
    "    \n",
    "    # Comparar respuestas\n",
    "    print(\"1Ô∏è‚É£ Comparaci√≥n: Headers B√°sicos vs Avanzados\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Request con headers b√°sicos\n",
    "    response_basico = requests.get('https://httpbin.org/headers', headers=headers_basicos)\n",
    "    print(\"\\nüì§ Headers enviados (b√°sicos):\")\n",
    "    for key, value in headers_basicos.items():\n",
    "        print(f\"   {key}: {value[:50]}{'...' if len(value) > 50 else ''}\")\n",
    "    \n",
    "    # Request con headers avanzados\n",
    "    response_avanzado = requests.get('https://httpbin.org/headers', headers=headers_avanzados)\n",
    "    print(\"\\nüì§ Headers enviados (avanzados):\")\n",
    "    for key, value in headers_avanzados.items():\n",
    "        print(f\"   {key}: {value[:50]}{'...' if len(value) > 50 else ''}\")\n",
    "    \n",
    "    # Analizar headers de respuesta\n",
    "    print(\"\\n\\n2Ô∏è‚É£ An√°lisis de Headers de Respuesta\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    response = requests.get('https://httpbin.org/response-headers?Content-Type=application/json&Custom-Header=valor-personalizado')\n",
    "    \n",
    "    print(\"üì• Headers recibidos importantes:\")\n",
    "    headers_importantes = [\n",
    "        'content-type', 'content-length', 'server', 'date', \n",
    "        'cache-control', 'set-cookie', 'location', 'custom-header'\n",
    "    ]\n",
    "    \n",
    "    for header in headers_importantes:\n",
    "        valor = response.headers.get(header, 'No presente')\n",
    "        print(f\"   {header.title()}: {valor}\")\n",
    "    \n",
    "    # Headers de seguridad\n",
    "    print(\"\\n\\n3Ô∏è‚É£ Headers de Seguridad Comunes\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    security_headers = {\n",
    "        'X-Frame-Options': 'Previene clickjacking',\n",
    "        'X-Content-Type-Options': 'Previene MIME sniffing',\n",
    "        'X-XSS-Protection': 'Protecci√≥n XSS',\n",
    "        'Strict-Transport-Security': 'Fuerza HTTPS',\n",
    "        'Content-Security-Policy': 'Control de recursos',\n",
    "        'Referrer-Policy': 'Control de referrer'\n",
    "    }\n",
    "    \n",
    "    # Probar un sitio real para ver headers de seguridad\n",
    "    try:\n",
    "        response = requests.head('https://github.com', timeout=5)\n",
    "        print(\"üîí Headers de seguridad en GitHub:\")\n",
    "        \n",
    "        for header, descripcion in security_headers.items():\n",
    "            valor = response.headers.get(header.lower(), 'No presente')\n",
    "            status = \"‚úÖ\" if valor != 'No presente' else \"‚ùå\"\n",
    "            print(f\"   {status} {header}: {valor[:50]}{'...' if len(str(valor)) > 50 else ''}\")\n",
    "            \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"‚ùå Error al acceder a GitHub: {e}\")\n",
    "    \n",
    "    # User-Agents m√°s comunes\n",
    "    print(\"\\n\\n4Ô∏è‚É£ User-Agents Recomendados para Scraping\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    user_agents = {\n",
    "        'Chrome Windows': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Chrome Mac': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Firefox Windows': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',\n",
    "        'Safari Mac': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',\n",
    "        'Edge Windows': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0'\n",
    "    }\n",
    "    \n",
    "    for navegador, ua in user_agents.items():\n",
    "        print(f\"\\nüåê {navegador}:\")\n",
    "        print(f\"   {ua}\")\n",
    "\n",
    "# Ejecutar an√°lisis\n",
    "analizar_headers_http()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sesiones y Cookies: Manteniendo el Estado üç™\n",
    "\n",
    "Las sesiones HTTP permiten mantener el estado entre m√∫ltiples requests. Son esenciales para sitios que requieren login o tienen comportamiento din√°mico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demostrar_sesiones_cookies():\n",
    "    \"\"\"Demostraci√≥n completa de sesiones y cookies\"\"\"\n",
    "    print(\"üç™ SESIONES Y COOKIES EN WEB SCRAPING\\n\")\n",
    "    print(\"‚ïê\" * 70)\n",
    "    \n",
    "    # 1. Sesiones b√°sicas\n",
    "    print(\"1Ô∏è‚É£ Creaci√≥n y Uso de Sesiones\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Crear una sesi√≥n\n",
    "    session = requests.Session()\n",
    "    \n",
    "    # Configurar headers por defecto para la sesi√≥n\n",
    "    session.headers.update({\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept-Language': 'es-ES,es;q=0.9'\n",
    "    })\n",
    "    \n",
    "    print(\"‚úÖ Sesi√≥n creada con headers por defecto\")\n",
    "    print(f\"üìã Headers de sesi√≥n: {len(session.headers)} configurados\")\n",
    "    \n",
    "    # 2. Manejo autom√°tico de cookies\n",
    "    print(\"\\n2Ô∏è‚É£ Manejo Autom√°tico de Cookies\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Primera request - establecer cookies\n",
    "    response = session.get('https://httpbin.org/cookies/set/session_id/abc123')\n",
    "    print(f\"‚úÖ Primera request completada: {response.status_code}\")\n",
    "    \n",
    "    # Ver cookies en la sesi√≥n\n",
    "    print(f\"üç™ Cookies almacenadas en sesi√≥n: {len(session.cookies)}\")\n",
    "    for cookie in session.cookies:\n",
    "        print(f\"   {cookie.name}: {cookie.value} (domain: {cookie.domain})\")\n",
    "    \n",
    "    # Segunda request - cookies se env√≠an autom√°ticamente\n",
    "    response = session.get('https://httpbin.org/cookies')\n",
    "    data = response.json()\n",
    "    print(f\"\\nüì§ Cookies enviadas autom√°ticamente: {data['cookies']}\")\n",
    "    \n",
    "    # 3. Manipulaci√≥n manual de cookies\n",
    "    print(\"\\n3Ô∏è‚É£ Manipulaci√≥n Manual de Cookies\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # A√±adir cookies manualmente\n",
    "    session.cookies.set('custom_cookie', 'valor_personalizado', domain='httpbin.org')\n",
    "    session.cookies.set('tracking_id', 'usuario_12345', domain='httpbin.org')\n",
    "    \n",
    "    # Verificar cookies\n",
    "    response = session.get('https://httpbin.org/cookies')\n",
    "    data = response.json()\n",
    "    print(f\"üç™ Todas las cookies: {data['cookies']}\")\n",
    "    \n",
    "    # 4. Persistencia de cookies\n",
    "    print(\"\\n4Ô∏è‚É£ Persistencia de Cookies\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    import pickle\n",
    "    import os\n",
    "    \n",
    "    # Guardar cookies\n",
    "    cookies_file = '../data/cookies_session.pkl'\n",
    "    os.makedirs(os.path.dirname(cookies_file), exist_ok=True)\n",
    "    \n",
    "    with open(cookies_file, 'wb') as f:\n",
    "        pickle.dump(session.cookies, f)\n",
    "    print(f\"üíæ Cookies guardadas en: {cookies_file}\")\n",
    "    \n",
    "    # Crear nueva sesi√≥n y cargar cookies\n",
    "    nueva_session = requests.Session()\n",
    "    \n",
    "    with open(cookies_file, 'rb') as f:\n",
    "        nueva_session.cookies.update(pickle.load(f))\n",
    "    \n",
    "    print(f\"üìÇ Cookies cargadas en nueva sesi√≥n: {len(nueva_session.cookies)}\")\n",
    "    \n",
    "    # Verificar que funcionan\n",
    "    response = nueva_session.get('https://httpbin.org/cookies')\n",
    "    data = response.json()\n",
    "    print(f\"‚úÖ Cookies persistentes funcionando: {len(data['cookies'])} cookies activas\")\n",
    "    \n",
    "    # 5. Cookies con expiraci√≥n\n",
    "    print(\"\\n5Ô∏è‚É£ An√°lisis de Cookies con Expiraci√≥n\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Obtener un sitio real con cookies complejas\n",
    "    try:\n",
    "        response = session.get('https://httpbin.org/cookies/set/expires_cookie/temporal', timeout=5)\n",
    "        \n",
    "        print(\"üïê Informaci√≥n detallada de cookies:\")\n",
    "        for cookie in session.cookies:\n",
    "            print(f\"\\n   üìã Cookie: {cookie.name}\")\n",
    "            print(f\"      Valor: {cookie.value}\")\n",
    "            print(f\"      Dominio: {cookie.domain}\")\n",
    "            print(f\"      Ruta: {cookie.path}\")\n",
    "            print(f\"      Segura: {cookie.secure}\")\n",
    "            print(f\"      HTTPOnly: {cookie.has_nonstandard_attr('HttpOnly')}\")\n",
    "            if cookie.expires:\n",
    "                import datetime\n",
    "                expiry = datetime.datetime.fromtimestamp(cookie.expires)\n",
    "                print(f\"      Expira: {expiry}\")\n",
    "            else:\n",
    "                print(f\"      Expira: Sesi√≥n (no persistente)\")\n",
    "                \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"‚ùå Error al obtener cookies detalladas: {e}\")\n",
    "    \n",
    "    return session\n",
    "\n",
    "# Ejecutar demostraci√≥n\n",
    "mi_session = demostrar_sesiones_cookies()\n",
    "print(f\"\\nüéâ Sesi√≥n configurada y lista para usar con {len(mi_session.cookies)} cookies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Formularios y POST Requests üìù\n",
    "\n",
    "Muchos sitios web requieren interactuar con formularios para acceder al contenido. Aprender a manejar formularios es crucial para el scraping avanzado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "\n",
    "def manejar_formularios():\n",
    "    \"\"\"Demostraci√≥n completa de manejo de formularios\"\"\"\n",
    "    print(\"üìù MANEJO AVANZADO DE FORMULARIOS\\n\")\n",
    "    print(\"‚ïê\" * 70)\n",
    "    \n",
    "    # 1. An√°lisis de formularios HTML\n",
    "    print(\"1Ô∏è‚É£ An√°lisis de Formularios HTML\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # HTML de formulario complejo\n",
    "    formulario_html = \"\"\"\n",
    "    <form method=\"post\" action=\"/login\" enctype=\"application/x-www-form-urlencoded\">\n",
    "        <input type=\"hidden\" name=\"csrf_token\" value=\"abc123xyz789\" />\n",
    "        <input type=\"hidden\" name=\"form_id\" value=\"login_form\" />\n",
    "        \n",
    "        <label for=\"username\">Usuario:</label>\n",
    "        <input type=\"text\" name=\"username\" id=\"username\" required />\n",
    "        \n",
    "        <label for=\"password\">Contrase√±a:</label>\n",
    "        <input type=\"password\" name=\"password\" id=\"password\" required />\n",
    "        \n",
    "        <label for=\"remember\">Recordarme:</label>\n",
    "        <input type=\"checkbox\" name=\"remember\" id=\"remember\" value=\"1\" />\n",
    "        \n",
    "        <label for=\"role\">Rol:</label>\n",
    "        <select name=\"role\" id=\"role\">\n",
    "            <option value=\"user\" selected>Usuario</option>\n",
    "            <option value=\"admin\">Administrador</option>\n",
    "        </select>\n",
    "        \n",
    "        <button type=\"submit\">Iniciar Sesi√≥n</button>\n",
    "    </form>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Parsear formulario\n",
    "    soup = BeautifulSoup(formulario_html, 'html.parser')\n",
    "    form = soup.find('form')\n",
    "    \n",
    "    print(f\"üìã Formulario encontrado:\")\n",
    "    print(f\"   M√©todo: {form.get('method', 'GET').upper()}\")\n",
    "    print(f\"   Acci√≥n: {form.get('action', 'misma p√°gina')}\")\n",
    "    print(f\"   Encoding: {form.get('enctype', 'application/x-www-form-urlencoded')}\")\n",
    "    \n",
    "    # Extraer todos los campos\n",
    "    campos = form.find_all(['input', 'select', 'textarea'])\n",
    "    print(f\"\\nüîç Campos encontrados: {len(campos)}\")\n",
    "    \n",
    "    form_data = {}\n",
    "    for campo in campos:\n",
    "        name = campo.get('name')\n",
    "        if not name:\n",
    "            continue\n",
    "            \n",
    "        tipo = campo.get('type', campo.name)\n",
    "        valor = campo.get('value', '')\n",
    "        required = '‚úÖ' if campo.has_attr('required') else '‚ùå'\n",
    "        \n",
    "        print(f\"   üìù {name} ({tipo}): '{valor}' | Requerido: {required}\")\n",
    "        \n",
    "        # Configurar valores por defecto\n",
    "        if tipo == 'hidden':\n",
    "            form_data[name] = valor\n",
    "        elif tipo == 'checkbox' and not valor:\n",
    "            form_data[name] = '1' if campo.has_attr('checked') else ''\n",
    "        elif tipo == 'select':\n",
    "            selected_option = campo.find('option', selected=True)\n",
    "            form_data[name] = selected_option.get('value') if selected_option else ''\n",
    "        else:\n",
    "            form_data[name] = valor\n",
    "    \n",
    "    # 2. Simulaci√≥n de env√≠o de formulario\n",
    "    print(\"\\n2Ô∏è‚É£ Simulaci√≥n de Env√≠o de Formulario\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Completar formulario\n",
    "    form_data.update({\n",
    "        'username': 'usuario_scraper',\n",
    "        'password': 'password123',\n",
    "        'remember': '1',\n",
    "        'role': 'user'\n",
    "    })\n",
    "    \n",
    "    print(\"üì§ Datos del formulario a enviar:\")\n",
    "    for key, value in form_data.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "    \n",
    "    # Simular POST request\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Content-Type': 'application/x-www-form-urlencoded',\n",
    "        'Referer': 'https://httpbin.org/forms/post'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = session.post('https://httpbin.org/post', data=form_data, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(f\"\\n‚úÖ Formulario enviado exitosamente: {response.status_code}\")\n",
    "            \n",
    "            data = response.json()\n",
    "            print(f\"üì® Datos recibidos por el servidor: {data['form']}\")\n",
    "            print(f\"üìã Headers enviados: {len(data['headers'])}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Error al enviar formulario: {response.status_code}\")\n",
    "            \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"‚ùå Error de conexi√≥n: {e}\")\n",
    "    \n",
    "    # 3. Manejo de diferentes tipos de encoding\n",
    "    print(\"\\n3Ô∏è‚É£ Diferentes Tipos de Encoding\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # application/x-www-form-urlencoded (por defecto)\n",
    "    print(\"\\nüìù Form URL Encoded:\")\n",
    "    response = session.post('https://httpbin.org/post', data=form_data)\n",
    "    data = response.json()\n",
    "    print(f\"   Content-Type enviado: {data['headers'].get('Content-Type')}\")\n",
    "    print(f\"   Datos: {data['form']}\")\n",
    "    \n",
    "    # multipart/form-data (para archivos)\n",
    "    print(\"\\nüìÅ Multipart Form Data:\")\n",
    "    files_data = {\n",
    "        'username': 'usuario_archivo',\n",
    "        'file': ('test.txt', 'Contenido del archivo de prueba', 'text/plain')\n",
    "    }\n",
    "    \n",
    "    response = session.post('https://httpbin.org/post', files=files_data)\n",
    "    data = response.json()\n",
    "    print(f\"   Content-Type enviado: {data['headers'].get('Content-Type')[:50]}...\")\n",
    "    print(f\"   Archivos: {data['files']}\")\n",
    "    print(f\"   Form data: {data['form']}\")\n",
    "    \n",
    "    # application/json\n",
    "    print(\"\\nüìä JSON Data:\")\n",
    "    json_data = {'username': 'usuario_json', 'preferences': {'theme': 'dark', 'lang': 'es'}}\n",
    "    response = session.post('https://httpbin.org/post', json=json_data)\n",
    "    data = response.json()\n",
    "    print(f\"   Content-Type enviado: {data['headers'].get('Content-Type')}\")\n",
    "    print(f\"   JSON: {data['json']}\")\n",
    "\n",
    "# Ejecutar demostraci√≥n\n",
    "manejar_formularios()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Beautiful Soup Avanzado: T√©cnicas Profesionales üç≤\n",
    "\n",
    "Beautiful Soup tiene caracter√≠sticas avanzadas que pueden hacer tu scraping mucho m√°s eficiente y robusto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "from bs4.builder import XMLParsedAsHTMLWarning\n",
    "import warnings\n",
    "\n",
    "# Suprimir warnings de Beautiful Soup para output m√°s limpio\n",
    "warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)\n",
    "\n",
    "def beautiful_soup_avanzado():\n",
    "    \"\"\"T√©cnicas avanzadas de Beautiful Soup\"\"\"\n",
    "    print(\"üç≤ BEAUTIFUL SOUP: T√âCNICAS AVANZADAS\\n\")\n",
    "    print(\"‚ïê\" * 70)\n",
    "    \n",
    "    # HTML complejo para demostraci√≥n\n",
    "    html_complejo = \"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html lang=\"es\">\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <title>P√°gina Compleja de Ejemplo</title>\n",
    "        <script type=\"application/ld+json\">\n",
    "        {\n",
    "            \"@context\": \"http://schema.org\",\n",
    "            \"@type\": \"Article\",\n",
    "            \"headline\": \"Noticia Principal\",\n",
    "            \"author\": \"Juan P√©rez\",\n",
    "            \"datePublished\": \"2024-01-15\"\n",
    "        }\n",
    "        </script>\n",
    "    </head>\n",
    "    <body>\n",
    "        <div class=\"container main-content\" data-section=\"primary\">\n",
    "            <article id=\"article-123\" class=\"post featured\" data-id=\"123\" data-category=\"tech\">\n",
    "                <header>\n",
    "                    <h1>T√≠tulo Principal con <em>√©nfasis</em> y <strong>importancia</strong></h1>\n",
    "                    <time datetime=\"2024-01-15T10:30:00Z\">15 de Enero, 2024</time>\n",
    "                    <div class=\"tags\">\n",
    "                        <span class=\"tag tech\">Tecnolog√≠a</span>\n",
    "                        <span class=\"tag ai\">Inteligencia Artificial</span>\n",
    "                        <span class=\"tag python\">Python</span>\n",
    "                    </div>\n",
    "                </header>\n",
    "                \n",
    "                <div class=\"content\">\n",
    "                    <p class=\"intro\">Este es el p√°rrafo introductorio con <a href=\"#link1\">enlace interno</a>.</p>\n",
    "                    <p>P√°rrafo normal con texto y <code>c√≥digo inline</code>.</p>\n",
    "                    \n",
    "                    <!-- Lista con diferentes tipos -->\n",
    "                    <ul class=\"lista-items\">\n",
    "                        <li data-priority=\"high\">Item importante</li>\n",
    "                        <li data-priority=\"medium\">Item normal</li>\n",
    "                        <li data-priority=\"low\">Item de baja prioridad</li>\n",
    "                    </ul>\n",
    "                    \n",
    "                    <!-- Tabla de datos -->\n",
    "                    <table class=\"data-table\">\n",
    "                        <thead>\n",
    "                            <tr>\n",
    "                                <th>Producto</th>\n",
    "                                <th>Precio</th>\n",
    "                                <th data-sortable=\"true\">Rating</th>\n",
    "                            </tr>\n",
    "                        </thead>\n",
    "                        <tbody>\n",
    "                            <tr data-product-id=\"1\">\n",
    "                                <td>Producto A</td>\n",
    "                                <td class=\"price\">$99.99</td>\n",
    "                                <td class=\"rating\">4.5</td>\n",
    "                            </tr>\n",
    "                            <tr data-product-id=\"2\">\n",
    "                                <td>Producto B</td>\n",
    "                                <td class=\"price\">$149.99</td>\n",
    "                                <td class=\"rating\">4.8</td>\n",
    "                            </tr>\n",
    "                        </tbody>\n",
    "                    </table>\n",
    "                </div>\n",
    "                \n",
    "                <footer class=\"article-footer\">\n",
    "                    <div class=\"author\">Por: <span class=\"author-name\">Mar√≠a Gonz√°lez</span></div>\n",
    "                    <div class=\"social-sharing\">\n",
    "                        <a href=\"#\" class=\"social-link\" data-platform=\"twitter\">Twitter</a>\n",
    "                        <a href=\"#\" class=\"social-link\" data-platform=\"facebook\">Facebook</a>\n",
    "                    </div>\n",
    "                </footer>\n",
    "            </article>\n",
    "            \n",
    "            <!-- Comentarios -->\n",
    "            <section id=\"comments\" class=\"comments-section\">\n",
    "                <h2>Comentarios (3)</h2>\n",
    "                <div class=\"comment\" data-comment-id=\"1\">\n",
    "                    <strong>Usuario1</strong>: Excelente art√≠culo!\n",
    "                    <time>hace 2 horas</time>\n",
    "                </div>\n",
    "                <div class=\"comment\" data-comment-id=\"2\">\n",
    "                    <strong>Usuario2</strong>: Muy informativo, gracias.\n",
    "                    <time>hace 1 hora</time>\n",
    "                </div>\n",
    "            </section>\n",
    "        </div>\n",
    "        \n",
    "        <!-- Sidebar -->\n",
    "        <aside class=\"sidebar\">\n",
    "            <div class=\"widget recent-posts\">\n",
    "                <h3>Posts Recientes</h3>\n",
    "                <ul>\n",
    "                    <li><a href=\"/post/1\">Post 1</a></li>\n",
    "                    <li><a href=\"/post/2\">Post 2</a></li>\n",
    "                </ul>\n",
    "            </div>\n",
    "        </aside>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Crear soup\n",
    "    soup = BeautifulSoup(html_complejo, 'html.parser')\n",
    "    \n",
    "    # 1. B√∫squedas con funciones personalizadas\n",
    "    print(\"1Ô∏è‚É£ B√∫squedas con Funciones Personalizadas\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    def tiene_precio_alto(tag):\n",
    "        \"\"\"Funci√≥n personalizada para encontrar precios altos\"\"\"\n",
    "        if tag.name != 'td' or not tag.has_attr('class'):\n",
    "            return False\n",
    "        if 'price' not in tag.get('class'):\n",
    "            return False\n",
    "        try:\n",
    "            precio = float(tag.text.replace('$', ''))\n",
    "            return precio > 100\n",
    "        except ValueError:\n",
    "            return False\n",
    "    \n",
    "    precios_altos = soup.find_all(tiene_precio_alto)\n",
    "    print(f\"üí∞ Precios altos encontrados: {len(precios_altos)}\")\n",
    "    for precio in precios_altos:\n",
    "        producto = precio.parent.find('td').text\n",
    "        print(f\"   üì¶ {producto}: {precio.text}\")\n",
    "    \n",
    "    # 2. Navegaci√≥n compleja del √°rbol\n",
    "    print(\"\\n2Ô∏è‚É£ Navegaci√≥n Compleja del √Årbol\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Encontrar elemento y navegar por sus relaciones\n",
    "    article = soup.find('article', id='article-123')\n",
    "    \n",
    "    print(f\"üìù Art√≠culo encontrado: ID {article.get('id')}\")\n",
    "    print(f\"   üë®‚Äçüëß‚Äçüë¶ Padre: {article.parent.name} (clase: {article.parent.get('class')})\")\n",
    "    print(f\"   üë∂ Hijos directos: {len([child for child in article.children if isinstance(child, Tag)])}\")\n",
    "    \n",
    "    # Hermanos\n",
    "    siguiente_hermano = article.find_next_sibling()\n",
    "    if siguiente_hermano:\n",
    "        print(f\"   üë´ Siguiente hermano: {siguiente_hermano.name} (ID: {siguiente_hermano.get('id')})\")\n",
    "    \n",
    "    # Ancestros espec√≠ficos\n",
    "    container = article.find_parent('div', class_='container')\n",
    "    print(f\"   üë¥ Container padre: {container.get('class')} (data-section: {container.get('data-section')})\")\n",
    "    \n",
    "    # 3. Extracci√≥n de texto avanzada\n",
    "    print(\"\\n3Ô∏è‚É£ Extracci√≥n de Texto Avanzada\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    titulo = soup.find('h1')\n",
    "    \n",
    "    # Diferentes formas de obtener texto\n",
    "    print(f\"üìù Texto completo: {titulo.get_text()}\")\n",
    "    print(f\"üìù Texto con separador: {titulo.get_text(separator=' | ', strip=True)}\")\n",
    "    print(f\"üìù Solo strings navegables: {[s.strip() for s in titulo.strings if s.strip()]}\")\n",
    "    \n",
    "    # Texto de elementos espec√≠ficos\n",
    "    enfasis = titulo.find('em')\n",
    "    importante = titulo.find('strong')\n",
    "    print(f\"üìù Texto con √©nfasis: {enfasis.text if enfasis else 'No encontrado'}\")\n",
    "    print(f\"üìù Texto importante: {importante.text if importante else 'No encontrado'}\")\n",
    "    \n",
    "    # 4. Expresiones regulares avanzadas\n",
    "    print(\"\\n4Ô∏è‚É£ B√∫squedas con Expresiones Regulares\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Buscar por texto que contenga patrones\n",
    "    comentarios_recientes = soup.find_all(string=re.compile(r'hace \\d+ hora'))\n",
    "    print(f\"üïê Comentarios recientes: {len(comentarios_recientes)}\")\n",
    "    for comentario in comentarios_recientes:\n",
    "        print(f\"   üí¨ {comentario.strip()}\")\n",
    "    \n",
    "    # Buscar atributos con patrones\n",
    "    elementos_data = soup.find_all(attrs={'data-comment-id': re.compile(r'\\d+')})\n",
    "    print(f\"\\nüè∑Ô∏è Elementos con data-comment-id num√©rico: {len(elementos_data)}\")\n",
    "    for elem in elementos_data:\n",
    "        id_comentario = elem.get('data-comment-id')\n",
    "        usuario = elem.find('strong').text if elem.find('strong') else 'Usuario desconocido'\n",
    "        print(f\"   #{id_comentario}: {usuario}\")\n",
    "    \n",
    "    # Buscar clases con patrones\n",
    "    tags = soup.find_all('span', class_=re.compile(r'tag \\w+'))\n",
    "    print(f\"\\nüè∑Ô∏è Tags encontrados: {len(tags)}\")\n",
    "    for tag in tags:\n",
    "        clases = ' '.join(tag.get('class'))\n",
    "        print(f\"   üè∑Ô∏è {tag.text}: {clases}\")\n",
    "    \n",
    "    # 5. Modificaci√≥n del DOM\n",
    "    print(\"\\n5Ô∏è‚É£ Modificaci√≥n del DOM\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Crear copia para modificar\n",
    "    soup_copia = BeautifulSoup(str(soup), 'html.parser')\n",
    "    \n",
    "    # A√±adir elemento\n",
    "    nuevo_comentario = soup_copia.new_tag('div', **{'class': 'comment', 'data-comment-id': '3'})\n",
    "    nuevo_comentario.string = 'Comentario a√±adido por scraper'\n",
    "    \n",
    "    seccion_comentarios = soup_copia.find('section', id='comments')\n",
    "    seccion_comentarios.append(nuevo_comentario)\n",
    "    \n",
    "    print(f\"‚ûï Comentario a√±adido. Total comentarios: {len(soup_copia.find_all('div', class_='comment'))}\")\n",
    "    \n",
    "    # Modificar atributo\n",
    "    article_copia = soup_copia.find('article')\n",
    "    article_copia['data-processed'] = 'true'\n",
    "    article_copia['data-scraped-at'] = '2024-01-15'\n",
    "    \n",
    "    print(f\"üîÑ Atributos modificados: {article_copia.get('data-processed')} | {article_copia.get('data-scraped-at')}\")\n",
    "    \n",
    "    # Eliminar elementos\n",
    "    scripts = soup_copia.find_all('script')\n",
    "    print(f\"üóëÔ∏è Scripts encontrados para eliminar: {len(scripts)}\")\n",
    "    for script in scripts:\n",
    "        script.decompose()\n",
    "    \n",
    "    scripts_despues = soup_copia.find_all('script')\n",
    "    print(f\"‚úÖ Scripts despu√©s de eliminar: {len(scripts_despues)}\")\n",
    "    \n",
    "    # 6. Extracci√≥n de JSON-LD (datos estructurados)\n",
    "    print(\"\\n6Ô∏è‚É£ Extracci√≥n de Datos Estructurados JSON-LD\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    import json\n",
    "    \n",
    "    # Buscar scripts con datos estructurados\n",
    "    json_scripts = soup.find_all('script', type='application/ld+json')\n",
    "    print(f\"üìä Scripts JSON-LD encontrados: {len(json_scripts)}\")\n",
    "    \n",
    "    for i, script in enumerate(json_scripts, 1):\n",
    "        try:\n",
    "            data = json.loads(script.string)\n",
    "            print(f\"\\n   üìÑ JSON-LD #{i}:\")\n",
    "            print(f\"      Tipo: {data.get('@type')}\")\n",
    "            print(f\"      T√≠tulo: {data.get('headline')}\")\n",
    "            print(f\"      Autor: {data.get('author')}\")\n",
    "            print(f\"      Fecha: {data.get('datePublished')}\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"   ‚ùå Error parsing JSON-LD: {e}\")\n",
    "\n",
    "# Ejecutar demostraci√≥n\n",
    "beautiful_soup_avanzado()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Manejo de Encodings y Contenido Internacional üåç\n",
    "\n",
    "Los problemas de encoding son comunes en web scraping, especialmente con contenido en diferentes idiomas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def manejar_encodings():\n",
    "    \"\"\"Manejo avanzado de encodings y contenido internacional\"\"\"\n",
    "    print(\"üåç MANEJO DE ENCODINGS Y CONTENIDO INTERNACIONAL\\n\")\n",
    "    print(\"‚ïê\" * 70)\n",
    "    \n",
    "    # 1. Detecci√≥n autom√°tica de encoding\n",
    "    print(\"1Ô∏è‚É£ Detecci√≥n Autom√°tica de Encoding\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Simular contenido con diferentes encodings\n",
    "    textos_ejemplo = {\n",
    "        'utf-8': 'Hola mundo! Caf√©, ni√±o, coraz√≥n üíñ',\n",
    "        'latin-1': 'Hola mundo! Caf\\xe9, ni\\xf1o, coraz\\xf3n',\n",
    "        'cp1252': 'Hola mundo! Smart quotes: \\u201cHello\\u201d',\n",
    "        'ascii': 'Hello world! Basic ASCII text only'\n",
    "    }\n",
    "    \n",
    "    for encoding, texto in textos_ejemplo.items():\n",
    "        try:\n",
    "            # Simular bytes con encoding espec√≠fico\n",
    "            if encoding == 'latin-1':\n",
    "                # Contenido que parece latin-1\n",
    "                texto_limpio = 'Hola mundo! Caf√©, ni√±o, coraz√≥n'\n",
    "                bytes_contenido = texto_limpio.encode('latin-1')\n",
    "            elif encoding == 'cp1252':\n",
    "                texto_limpio = 'Hola mundo! Smart quotes: \"Hello\"'\n",
    "                bytes_contenido = texto_limpio.encode('cp1252')\n",
    "            else:\n",
    "                bytes_contenido = texto.encode(encoding)\n",
    "            \n",
    "            # Detectar encoding\n",
    "            detected = chardet.detect(bytes_contenido)\n",
    "            \n",
    "            print(f\"\\nüìù Encoding {encoding}:\")\n",
    "            print(f\"   Detectado: {detected['encoding']} (confianza: {detected['confidence']:.2f})\")\n",
    "            print(f\"   Texto original: {texto}\")\n",
    "            \n",
    "            # Decodificar correctamente\n",
    "            if detected['encoding']:\n",
    "                texto_decodificado = bytes_contenido.decode(detected['encoding'])\n",
    "                print(f\"   Decodificado: {texto_decodificado}\")\n",
    "            \n",
    "        except UnicodeError as e:\n",
    "            print(f\"   ‚ùå Error de Unicode: {e}\")\n",
    "    \n",
    "    # 2. Funci√≥n robusta para detecci√≥n de encoding\n",
    "    print(\"\\n\\n2Ô∏è‚É£ Funci√≥n Robusta de Detecci√≥n\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    def detectar_encoding_robusto(response):\n",
    "        \"\"\"Detecta encoding de forma robusta\"\"\"\n",
    "        \n",
    "        # 1. Intentar con el encoding del header\n",
    "        content_type = response.headers.get('content-type', '')\n",
    "        if 'charset=' in content_type:\n",
    "            charset = content_type.split('charset=')[-1].split(';')[0].strip()\n",
    "            print(f\"   üìã Encoding del header: {charset}\")\n",
    "            try:\n",
    "                test_decode = response.content[:100].decode(charset)\n",
    "                return charset, 'header'\n",
    "            except UnicodeDecodeError:\n",
    "                print(f\"   ‚ö†Ô∏è Header encoding {charset} fall√≥\")\n",
    "        \n",
    "        # 2. Buscar en meta tags\n",
    "        try:\n",
    "            # Parsear solo el head con encoding aproximado\n",
    "            head_content = response.content[:2000]  # Primeros 2KB\n",
    "            soup_temp = BeautifulSoup(head_content, 'html.parser')\n",
    "            \n",
    "            # Buscar meta charset\n",
    "            meta_charset = soup_temp.find('meta', charset=True)\n",
    "            if meta_charset:\n",
    "                charset = meta_charset.get('charset')\n",
    "                print(f\"   üìã Meta charset: {charset}\")\n",
    "                try:\n",
    "                    test_decode = response.content[:100].decode(charset)\n",
    "                    return charset, 'meta_charset'\n",
    "                except UnicodeDecodeError:\n",
    "                    pass\n",
    "            \n",
    "            # Buscar meta http-equiv\n",
    "            meta_equiv = soup_temp.find('meta', attrs={'http-equiv': 'Content-Type'})\n",
    "            if meta_equiv and meta_equiv.get('content'):\n",
    "                content = meta_equiv.get('content')\n",
    "                if 'charset=' in content:\n",
    "                    charset = content.split('charset=')[-1].strip()\n",
    "                    print(f\"   üìã Meta http-equiv: {charset}\")\n",
    "                    try:\n",
    "                        test_decode = response.content[:100].decode(charset)\n",
    "                        return charset, 'meta_equiv'\n",
    "                    except UnicodeDecodeError:\n",
    "                        pass\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Error parseando meta tags: {e}\")\n",
    "        \n",
    "        # 3. Usar chardet como √∫ltimo recurso\n",
    "        detected = chardet.detect(response.content[:10000])  # Primeros 10KB\n",
    "        if detected['encoding'] and detected['confidence'] > 0.7:\n",
    "            print(f\"   üîç Chardet detect√≥: {detected['encoding']} (confianza: {detected['confidence']:.2f})\")\n",
    "            return detected['encoding'], 'chardet'\n",
    "        \n",
    "        # 4. UTF-8 como fallback\n",
    "        print(f\"   üîÑ Usando UTF-8 como fallback\")\n",
    "        return 'utf-8', 'fallback'\n",
    "    \n",
    "    # Probar con diferentes sitios\n",
    "    sitios_prueba = [\n",
    "        'https://httpbin.org/html',\n",
    "        'https://httpbin.org/encoding/utf8'\n",
    "    ]\n",
    "    \n",
    "    session = requests.Session()\n",
    "    \n",
    "    for url in sitios_prueba:\n",
    "        try:\n",
    "            print(f\"\\nüåê Probando: {url}\")\n",
    "            response = session.get(url, timeout=5)\n",
    "            \n",
    "            encoding, metodo = detectar_encoding_robusto(response)\n",
    "            print(f\"   ‚úÖ Encoding final: {encoding} (m√©todo: {metodo})\")\n",
    "            \n",
    "            # Decodificar contenido\n",
    "            try:\n",
    "                if encoding != response.encoding:\n",
    "                    response.encoding = encoding\n",
    "                \n",
    "                contenido = response.text[:100]\n",
    "                print(f\"   üìù Contenido (primeros 100 chars): {contenido[:50]}...\")\n",
    "                \n",
    "            except UnicodeDecodeError as e:\n",
    "                print(f\"   ‚ùå Error decodificando: {e}\")\n",
    "                \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"   ‚ùå Error de conexi√≥n: {e}\")\n",
    "    \n",
    "    # 3. Manejo de caracteres especiales\n",
    "    print(\"\\n\\n3Ô∏è‚É£ Manejo de Caracteres Especiales\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # HTML con caracteres especiales\n",
    "    html_internacional = \"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head><meta charset=\"UTF-8\"></head>\n",
    "    <body>\n",
    "        <h1>Texto Internacional</h1>\n",
    "        <p>Espa√±ol: ni√±o, coraz√≥n, √±o√±o</p>\n",
    "        <p>Franc√©s: caf√©, r√©sum√©, na√Øve</p>\n",
    "        <p>Alem√°n: M√ºller, Gr√∂√üe, wei√ü</p>\n",
    "        <p>Japon√©s: „Åì„Çì„Å´„Å°„ÅØ (Konnichiwa)</p>\n",
    "        <p>Emojis: üåü üíñ üöÄ üçï</p>\n",
    "        <p>S√≠mbolos: ¬© ¬Æ ‚Ñ¢ ‚Ç¨ ¬£ ¬•</p>\n",
    "        <p>Entidades HTML: &lt;tag&gt; &amp; &quot;quote&quot;</p>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    soup = BeautifulSoup(html_internacional, 'html.parser')\n",
    "    \n",
    "    print(\"üåç Extracci√≥n de texto internacional:\")\n",
    "    parrafos = soup.find_all('p')\n",
    "    \n",
    "    for i, p in enumerate(parrafos, 1):\n",
    "        texto = p.get_text()\n",
    "        print(f\"   {i}. {texto}\")\n",
    "        \n",
    "        # Mostrar informaci√≥n de encoding\n",
    "        try:\n",
    "            bytes_utf8 = texto.encode('utf-8')\n",
    "            print(f\"      UTF-8 bytes: {len(bytes_utf8)} bytes\")\n",
    "        except UnicodeEncodeError as e:\n",
    "            print(f\"      ‚ùå Error encoding UTF-8: {e}\")\n",
    "    \n",
    "    # 4. Limpieza de texto\n",
    "    print(\"\\n\\n4Ô∏è‚É£ Limpieza de Texto Avanzada\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    import unicodedata\n",
    "    \n",
    "    def limpiar_texto_avanzado(texto):\n",
    "        \"\"\"Limpia texto de forma avanzada\"\"\"\n",
    "        \n",
    "        # 1. Normalizar Unicode\n",
    "        texto = unicodedata.normalize('NFKD', texto)\n",
    "        \n",
    "        # 2. Limpiar espacios en blanco\n",
    "        texto = ' '.join(texto.split())\n",
    "        \n",
    "        # 3. Remover caracteres de control\n",
    "        texto = ''.join(char for char in texto if unicodedata.category(char)[0] != 'C' or char in '\\n\\t\\r')\n",
    "        \n",
    "        return texto.strip()\n",
    "    \n",
    "    # Texto problem√°tico para limpiar\n",
    "    texto_sucio = \"\\n\\n  Texto    con    espacios\\t\\t\\textra   \\n  y caracteres\\x00 de control\\x01  \\n\\n\"\n",
    "    \n",
    "    print(f\"üìù Texto original: {repr(texto_sucio)}\")\n",
    "    texto_limpio = limpiar_texto_avanzado(texto_sucio)\n",
    "    print(f\"‚ú® Texto limpio: {repr(texto_limpio)}\")\n",
    "\n",
    "# Instalar chardet si no est√° disponible\n",
    "try:\n",
    "    import chardet\n",
    "    manejar_encodings()\n",
    "except ImportError:\n",
    "    print(\"üì¶ Instalando chardet...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'chardet'])\n",
    "    import chardet\n",
    "    manejar_encodings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Paginaci√≥n y Crawling Avanzado üìÑ\n",
    "\n",
    "Muchos sitios web dividen el contenido en p√°ginas. Aprender a manejar paginaci√≥n es crucial para extraer datasets completos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from urllib.parse import urljoin, urlparse, parse_qs\n",
    "\n",
    "def manejar_paginacion():\n",
    "    \"\"\"T√©cnicas avanzadas para manejar paginaci√≥n\"\"\"\n",
    "    print(\"üìÑ MANEJO AVANZADO DE PAGINACI√ìN\\n\")\n",
    "    print(\"‚ïê\" * 70)\n",
    "    \n",
    "    # 1. Paginaci√≥n b√°sica con quotes.toscrape.com\n",
    "    print(\"1Ô∏è‚É£ Paginaci√≥n B√°sica\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    def scrape_quotes_paginado():\n",
    "        \"\"\"Scraper con paginaci√≥n autom√°tica\"\"\"\n",
    "        base_url = \"http://quotes.toscrape.com\"\n",
    "        session = requests.Session()\n",
    "        session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        })\n",
    "        \n",
    "        todas_citas = []\n",
    "        pagina = 1\n",
    "        max_paginas = 3  # Limitar para demostraci√≥n\n",
    "        \n",
    "        while pagina <= max_paginas:\n",
    "            url = f\"{base_url}/page/{pagina}/\"\n",
    "            print(f\"\\nüìñ Procesando p√°gina {pagina}: {url}\")\n",
    "            \n",
    "            try:\n",
    "                response = session.get(url, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "                # Extraer citas de esta p√°gina\n",
    "                quotes = soup.find_all('div', class_='quote')\n",
    "                \n",
    "                if not quotes:\n",
    "                    print(f\"   ‚ùå No se encontraron citas en p√°gina {pagina}. Terminando.\")\n",
    "                    break\n",
    "                \n",
    "                print(f\"   ‚úÖ Encontradas {len(quotes)} citas\")\n",
    "                \n",
    "                for quote in quotes:\n",
    "                    texto = quote.find('span', class_='text').text\n",
    "                    autor = quote.find('small', class_='author').text\n",
    "                    tags = [tag.text for tag in quote.find_all('a', class_='tag')]\n",
    "                    \n",
    "                    todas_citas.append({\n",
    "                        'pagina': pagina,\n",
    "                        'texto': texto,\n",
    "                        'autor': autor,\n",
    "                        'tags': tags\n",
    "                    })\n",
    "                \n",
    "                # Buscar enlace a siguiente p√°gina\n",
    "                next_btn = soup.find('li', class_='next')\n",
    "                if not next_btn:\n",
    "                    print(f\"   üèÅ No hay m√°s p√°ginas despu√©s de la p√°gina {pagina}\")\n",
    "                    break\n",
    "                \n",
    "                # Delay entre requests\n",
    "                delay = random.uniform(1, 2)\n",
    "                print(f\"   ‚è≥ Esperando {delay:.2f} segundos...\")\n",
    "                time.sleep(delay)\n",
    "                \n",
    "                pagina += 1\n",
    "                \n",
    "            except requests.RequestException as e:\n",
    "                print(f\"   ‚ùå Error en p√°gina {pagina}: {e}\")\n",
    "                break\n",
    "        \n",
    "        return todas_citas\n",
    "    \n",
    "    # Ejecutar scraping paginado\n",
    "    citas_paginadas = scrape_quotes_paginado()\n",
    "    \n",
    "    print(f\"\\nüìä Resumen de paginaci√≥n:\")\n",
    "    print(f\"   üìö Total de citas recopiladas: {len(citas_paginadas)}\")\n",
    "    \n",
    "    # Agrupar por p√°gina\n",
    "    from collections import Counter\n",
    "    citas_por_pagina = Counter(cita['pagina'] for cita in citas_paginadas)\n",
    "    for pagina, cantidad in sorted(citas_por_pagina.items()):\n",
    "        print(f\"   üìÑ P√°gina {pagina}: {cantidad} citas\")\n",
    "    \n",
    "    # Autores m√°s frecuentes\n",
    "    autores_frecuentes = Counter(cita['autor'] for cita in citas_paginadas)\n",
    "    print(f\"\\nüë• Autores m√°s frecuentes:\")\n",
    "    for autor, cantidad in autores_frecuentes.most_common(3):\n",
    "        print(f\"   ‚úçÔ∏è {autor}: {cantidad} citas\")\n",
    "    \n",
    "    # 2. Detecci√≥n autom√°tica de paginaci√≥n\n",
    "    print(\"\\n\\n2Ô∏è‚É£ Detecci√≥n Autom√°tica de Paginaci√≥n\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    def detectar_paginacion(soup, current_url):\n",
    "        \"\"\"Detecta patrones de paginaci√≥n autom√°ticamente\"\"\"\n",
    "        \n",
    "        patrones_paginacion = []\n",
    "        \n",
    "        # 1. Buscar enlaces \"Next\" t√≠picos\n",
    "        next_patterns = [\n",
    "            ('a', {'class': re.compile(r'next', re.I)}),\n",
    "            ('a', {'id': re.compile(r'next', re.I)}),\n",
    "            ('a', {'text': re.compile(r'next|siguiente|‚Üí|¬ª', re.I)}),\n",
    "            ('li', {'class': re.compile(r'next', re.I)})\n",
    "        ]\n",
    "        \n",
    "        for tag, attrs in next_patterns:\n",
    "            if 'text' in attrs:\n",
    "                elementos = soup.find_all(tag, string=attrs['text'])\n",
    "            else:\n",
    "                elementos = soup.find_all(tag, attrs)\n",
    "            \n",
    "            for elem in elementos:\n",
    "                # Si es li, buscar enlace dentro\n",
    "                if tag == 'li':\n",
    "                    link = elem.find('a')\n",
    "                    if link and link.get('href'):\n",
    "                        href = link.get('href')\n",
    "                        full_url = urljoin(current_url, href)\n",
    "                        patrones_paginacion.append({\n",
    "                            'tipo': 'next_link_li',\n",
    "                            'elemento': str(elem)[:100] + '...',\n",
    "                            'url': full_url\n",
    "                        })\n",
    "                else:\n",
    "                    href = elem.get('href')\n",
    "                    if href:\n",
    "                        full_url = urljoin(current_url, href)\n",
    "                        patrones_paginacion.append({\n",
    "                            'tipo': 'next_link_direct',\n",
    "                            'elemento': str(elem)[:100] + '...',\n",
    "                            'url': full_url\n",
    "                        })\n",
    "        \n",
    "        # 2. Buscar enlaces numerados (1, 2, 3, ...)\n",
    "        links_numericos = soup.find_all('a', href=True)\n",
    "        for link in links_numericos:\n",
    "            href = link.get('href')\n",
    "            texto = link.get_text(strip=True)\n",
    "            \n",
    "            if texto.isdigit() and int(texto) > 1:\n",
    "                full_url = urljoin(current_url, href)\n",
    "                patrones_paginacion.append({\n",
    "                    'tipo': 'numbered_page',\n",
    "                    'elemento': str(link),\n",
    "                    'url': full_url,\n",
    "                    'numero_pagina': int(texto)\n",
    "                })\n",
    "        \n",
    "        # 3. Buscar patrones en URLs\n",
    "        parsed_url = urlparse(current_url)\n",
    "        path_parts = parsed_url.path.strip('/').split('/')\n",
    "        query_params = parse_qs(parsed_url.query)\n",
    "        \n",
    "        url_info = {\n",
    "            'base_url': f\"{parsed_url.scheme}://{parsed_url.netloc}\",\n",
    "            'path_parts': path_parts,\n",
    "            'query_params': query_params\n",
    "        }\n",
    "        \n",
    "        return patrones_paginacion, url_info\n",
    "    \n",
    "    # Probar detecci√≥n con quotes.toscrape.com\n",
    "    try:\n",
    "        session = requests.Session()\n",
    "        response = session.get('http://quotes.toscrape.com/page/1/', timeout=5)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        patrones, url_info = detectar_paginacion(soup, response.url)\n",
    "        \n",
    "        print(f\"üîç Patrones de paginaci√≥n detectados: {len(patrones)}\")\n",
    "        for i, patron in enumerate(patrones[:5], 1):  # Mostrar primeros 5\n",
    "            print(f\"\\n   {i}. Tipo: {patron['tipo']}\")\n",
    "            if 'numero_pagina' in patron:\n",
    "                print(f\"      P√°gina: {patron['numero_pagina']}\")\n",
    "            print(f\"      URL: {patron['url']}\")\n",
    "            print(f\"      Elemento: {patron['elemento'][:80]}...\")\n",
    "        \n",
    "        print(f\"\\nüîó Informaci√≥n de URL:\")\n",
    "        print(f\"   Base URL: {url_info['base_url']}\")\n",
    "        print(f\"   Path parts: {url_info['path_parts']}\")\n",
    "        print(f\"   Query params: {url_info['query_params']}\")\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"‚ùå Error probando detecci√≥n: {e}\")\n",
    "    \n",
    "    # 3. Estrategias de paginaci√≥n\n",
    "    print(\"\\n\\n3Ô∏è‚É£ Estrategias de Paginaci√≥n\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    estrategias = {\n",
    "        'üîó Siguiente/Anterior': {\n",
    "            'descripcion': 'Enlaces \"Next\" y \"Previous\"',\n",
    "            'ventajas': 'Simple, respeta la estructura del sitio',\n",
    "            'desventajas': 'Lento, secuencial',\n",
    "            'ejemplo': '/page/next/ o ?page=2'\n",
    "        },\n",
    "        'üî¢ P√°ginas Numeradas': {\n",
    "            'descripcion': 'Enlaces directos a p√°ginas espec√≠ficas',\n",
    "            'ventajas': 'Permite paralelizaci√≥n, acceso aleatorio',\n",
    "            'desventajas': 'Puede requerir conocer el total de p√°ginas',\n",
    "            'ejemplo': '/page/1/, /page/2/, /page/3/'\n",
    "        },\n",
    "        'üìä Offset/Limit': {\n",
    "            'descripcion': 'Par√°metros de desplazamiento',\n",
    "            'ventajas': 'Flexible, com√∫n en APIs',\n",
    "            'desventajas': 'Puede tener problemas con datos cambiantes',\n",
    "            'ejemplo': '?offset=20&limit=10'\n",
    "        },\n",
    "        'üîÑ Scroll Infinito': {\n",
    "            'descripcion': 'Cargar m√°s contenido v√≠a AJAX',\n",
    "            'ventajas': 'UX moderno, eficiente',\n",
    "            'desventajas': 'Requiere JavaScript/Selenium',\n",
    "            'ejemplo': 'Requests AJAX con cursores o timestamps'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for estrategia, info in estrategias.items():\n",
    "        print(f\"\\n{estrategia}\")\n",
    "        print(f\"   üìã {info['descripcion']}\")\n",
    "        print(f\"   ‚úÖ Ventajas: {info['ventajas']}\")\n",
    "        print(f\"   ‚ùå Desventajas: {info['desventajas']}\")\n",
    "        print(f\"   üí° Ejemplo: {info['ejemplo']}\")\n",
    "\n",
    "# Ejecutar demostraci√≥n\n",
    "manejar_paginacion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ejercicio Pr√°ctico Avanzado: Scraper de E-commerce üõí\n",
    "\n",
    "¬°Hora de aplicar todo lo aprendido! Vamos a crear un scraper completo para un sitio de e-commerce con m√∫ltiples p√°ginas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EJERCICIO AVANZADO: Scraper de E-commerce con Paginaci√≥n\n",
    "# Objetivo: Crear un scraper completo que maneje sesiones, paginaci√≥n y datos complejos\n",
    "\n",
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Optional\n",
    "import os\n",
    "\n",
    "@dataclass\n",
    "class Producto:\n",
    "    \"\"\"Clase para representar un producto\"\"\"\n",
    "    id: str\n",
    "    titulo: str\n",
    "    precio: float\n",
    "    precio_original: Optional[float]\n",
    "    disponible: bool\n",
    "    rating: float\n",
    "    num_reviews: int\n",
    "    imagen_url: str\n",
    "    producto_url: str\n",
    "    categoria: str\n",
    "    fecha_scraping: str\n",
    "\n",
    "class EcommerceScraper:\n",
    "    \"\"\"Scraper avanzado para sitios de e-commerce\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str, max_paginas: int = 5):\n",
    "        self.base_url = base_url\n",
    "        self.max_paginas = max_paginas\n",
    "        self.productos = []\n",
    "        \n",
    "        # Configurar sesi√≥n\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'es-ES,es;q=0.8,en-US;q=0.5,en;q=0.3',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "        })\n",
    "        \n",
    "        # Estad√≠sticas\n",
    "        self.stats = {\n",
    "            'paginas_procesadas': 0,\n",
    "            'productos_encontrados': 0,\n",
    "            'errores': 0,\n",
    "            'tiempo_inicio': None,\n",
    "            'tiempo_total': 0\n",
    "        }\n",
    "    \n",
    "    def extraer_producto(self, elemento_producto) -> Optional[Producto]:\n",
    "        \"\"\"Extrae informaci√≥n de un elemento producto\"\"\"\n",
    "        try:\n",
    "            # Extraer t√≠tulo\n",
    "            titulo_elem = elemento_producto.find(['h2', 'h3', 'h4'], class_=re.compile(r'title|name|product-name', re.I))\n",
    "            if not titulo_elem:\n",
    "                titulo_elem = elemento_producto.find(['h2', 'h3', 'h4'])\n",
    "            titulo = titulo_elem.get_text(strip=True) if titulo_elem else 'T√≠tulo no encontrado'\n",
    "            \n",
    "            # Extraer precio\n",
    "            precio_elem = elemento_producto.find(class_=re.compile(r'price|cost|amount', re.I))\n",
    "            if precio_elem:\n",
    "                precio_texto = precio_elem.get_text(strip=True)\n",
    "                # Extraer n√∫mero del precio\n",
    "                precio_match = re.search(r'([\\d,.]+)', precio_texto.replace('¬£', '').replace('$', ''))\n",
    "                precio = float(precio_match.group(1).replace(',', '')) if precio_match else 0.0\n",
    "            else:\n",
    "                precio = 0.0\n",
    "            \n",
    "            # Extraer rating (si existe)\n",
    "            rating_elem = elemento_producto.find(class_=re.compile(r'rating|star', re.I))\n",
    "            rating = 0.0\n",
    "            if rating_elem:\n",
    "                # Buscar texto con rating\n",
    "                rating_texto = rating_elem.get_text()\n",
    "                rating_match = re.search(r'([\\d.]+)', rating_texto)\n",
    "                if rating_match:\n",
    "                    rating = float(rating_match.group(1))\n",
    "                # O buscar en clases CSS (ej: star-rating-4)\n",
    "                else:\n",
    "                    for clase in rating_elem.get('class', []):\n",
    "                        rating_match = re.search(r'(\\d)', clase)\n",
    "                        if rating_match:\n",
    "                            rating = float(rating_match.group(1))\n",
    "                            break\n",
    "            \n",
    "            # Extraer imagen\n",
    "            img_elem = elemento_producto.find('img')\n",
    "            imagen_url = ''\n",
    "            if img_elem:\n",
    "                imagen_url = img_elem.get('src') or img_elem.get('data-src', '')\n",
    "                if imagen_url and not imagen_url.startswith('http'):\n",
    "                    imagen_url = urljoin(self.base_url, imagen_url)\n",
    "            \n",
    "            # Extraer enlace al producto\n",
    "            link_elem = elemento_producto.find('a', href=True)\n",
    "            producto_url = ''\n",
    "            if link_elem:\n",
    "                producto_url = link_elem.get('href')\n",
    "                if producto_url and not producto_url.startswith('http'):\n",
    "                    producto_url = urljoin(self.base_url, producto_url)\n",
    "            \n",
    "            # Generar ID √∫nico\n",
    "            producto_id = str(hash(titulo + str(precio)))[-8:]\n",
    "            \n",
    "            return Producto(\n",
    "                id=producto_id,\n",
    "                titulo=titulo[:200],  # Limitar longitud\n",
    "                precio=precio,\n",
    "                precio_original=None,  # Podr√≠a implementarse\n",
    "                disponible=True,  # Asumir disponible si aparece en lista\n",
    "                rating=rating,\n",
    "                num_reviews=0,  # Podr√≠a implementarse\n",
    "                imagen_url=imagen_url,\n",
    "                producto_url=producto_url,\n",
    "                categoria='general',  # Podr√≠a extraerse de breadcrumbs\n",
    "                fecha_scraping=datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error extrayendo producto: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def scrape_pagina(self, url: str) -> List[Producto]:\n",
    "        \"\"\"Scrape una p√°gina espec√≠fica\"\"\"\n",
    "        productos_pagina = []\n",
    "        \n",
    "        try:\n",
    "            print(f\"\\nüîç Scraping: {url}\")\n",
    "            \n",
    "            response = self.session.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Buscar contenedores de productos con diferentes patrones\n",
    "            selectores_producto = [\n",
    "                'article.product',\n",
    "                '.product-item',\n",
    "                '.product-card',\n",
    "                '.book',  # Para books.toscrape.com\n",
    "                '.product',\n",
    "                '[data-product-id]'\n",
    "            ]\n",
    "            \n",
    "            elementos_productos = []\n",
    "            for selector in selectores_producto:\n",
    "                elementos = soup.select(selector)\n",
    "                if elementos:\n",
    "                    elementos_productos = elementos\n",
    "                    print(f\"   ‚úÖ Usando selector: {selector} ({len(elementos)} productos)\")\n",
    "                    break\n",
    "            \n",
    "            if not elementos_productos:\n",
    "                print(f\"   ‚ö†Ô∏è No se encontraron productos con los selectores conocidos\")\n",
    "                return productos_pagina\n",
    "            \n",
    "            # Extraer cada producto\n",
    "            for i, elemento in enumerate(elementos_productos, 1):\n",
    "                producto = self.extraer_producto(elemento)\n",
    "                if producto:\n",
    "                    productos_pagina.append(producto)\n",
    "                    print(f\"   üì¶ Producto {i}: {producto.titulo[:50]}... - ${producto.precio}\")\n",
    "            \n",
    "            print(f\"   ‚úÖ Extra√≠dos {len(productos_pagina)} productos de esta p√°gina\")\n",
    "            \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"   ‚ùå Error de red: {e}\")\n",
    "            self.stats['errores'] += 1\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error inesperado: {e}\")\n",
    "            self.stats['errores'] += 1\n",
    "        \n",
    "        return productos_pagina\n",
    "    \n",
    "    def encontrar_siguiente_pagina(self, soup, current_url: str) -> Optional[str]:\n",
    "        \"\"\"Encuentra la URL de la siguiente p√°gina\"\"\"\n",
    "        \n",
    "        # Patrones comunes para \"siguiente p√°gina\"\n",
    "        patrones_next = [\n",
    "            ('a', {'class': re.compile(r'next', re.I)}),\n",
    "            ('li', {'class': 'next'}),\n",
    "            ('a', {'title': re.compile(r'next', re.I)}),\n",
    "            ('a', {'rel': 'next'})\n",
    "        ]\n",
    "        \n",
    "        for tag, attrs in patrones_next:\n",
    "            elemento = soup.find(tag, attrs)\n",
    "            if elemento:\n",
    "                if tag == 'li':\n",
    "                    link = elemento.find('a')\n",
    "                    if link and link.get('href'):\n",
    "                        return urljoin(current_url, link.get('href'))\n",
    "                else:\n",
    "                    href = elemento.get('href')\n",
    "                    if href:\n",
    "                        return urljoin(current_url, href)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def scrape_completo(self) -> List[Producto]:\n",
    "        \"\"\"Ejecuta el scraping completo con paginaci√≥n\"\"\"\n",
    "        print(\"üõí INICIANDO SCRAPING DE E-COMMERCE\\n\")\n",
    "        print(\"‚ïê\" * 70)\n",
    "        \n",
    "        self.stats['tiempo_inicio'] = time.time()\n",
    "        \n",
    "        url_actual = self.base_url\n",
    "        pagina_num = 1\n",
    "        \n",
    "        while pagina_num <= self.max_paginas and url_actual:\n",
    "            print(f\"\\nüìÑ P√ÅGINA {pagina_num} de {self.max_paginas}\")\n",
    "            print(f\"üîó URL: {url_actual}\")\n",
    "            \n",
    "            # Scrape p√°gina actual\n",
    "            productos_pagina = self.scrape_pagina(url_actual)\n",
    "            self.productos.extend(productos_pagina)\n",
    "            \n",
    "            self.stats['paginas_procesadas'] += 1\n",
    "            self.stats['productos_encontrados'] += len(productos_pagina)\n",
    "            \n",
    "            # Buscar siguiente p√°gina\n",
    "            if pagina_num < self.max_paginas:\n",
    "                try:\n",
    "                    response = self.session.get(url_actual, timeout=10)\n",
    "                    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                    url_siguiente = self.encontrar_siguiente_pagina(soup, url_actual)\n",
    "                    \n",
    "                    if url_siguiente and url_siguiente != url_actual:\n",
    "                        print(f\"   ‚û°Ô∏è Siguiente p√°gina encontrada: {url_siguiente}\")\n",
    "                        url_actual = url_siguiente\n",
    "                    else:\n",
    "                        print(f\"   üèÅ No hay m√°s p√°ginas\")\n",
    "                        break\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ùå Error buscando siguiente p√°gina: {e}\")\n",
    "                    break\n",
    "            \n",
    "            # Delay entre p√°ginas\n",
    "            if pagina_num < self.max_paginas and url_actual:\n",
    "                delay = random.uniform(1, 3)\n",
    "                print(f\"   ‚è≥ Delay de {delay:.2f} segundos...\")\n",
    "                time.sleep(delay)\n",
    "            \n",
    "            pagina_num += 1\n",
    "        \n",
    "        self.stats['tiempo_total'] = time.time() - self.stats['tiempo_inicio']\n",
    "        \n",
    "        return self.productos\n",
    "    \n",
    "    def mostrar_estadisticas(self):\n",
    "        \"\"\"Muestra estad√≠sticas del scraping\"\"\"\n",
    "        print(\"\\n\" + \"‚ïê\" * 70)\n",
    "        print(\"üìä ESTAD√çSTICAS FINALES\")\n",
    "        print(\"‚ïê\" * 70)\n",
    "        \n",
    "        print(f\"üìÑ P√°ginas procesadas: {self.stats['paginas_procesadas']}\")\n",
    "        print(f\"üõçÔ∏è Productos encontrados: {self.stats['productos_encontrados']}\")\n",
    "        print(f\"‚ùå Errores: {self.stats['errores']}\")\n",
    "        print(f\"‚è±Ô∏è Tiempo total: {self.stats['tiempo_total']:.2f} segundos\")\n",
    "        \n",
    "        if self.productos:\n",
    "            precios = [p.precio for p in self.productos if p.precio > 0]\n",
    "            if precios:\n",
    "                print(f\"üí∞ Precio promedio: ${sum(precios)/len(precios):.2f}\")\n",
    "                print(f\"üí∞ Precio m√≠nimo: ${min(precios):.2f}\")\n",
    "                print(f\"üí∞ Precio m√°ximo: ${max(precios):.2f}\")\n",
    "            \n",
    "            # Top productos por precio\n",
    "            productos_ordenados = sorted([p for p in self.productos if p.precio > 0], \n",
    "                                       key=lambda x: x.precio, reverse=True)\n",
    "            \n",
    "            print(f\"\\nüèÜ TOP 3 PRODUCTOS M√ÅS CAROS:\")\n",
    "            for i, producto in enumerate(productos_ordenados[:3], 1):\n",
    "                print(f\"   {i}. {producto.titulo[:50]}... - ${producto.precio}\")\n",
    "    \n",
    "    def guardar_datos(self, formato='json'):\n",
    "        \"\"\"Guarda los datos en diferentes formatos\"\"\"\n",
    "        if not self.productos:\n",
    "            print(\"‚ùå No hay productos para guardar\")\n",
    "            return\n",
    "        \n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        \n",
    "        # Crear directorio de datos\n",
    "        os.makedirs('../data', exist_ok=True)\n",
    "        \n",
    "        if formato == 'json':\n",
    "            filename = f'../data/productos_{timestamp}.json'\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump([asdict(p) for p in self.productos], f, \n",
    "                         ensure_ascii=False, indent=2)\n",
    "            print(f\"üíæ Datos guardados en JSON: {filename}\")\n",
    "        \n",
    "        elif formato == 'csv':\n",
    "            filename = f'../data/productos_{timestamp}.csv'\n",
    "            with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
    "                if self.productos:\n",
    "                    writer = csv.DictWriter(f, fieldnames=asdict(self.productos[0]).keys())\n",
    "                    writer.writeheader()\n",
    "                    for producto in self.productos:\n",
    "                        writer.writerow(asdict(producto))\n",
    "            print(f\"üìä Datos guardados en CSV: {filename}\")\n",
    "\n",
    "# ¬°EJECUTAR EL SCRAPER COMPLETO!\n",
    "print(\"üöÄ EJERCICIO AVANZADO: SCRAPER DE E-COMMERCE\\n\")\n",
    "print(\"‚ïê\" * 70)\n",
    "print(\"Vamos a scrapear books.toscrape.com como ejemplo de e-commerce\")\n",
    "print(\"Este scraper incluye:\")\n",
    "print(\"  ‚úÖ Manejo de sesiones HTTP\")\n",
    "print(\"  ‚úÖ Paginaci√≥n autom√°tica\")\n",
    "print(\"  ‚úÖ Extracci√≥n de datos complejos\")\n",
    "print(\"  ‚úÖ Manejo de errores robusto\")\n",
    "print(\"  ‚úÖ Estad√≠sticas detalladas\")\n",
    "print(\"  ‚úÖ Exportaci√≥n de datos\")\n",
    "\n",
    "# Crear y ejecutar scraper\n",
    "scraper = EcommerceScraper(\n",
    "    base_url='http://books.toscrape.com/',\n",
    "    max_paginas=3  # Limitar para demostraci√≥n\n",
    ")\n",
    "\n",
    "# Ejecutar scraping\n",
    "productos = scraper.scrape_completo()\n",
    "\n",
    "# Mostrar estad√≠sticas\n",
    "scraper.mostrar_estadisticas()\n",
    "\n",
    "# Guardar datos\n",
    "scraper.guardar_datos('json')\n",
    "scraper.guardar_datos('csv')\n",
    "\n",
    "print(\"\\nüéâ ¬°EJERCICIO COMPLETADO EXITOSAMENTE!\")\n",
    "print(f\"üìä Se scrapearon {len(productos)} productos de {scraper.stats['paginas_procesadas']} p√°ginas\")\n",
    "print(\"üíæ Datos guardados en formato JSON y CSV\")\n",
    "print(\"\\nüèÜ ¬°Has creado un scraper de e-commerce profesional!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Resumen y Pr√≥ximos Pasos üéØ\n",
    "\n",
    "### üéì Lo que has dominado en esta lecci√≥n:\n",
    "\n",
    "#### ‚úÖ **HTTP Avanzado**\n",
    "- üåê M√©todos HTTP y sus usos apropiados\n",
    "- üïµÔ∏è Headers HTTP y su importancia\n",
    "- üç™ Manejo avanzado de sesiones y cookies\n",
    "- üì§ POST requests y env√≠o de formularios\n",
    "\n",
    "#### ‚úÖ **Beautiful Soup Profesional**\n",
    "- üîç T√©cnicas de b√∫squeda avanzadas\n",
    "- üå≥ Navegaci√≥n compleja del DOM\n",
    "- üìù Extracci√≥n de texto sofisticada\n",
    "- üõ†Ô∏è Modificaci√≥n del DOM\n",
    "\n",
    "#### ‚úÖ **Contenido Complejo**\n",
    "- üåç Manejo de encodings internacionales\n",
    "- üìÑ Paginaci√≥n y crawling avanzado\n",
    "- üîó Detecci√≥n autom√°tica de patrones\n",
    "- üìä Extracci√≥n de datos estructurados\n",
    "\n",
    "#### ‚úÖ **Proyecto Profesional**\n",
    "- üõí Scraper de e-commerce completo\n",
    "- üìä Manejo de datos con dataclasses\n",
    "- üíæ Exportaci√≥n en m√∫ltiples formatos\n",
    "- üìà Estad√≠sticas y monitoreo\n",
    "\n",
    "### üöÄ Pr√≥xima Lecci√≥n: XPath y Selecci√≥n Avanzada\n",
    "\n",
    "En la **Lecci√≥n 3** exploraremos:\n",
    "\n",
    "#### üõ§Ô∏è **XPath Completo**\n",
    "- Sintaxis XPath desde b√°sico hasta experto\n",
    "- Funciones XPath avanzadas\n",
    "- Comparaci√≥n XPath vs CSS Selectors\n",
    "- Casos de uso espec√≠ficos\n",
    "\n",
    "#### üéØ **Selecci√≥n Avanzada**\n",
    "- T√©cnicas de selecci√≥n precisas\n",
    "- Manejo de contenido din√°mico\n",
    "- Optimizaci√≥n de selectores\n",
    "- Debugging y troubleshooting\n",
    "\n",
    "### üí™ Ejercicios para Practicar\n",
    "\n",
    "1. **Scraper de Noticias con Login**: Crear un scraper que maneje autenticaci√≥n\n",
    "2. **Monitor de Precios**: Sistema que rastrea cambios de precios\n",
    "3. **Agregador Multi-sitio**: Combinar datos de m√∫ltiples fuentes\n",
    "\n",
    "### üìö Recursos Recomendados\n",
    "\n",
    "- [HTTP Status Codes Reference](https://httpstatuses.com/)\n",
    "- [Beautiful Soup Advanced Guide](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Unicode in Python](https://docs.python.org/3/howto/unicode.html)\n",
    "- [Regular Expressions Guide](https://docs.python.org/3/library/re.html)\n",
    "\n",
    "---\n",
    "\n",
    "### üèÜ ¬°Felicidades por Completar la Lecci√≥n 2!\n",
    "\n",
    "Ahora dominas t√©cnicas avanzadas de HTTP y Beautiful Soup. Puedes crear scrapers robustos que manejan:\n",
    "- ‚ú® Sitios complejos con autenticaci√≥n\n",
    "- üîÑ M√∫ltiples p√°ginas autom√°ticamente  \n",
    "- üåç Contenido internacional\n",
    "- üìä Datos estructurados y complejos\n",
    "\n",
    "**¬°Tu arsenal de scraping se est√° volviendo muy poderoso! üåü**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}