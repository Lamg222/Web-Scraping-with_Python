{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üï∑Ô∏è Lecci√≥n 5: Introducci√≥n a Scrapy Framework\n",
    "\n",
    "## üéØ Objetivos\n",
    "\n",
    "- Entender la arquitectura de Scrapy\n",
    "- Crear y configurar proyectos Scrapy\n",
    "- Desarrollar spiders b√°sicos y avanzados\n",
    "- Usar Items y Pipelines\n",
    "- Configurar settings y middlewares\n",
    "- Manejar requests y responses eficientemente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è ¬øQu√© es Scrapy?\n",
    "\n",
    "**Scrapy** es el framework m√°s potente y completo para web scraping en Python. A diferencia de Beautiful Soup que es una biblioteca, Scrapy es un framework completo con:\n",
    "\n",
    "### üåü Ventajas de Scrapy:\n",
    "- **As√≠ncrono**: Maneja m√∫ltiples requests simult√°neamente\n",
    "- **Robusto**: Manejo autom√°tico de errores y reintentos\n",
    "- **Escalable**: Perfecto para proyectos grandes\n",
    "- **Extensible**: Middlewares y pipelines personalizables\n",
    "- **Integrado**: Exportaci√≥n autom√°tica a JSON, CSV, XML\n",
    "\n",
    "### üèõÔ∏è Arquitectura de Scrapy:\n",
    "```\n",
    "Engine ‚Üê ‚Üí Scheduler ‚Üê ‚Üí Downloader\n",
    "   ‚Üï              ‚Üï         ‚Üï\n",
    "Spider ‚Üê ‚Üí Items ‚Üí Item Pipeline\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalaci√≥n y configuraci√≥n inicial\n",
    "import scrapy\n",
    "from scrapy import Request, Spider\n",
    "from scrapy.http import Response\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "print(\"üï∑Ô∏è SCRAPY FRAMEWORK INTRODUCTION\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Scrapy version: {scrapy.__version__}\")\n",
    "print(\"‚úÖ Scrapy importado correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Creando tu Primer Spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spider b√°sico para Quotes to Scrape\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = 'quotes'\n",
    "    allowed_domains = ['quotes.toscrape.com']\n",
    "    start_urls = ['http://quotes.toscrape.com/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        \"\"\"M√©todo principal de parsing\"\"\"\n",
    "        print(f\"üåê Procesando: {response.url}\")\n",
    "        \n",
    "        # Extraer todas las citas\n",
    "        quotes = response.css('.quote')\n",
    "        print(f\"üìñ Encontradas {len(quotes)} citas\")\n",
    "        \n",
    "        for quote in quotes:\n",
    "            # Extraer datos con CSS selectors\n",
    "            yield {\n",
    "                'text': quote.css('.text::text').get(),\n",
    "                'author': quote.css('.author::text').get(),\n",
    "                'tags': quote.css('.tag::text').getall(),\n",
    "            }\n",
    "        \n",
    "        # Seguir a la siguiente p√°gina\n",
    "        next_page = response.css('.next a::attr(href)').get()\n",
    "        if next_page:\n",
    "            print(f\"‚û°Ô∏è Siguiente p√°gina: {next_page}\")\n",
    "            yield response.follow(next_page, self.parse)\n",
    "\n",
    "print(\"üï∑Ô∏è SPIDER B√ÅSICO CREADO\")\n",
    "print(\"Componentes del Spider:\")\n",
    "print(\"  ‚Ä¢ name: Identificador √∫nico\")\n",
    "print(\"  ‚Ä¢ start_urls: URLs iniciales\")\n",
    "print(\"  ‚Ä¢ parse(): M√©todo de procesamiento\")\n",
    "print(\"  ‚Ä¢ yield: Retorna datos o requests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è Items y Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir Items (estructura de datos)\n",
    "import scrapy\n",
    "from scrapy import Item, Field\n",
    "\n",
    "class QuoteItem(scrapy.Item):\n",
    "    \"\"\"Estructura para almacenar citas\"\"\"\n",
    "    text = scrapy.Field()\n",
    "    author = scrapy.Field()\n",
    "    tags = scrapy.Field()\n",
    "    url = scrapy.Field()\n",
    "    scraped_at = scrapy.Field()\n",
    "\n",
    "class ProductItem(scrapy.Item):\n",
    "    \"\"\"Estructura para productos e-commerce\"\"\"\n",
    "    name = scrapy.Field()\n",
    "    price = scrapy.Field()\n",
    "    description = scrapy.Field()\n",
    "    images = scrapy.Field()\n",
    "    availability = scrapy.Field()\n",
    "    rating = scrapy.Field()\n",
    "    reviews_count = scrapy.Field()\n",
    "\n",
    "print(\"üè∑Ô∏è ITEMS DEFINIDOS\")\n",
    "print(\"Items act√∫an como estructuras de datos validadas\")\n",
    "\n",
    "# Pipeline b√°sico\n",
    "class ValidationPipeline:\n",
    "    \"\"\"Pipeline para validar datos\"\"\"\n",
    "    \n",
    "    def process_item(self, item, spider):\n",
    "        if not item.get('text'):\n",
    "            raise scrapy.exceptions.DropItem(\"Texto faltante\")\n",
    "        \n",
    "        if not item.get('author'):\n",
    "            raise scrapy.exceptions.DropItem(\"Autor faltante\")\n",
    "        \n",
    "        return item\n",
    "\n",
    "class ProcessingPipeline:\n",
    "    \"\"\"Pipeline para procesar datos\"\"\"\n",
    "    \n",
    "    def process_item(self, item, spider):\n",
    "        # Limpiar texto\n",
    "        if item.get('text'):\n",
    "            item['text'] = item['text'].strip('\"')\n",
    "        \n",
    "        # Convertir tags a min√∫sculas\n",
    "        if item.get('tags'):\n",
    "            item['tags'] = [tag.lower() for tag in item['tags']]\n",
    "        \n",
    "        # Agregar timestamp\n",
    "        from datetime import datetime\n",
    "        item['scraped_at'] = datetime.now().isoformat()\n",
    "        \n",
    "        return item\n",
    "\n",
    "print(\"\\nüîß PIPELINES CREADOS\")\n",
    "print(\"  ‚Ä¢ ValidationPipeline: Valida datos requeridos\")\n",
    "print(\"  ‚Ä¢ ProcessingPipeline: Limpia y procesa datos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üï∑Ô∏è Spider Avanzado con Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedQuotesSpider(scrapy.Spider):\n",
    "    name = 'advanced_quotes'\n",
    "    allowed_domains = ['quotes.toscrape.com']\n",
    "    start_urls = ['http://quotes.toscrape.com/']\n",
    "    \n",
    "    custom_settings = {\n",
    "        'ITEM_PIPELINES': {\n",
    "            '__main__.ValidationPipeline': 300,\n",
    "            '__main__.ProcessingPipeline': 400,\n",
    "        },\n",
    "        'DOWNLOAD_DELAY': 1,  # Pausa entre requests\n",
    "        'RANDOMIZE_DOWNLOAD_DELAY': 0.5,\n",
    "        'USER_AGENT': 'advanced_quotes_spider'\n",
    "    }\n",
    "\n",
    "    def parse(self, response):\n",
    "        \"\"\"Parse principal con Items\"\"\"\n",
    "        self.logger.info(f'Parseando p√°gina: {response.url}')\n",
    "        \n",
    "        # Extraer citas usando Items\n",
    "        quotes = response.css('.quote')\n",
    "        \n",
    "        for quote in quotes:\n",
    "            item = QuoteItem()\n",
    "            item['text'] = quote.css('.text::text').get()\n",
    "            item['author'] = quote.css('.author::text').get()\n",
    "            item['tags'] = quote.css('.tag::text').getall()\n",
    "            item['url'] = response.url\n",
    "            \n",
    "            yield item\n",
    "            \n",
    "            # Tambi√©n obtener informaci√≥n del autor\n",
    "            author_url = quote.css('.author + a::attr(href)').get()\n",
    "            if author_url:\n",
    "                yield response.follow(\n",
    "                    author_url, \n",
    "                    self.parse_author,\n",
    "                    meta={'author_name': item['author']}\n",
    "                )\n",
    "        \n",
    "        # Paginaci√≥n\n",
    "        next_page = response.css('.next a::attr(href)').get()\n",
    "        if next_page:\n",
    "            yield response.follow(next_page, self.parse)\n",
    "    \n",
    "    def parse_author(self, response):\n",
    "        \"\"\"Parse informaci√≥n de autores\"\"\"\n",
    "        author_name = response.meta['author_name']\n",
    "        \n",
    "        yield {\n",
    "            'author_name': author_name,\n",
    "            'born_date': response.css('.author-born-date::text').get(),\n",
    "            'born_location': response.css('.author-born-location::text').get(),\n",
    "            'description': response.css('.author-description::text').get(),\n",
    "        }\n",
    "\n",
    "print(\"üï∑Ô∏è SPIDER AVANZADO CREADO\")\n",
    "print(\"Caracter√≠sticas avanzadas:\")\n",
    "print(\"  ‚Ä¢ Custom settings por spider\")\n",
    "print(\"  ‚Ä¢ Items estructurados\")\n",
    "print(\"  ‚Ä¢ M√∫ltiples callbacks (parse_author)\")\n",
    "print(\"  ‚Ä¢ Logging integrado\")\n",
    "print(\"  ‚Ä¢ Meta data entre requests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuraci√≥n y Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulaci√≥n de settings.py\n",
    "SCRAPY_SETTINGS = {\n",
    "    # Identificaci√≥n del bot\n",
    "    'BOT_NAME': 'mi_scraper',\n",
    "    'USER_AGENT': 'mi_scraper (+http://www.yourdomain.com)',\n",
    "    \n",
    "    # Configuraci√≥n de cortes√≠a\n",
    "    'ROBOTSTXT_OBEY': True,\n",
    "    'DOWNLOAD_DELAY': 3,\n",
    "    'RANDOMIZE_DOWNLOAD_DELAY': 0.5,\n",
    "    'CONCURRENT_REQUESTS': 16,\n",
    "    'CONCURRENT_REQUESTS_PER_DOMAIN': 8,\n",
    "    \n",
    "    # Pipelines\n",
    "    'ITEM_PIPELINES': {\n",
    "        'myproject.pipelines.ValidationPipeline': 300,\n",
    "        'myproject.pipelines.DuplicatesPipeline': 400,\n",
    "        'myproject.pipelines.DatabasePipeline': 500,\n",
    "    },\n",
    "    \n",
    "    # Middlewares\n",
    "    'DOWNLOADER_MIDDLEWARES': {\n",
    "        'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,\n",
    "        'scrapy_user_agents.middlewares.RandomUserAgentMiddleware': 400,\n",
    "        'scrapy.downloadermiddlewares.retry.RetryMiddleware': 500,\n",
    "    },\n",
    "    \n",
    "    # Configuraci√≥n de reintentos\n",
    "    'RETRY_TIMES': 3,\n",
    "    'RETRY_HTTP_CODES': [500, 502, 503, 504, 408, 429],\n",
    "    \n",
    "    # Exportaci√≥n\n",
    "    'FEEDS': {\n",
    "        'output/quotes.json': {\n",
    "            'format': 'json',\n",
    "            'encoding': 'utf8',\n",
    "            'store_empty': False,\n",
    "        },\n",
    "        'output/quotes.csv': {\n",
    "            'format': 'csv',\n",
    "            'encoding': 'utf8',\n",
    "        },\n",
    "    },\n",
    "    \n",
    "    # Logging\n",
    "    'LOG_LEVEL': 'INFO',\n",
    "    'LOG_FILE': 'scrapy.log',\n",
    "    \n",
    "    # Cache\n",
    "    'HTTPCACHE_ENABLED': True,\n",
    "    'HTTPCACHE_EXPIRATION_SECS': 3600,\n",
    "    'HTTPCACHE_DIR': 'httpcache',\n",
    "}\n",
    "\n",
    "print(\"‚öôÔ∏è CONFIGURACI√ìN DE SCRAPY\")\n",
    "print(\"=\" * 35)\n",
    "print(\"Configuraciones importantes:\")\n",
    "print(f\"  ü§ñ Bot Name: {SCRAPY_SETTINGS['BOT_NAME']}\")\n",
    "print(f\"  ‚è±Ô∏è Download Delay: {SCRAPY_SETTINGS['DOWNLOAD_DELAY']}s\")\n",
    "print(f\"  üîÑ Concurrent Requests: {SCRAPY_SETTINGS['CONCURRENT_REQUESTS']}\")\n",
    "print(f\"  üìä Pipelines: {len(SCRAPY_SETTINGS['ITEM_PIPELINES'])}\")\n",
    "print(f\"  üìÅ Export Formats: {len(SCRAPY_SETTINGS['FEEDS'])}\")\n",
    "print(f\"  üíæ Cache Enabled: {SCRAPY_SETTINGS['HTTPCACHE_ENABLED']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Middlewares Personalizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Middleware para rotar User Agents\n",
    "class RotateUserAgentMiddleware:\n",
    "    def __init__(self):\n",
    "        self.user_agents = [\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',\n",
    "            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36',\n",
    "        ]\n",
    "        self.current_ua = 0\n",
    "    \n",
    "    def process_request(self, request, spider):\n",
    "        ua = self.user_agents[self.current_ua]\n",
    "        request.headers['User-Agent'] = ua\n",
    "        self.current_ua = (self.current_ua + 1) % len(self.user_agents)\n",
    "        return None\n",
    "\n",
    "# Middleware para manejar proxies\n",
    "class ProxyMiddleware:\n",
    "    def __init__(self):\n",
    "        self.proxies = [\n",
    "            'http://proxy1:8000',\n",
    "            'http://proxy2:8000',\n",
    "            'http://proxy3:8000',\n",
    "        ]\n",
    "        self.current_proxy = 0\n",
    "    \n",
    "    def process_request(self, request, spider):\n",
    "        proxy = self.proxies[self.current_proxy]\n",
    "        request.meta['proxy'] = proxy\n",
    "        self.current_proxy = (self.current_proxy + 1) % len(self.proxies)\n",
    "        return None\n",
    "\n",
    "# Middleware para manejar errores\n",
    "class ErrorHandlingMiddleware:\n",
    "    def process_response(self, request, response, spider):\n",
    "        if response.status in [403, 404, 429]:\n",
    "            spider.logger.warning(f'Error {response.status} en {request.url}')\n",
    "            # Crear nuevo request con delay\n",
    "            new_request = request.copy()\n",
    "            new_request.meta['delay'] = 10\n",
    "            return new_request\n",
    "        return response\n",
    "    \n",
    "    def process_exception(self, request, exception, spider):\n",
    "        spider.logger.error(f'Excepci√≥n en {request.url}: {exception}')\n",
    "        # Reintentar con delay\n",
    "        new_request = request.copy()\n",
    "        new_request.meta['retry_times'] = request.meta.get('retry_times', 0) + 1\n",
    "        if new_request.meta['retry_times'] < 3:\n",
    "            return new_request\n",
    "        return None\n",
    "\n",
    "print(\"üõ†Ô∏è MIDDLEWARES PERSONALIZADOS\")\n",
    "print(\"=\" * 35)\n",
    "print(\"Middlewares disponibles:\")\n",
    "print(\"  üîÑ RotateUserAgentMiddleware: Rota User Agents\")\n",
    "print(\"  üåê ProxyMiddleware: Maneja proxies\")\n",
    "print(\"  ‚ùå ErrorHandlingMiddleware: Maneja errores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè™ Spider para E-commerce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EcommerceSpider(scrapy.Spider):\n",
    "    name = 'books'\n",
    "    allowed_domains = ['books.toscrape.com']\n",
    "    start_urls = ['http://books.toscrape.com/']\n",
    "    \n",
    "    custom_settings = {\n",
    "        'DOWNLOAD_DELAY': 1,\n",
    "        'CONCURRENT_REQUESTS_PER_DOMAIN': 2,\n",
    "        'FEEDS': {\n",
    "            'books.json': {\n",
    "                'format': 'json',\n",
    "                'encoding': 'utf8',\n",
    "                'indent': 2\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def parse(self, response):\n",
    "        \"\"\"Parse p√°gina principal y categor√≠as\"\"\"\n",
    "        # Obtener todos los libros de la p√°gina\n",
    "        books = response.css('article.product_pod')\n",
    "        \n",
    "        for book in books:\n",
    "            book_url = book.css('h3 a::attr(href)').get()\n",
    "            if book_url:\n",
    "                yield response.follow(\n",
    "                    book_url, \n",
    "                    self.parse_book,\n",
    "                    meta={'category': self.get_category_from_breadcrumb(response)}\n",
    "                )\n",
    "        \n",
    "        # Seguir paginaci√≥n\n",
    "        next_page = response.css('.next a::attr(href)').get()\n",
    "        if next_page:\n",
    "            yield response.follow(next_page, self.parse)\n",
    "    \n",
    "    def parse_book(self, response):\n",
    "        \"\"\"Parse p√°gina individual del libro\"\"\"\n",
    "        # Crear item del producto\n",
    "        item = ProductItem()\n",
    "        \n",
    "        # Informaci√≥n b√°sica\n",
    "        item['name'] = response.css('h1::text').get()\n",
    "        item['price'] = response.css('.price_color::text').get()\n",
    "        item['availability'] = response.css('.availability::text').re_first(r'In stock \\((\\d+) available\\)')\n",
    "        item['description'] = response.css('#product_description + p::text').get()\n",
    "        \n",
    "        # Rating (convertir estrellas a n√∫mero)\n",
    "        rating_class = response.css('.star-rating::attr(class)').get()\n",
    "        if rating_class:\n",
    "            rating_map = {'One': 1, 'Two': 2, 'Three': 3, 'Four': 4, 'Five': 5}\n",
    "            rating_text = rating_class.replace('star-rating ', '')\n",
    "            item['rating'] = rating_map.get(rating_text, 0)\n",
    "        \n",
    "        # Informaci√≥n adicional de la tabla\n",
    "        table_rows = response.css('table tr')\n",
    "        for row in table_rows:\n",
    "            field = row.css('td:first-child::text').get()\n",
    "            value = row.css('td:last-child::text').get()\n",
    "            \n",
    "            if field == 'Number of reviews':\n",
    "                item['reviews_count'] = int(value) if value.isdigit() else 0\n",
    "        \n",
    "        # Im√°genes\n",
    "        image_url = response.css('#product_gallery img::attr(src)').get()\n",
    "        if image_url:\n",
    "            item['images'] = [response.urljoin(image_url)]\n",
    "        \n",
    "        # Categor√≠a desde meta\n",
    "        category = response.meta.get('category', 'Unknown')\n",
    "        \n",
    "        # Agregar datos calculados\n",
    "        price_text = item.get('price', '¬£0.00')\n",
    "        price_numeric = float(price_text.replace('¬£', '').replace(',', '')) if price_text else 0\n",
    "        \n",
    "        yield {\n",
    "            **item,\n",
    "            'category': category,\n",
    "            'price_numeric': price_numeric,\n",
    "            'url': response.url,\n",
    "            'scraped_at': self.get_timestamp()\n",
    "        }\n",
    "    \n",
    "    def get_category_from_breadcrumb(self, response):\n",
    "        \"\"\"Extraer categor√≠a del breadcrumb\"\"\"\n",
    "        breadcrumb = response.css('.breadcrumb li:last-child::text').get()\n",
    "        return breadcrumb.strip() if breadcrumb else 'All'\n",
    "    \n",
    "    def get_timestamp(self):\n",
    "        \"\"\"Obtener timestamp actual\"\"\"\n",
    "        from datetime import datetime\n",
    "        return datetime.now().isoformat()\n",
    "\n",
    "print(\"üè™ SPIDER E-COMMERCE CREADO\")\n",
    "print(\"Caracter√≠sticas del spider:\")\n",
    "print(\"  üìö Scraping de libros completo\")\n",
    "print(\"  üîó Seguimiento de enlaces de productos\")\n",
    "print(\"  üìä Extracci√≥n de ratings y precios\")\n",
    "print(\"  üì± Manejo de im√°genes y metadatos\")\n",
    "print(\"  üìÑ Paginaci√≥n autom√°tica\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Ejercicios Pr√°cticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ EJERCICIOS SCRAPY\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# Ejercicio 1: Spider b√°sico\n",
    "class NewsSpider(scrapy.Spider):\n",
    "    \"\"\"Ejercicio: Crear spider para noticias\"\"\"\n",
    "    name = 'news'\n",
    "    start_urls = ['https://example-news-site.com']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        # Extraer t√≠tulos de noticias\n",
    "        articles = response.css('.article')\n",
    "        \n",
    "        for article in articles:\n",
    "            yield {\n",
    "                'title': article.css('.title::text').get(),\n",
    "                'summary': article.css('.summary::text').get(),\n",
    "                'date': article.css('.date::text').get(),\n",
    "                'author': article.css('.author::text').get(),\n",
    "                'url': response.url\n",
    "            }\n",
    "\n",
    "# Ejercicio 2: Pipeline de limpieza\n",
    "class CleaningPipeline:\n",
    "    \"\"\"Pipeline para limpiar y validar noticias\"\"\"\n",
    "    \n",
    "    def process_item(self, item, spider):\n",
    "        # Limpiar t√≠tulo\n",
    "        if item.get('title'):\n",
    "            item['title'] = item['title'].strip().title()\n",
    "        \n",
    "        # Validar fecha\n",
    "        if not item.get('date'):\n",
    "            from datetime import datetime\n",
    "            item['date'] = datetime.now().strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Calcular longitud del resumen\n",
    "        if item.get('summary'):\n",
    "            item['summary_length'] = len(item['summary'])\n",
    "        \n",
    "        return item\n",
    "\n",
    "# Ejercicio 3: Spider con formularios\n",
    "class SearchSpider(scrapy.Spider):\n",
    "    \"\"\"Spider que maneja formularios de b√∫squeda\"\"\"\n",
    "    name = 'search'\n",
    "    start_urls = ['https://example-search.com']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        # Llenar formulario de b√∫squeda\n",
    "        return scrapy.FormRequest.from_response(\n",
    "            response,\n",
    "            formdata={'query': 'python scrapy'},\n",
    "            callback=self.parse_results\n",
    "        )\n",
    "    \n",
    "    def parse_results(self, response):\n",
    "        # Procesar resultados de b√∫squeda\n",
    "        results = response.css('.search-result')\n",
    "        \n",
    "        for result in results:\n",
    "            yield {\n",
    "                'title': result.css('.title::text').get(),\n",
    "                'link': result.css('a::attr(href)').get(),\n",
    "                'snippet': result.css('.snippet::text').get()\n",
    "            }\n",
    "\n",
    "print(\"üìù EJERCICIOS CREADOS:\")\n",
    "print(\"  1. üì∞ NewsSpider - Spider para noticias\")\n",
    "print(\"  2. üßπ CleaningPipeline - Limpieza de datos\")\n",
    "print(\"  3. üîç SearchSpider - Formularios y b√∫squedas\")\n",
    "\n",
    "# Simulaci√≥n de ejecuci√≥n\n",
    "print(\"\\nüöÄ SIMULACI√ìN DE EJECUCI√ìN:\")\n",
    "print(\"Comandos t√≠picos de Scrapy:\")\n",
    "print(\"  scrapy startproject mi_proyecto\")\n",
    "print(\"  scrapy genspider quotes quotes.toscrape.com\")\n",
    "print(\"  scrapy crawl quotes -o quotes.json\")\n",
    "print(\"  scrapy shell 'http://quotes.toscrape.com'\")\n",
    "print(\"  scrapy list  # Ver spiders disponibles\")\n",
    "print(\"  scrapy check quotes  # Verificar spider\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Ejemplo Completo: Spider + Pipeline + Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo completo integrado\n",
    "class ComprehensiveSpider(scrapy.Spider):\n",
    "    \"\"\"Spider completo con todas las caracter√≠sticas\"\"\"\n",
    "    name = 'comprehensive'\n",
    "    allowed_domains = ['quotes.toscrape.com']\n",
    "    start_urls = ['http://quotes.toscrape.com/']\n",
    "    \n",
    "    custom_settings = {\n",
    "        'ITEM_PIPELINES': {\n",
    "            '__main__.ValidationPipeline': 300,\n",
    "            '__main__.ProcessingPipeline': 400,\n",
    "        },\n",
    "        'FEEDS': {\n",
    "            'comprehensive_output.json': {\n",
    "                'format': 'json',\n",
    "                'encoding': 'utf8',\n",
    "                'indent': 2\n",
    "            },\n",
    "            'comprehensive_output.csv': {\n",
    "                'format': 'csv',\n",
    "                'encoding': 'utf8'\n",
    "            }\n",
    "        },\n",
    "        'DOWNLOAD_DELAY': 1,\n",
    "        'RANDOMIZE_DOWNLOAD_DELAY': 0.5,\n",
    "        'USER_AGENT': 'comprehensive_spider 1.0'\n",
    "    }\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.stats = {\n",
    "            'quotes_scraped': 0,\n",
    "            'authors_scraped': 0,\n",
    "            'pages_processed': 0,\n",
    "            'errors': 0\n",
    "        }\n",
    "    \n",
    "    def parse(self, response):\n",
    "        \"\"\"Parse principal con estad√≠sticas\"\"\"\n",
    "        self.stats['pages_processed'] += 1\n",
    "        self.logger.info(f\"Procesando p√°gina {self.stats['pages_processed']}: {response.url}\")\n",
    "        \n",
    "        quotes = response.css('.quote')\n",
    "        \n",
    "        for quote in quotes:\n",
    "            try:\n",
    "                item = QuoteItem()\n",
    "                item['text'] = quote.css('.text::text').get()\n",
    "                item['author'] = quote.css('.author::text').get()\n",
    "                item['tags'] = quote.css('.tag::text').getall()\n",
    "                item['url'] = response.url\n",
    "                \n",
    "                self.stats['quotes_scraped'] += 1\n",
    "                yield item\n",
    "                \n",
    "                # Obtener info del autor\n",
    "                author_url = quote.css('.author + a::attr(href)').get()\n",
    "                if author_url:\n",
    "                    yield response.follow(\n",
    "                        author_url,\n",
    "                        self.parse_author,\n",
    "                        meta={'author_name': item['author']}\n",
    "                    )\n",
    "                    \n",
    "            except Exception as e:\n",
    "                self.stats['errors'] += 1\n",
    "                self.logger.error(f\"Error procesando cita: {e}\")\n",
    "        \n",
    "        # Paginaci√≥n\n",
    "        next_page = response.css('.next a::attr(href)').get()\n",
    "        if next_page and self.stats['pages_processed'] < 5:  # Limitar para demo\n",
    "            yield response.follow(next_page, self.parse)\n",
    "        else:\n",
    "            self.logger.info(\"Scraping completado - Estad√≠sticas finales:\")\n",
    "            for key, value in self.stats.items():\n",
    "                self.logger.info(f\"  {key}: {value}\")\n",
    "    \n",
    "    def parse_author(self, response):\n",
    "        \"\"\"Parse informaci√≥n detallada de autores\"\"\"\n",
    "        try:\n",
    "            self.stats['authors_scraped'] += 1\n",
    "            \n",
    "            yield {\n",
    "                'type': 'author',\n",
    "                'name': response.meta['author_name'],\n",
    "                'born_date': response.css('.author-born-date::text').get(),\n",
    "                'born_location': response.css('.author-born-location::text').get(),\n",
    "                'description': response.css('.author-description::text').get(),\n",
    "                'url': response.url\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.stats['errors'] += 1\n",
    "            self.logger.error(f\"Error procesando autor: {e}\")\n",
    "\n",
    "print(\"üìä SPIDER COMPREHENSIVO CREADO\")\n",
    "print(\"Caracter√≠sticas completas:\")\n",
    "print(\"  üìà Estad√≠sticas en tiempo real\")\n",
    "print(\"  üìù Logging detallado\")\n",
    "print(\"  üîÑ Manejo de errores robusto\")\n",
    "print(\"  üì§ Export m√∫ltiple (JSON + CSV)\")\n",
    "print(\"  ‚öôÔ∏è Settings personalizados\")\n",
    "print(\"  üï∑Ô∏è Multi-callback processing\")\n",
    "\n",
    "# Simulaci√≥n de resultados\n",
    "print(\"\\nüìã EJEMPLO DE RESULTADOS:\")\n",
    "sample_results = {\n",
    "    'quotes_scraped': 50,\n",
    "    'authors_scraped': 25,\n",
    "    'pages_processed': 5,\n",
    "    'errors': 2\n",
    "}\n",
    "\n",
    "for key, value in sample_results.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ Resumen de la Lecci√≥n\n",
    "\n",
    "### üéØ Lo que Hemos Dominado\n",
    "\n",
    "1. **Fundamentos de Scrapy**:\n",
    "   - Arquitectura del framework\n",
    "   - Componentes principales (Engine, Scheduler, Downloader)\n",
    "   - Diferencias con Beautiful Soup\n",
    "\n",
    "2. **Spiders Avanzados**:\n",
    "   - Creaci√≥n de spiders b√°sicos y avanzados\n",
    "   - Manejo de m√∫ltiples callbacks\n",
    "   - Paginaci√≥n autom√°tica\n",
    "   - Extracci√≥n de datos complejos\n",
    "\n",
    "3. **Items y Pipelines**:\n",
    "   - Definici√≥n de estructuras de datos\n",
    "   - Validaci√≥n autom√°tica\n",
    "   - Procesamiento de datos\n",
    "   - Pipelines de limpieza y transformaci√≥n\n",
    "\n",
    "4. **Configuraci√≥n Avanzada**:\n",
    "   - Settings personalizados\n",
    "   - Middlewares custom\n",
    "   - Manejo de errores y reintentos\n",
    "   - Configuraci√≥n de cortes√≠a\n",
    "\n",
    "5. **Casos de Uso Reales**:\n",
    "   - Spider para e-commerce\n",
    "   - Manejo de formularios\n",
    "   - Extracci√≥n de metadatos\n",
    "   - Export autom√°tico\n",
    "\n",
    "### üöÄ Pr√≥xima Lecci√≥n: Spiders Avanzados\n",
    "\n",
    "En la siguiente lecci√≥n profundizaremos en:\n",
    "- Spiders especializados (CrawlSpider, XMLFeedSpider)\n",
    "- Manejo de JavaScript con Splash\n",
    "- Scraping distribuido con Scrapyd\n",
    "- Debugging y optimizaci√≥n\n",
    "- Casos de uso complejos\n",
    "\n",
    "### üí° Comandos Esenciales\n",
    "\n",
    "```bash\n",
    "# Crear proyecto\n",
    "scrapy startproject myproject\n",
    "\n",
    "# Generar spider\n",
    "scrapy genspider quotes quotes.toscrape.com\n",
    "\n",
    "# Ejecutar spider\n",
    "scrapy crawl quotes -o output.json\n",
    "\n",
    "# Shell interactivo\n",
    "scrapy shell \"http://quotes.toscrape.com\"\n",
    "\n",
    "# Verificar spider\n",
    "scrapy check quotes\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "¬°Excelente progreso! üéâ Ahora dominas Scrapy y puedes crear scrapers industriales robustos y escalables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}